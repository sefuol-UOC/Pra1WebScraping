id;category_code;category;title;date;author_1;author_2;author_3;author_4;author_5;summary;link
arXiv:cs/9308102;[cs.AI];Computer Science > Artificial Intelligence;A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems;[Submitted on 1 Aug 1993];M. P. Wellman;;;;;Market price systems constitute a well-understood class of mechanisms thatunder certain conditions provide effective decentralization of decision makingwith minimal communication overhead. In a market-oriented programming approachto distributed problem solving, we derive the activities and resourceallocations for a set of computational agents by computing the competitiveequilibrium of an artificial economy. WALRAS provides basic constructs fordefining computational market structures, and protocols for deriving theircorresponding price equilibria. In a particular realization of this approachfor a form of multicommodity flow problem, we see that careful construction ofthe decision process according to economic principles can lead to efficientdistributed resource allocation, and that the behavior of the system can bemeaningfully analyzed in economic terms.;https://arxiv.org/abs/cs/9308102
arXiv:cs/9308101;[cs.AI];Computer Science > Artificial Intelligence;Dynamic Backtracking;[Submitted on 1 Aug 1993];M. L. Ginsberg;;;;;Because of their occasional need to return to shallow points in a searchtree, existing backtracking methods can sometimes erase meaningful progresstoward solving a search problem. In this paper, we present a method by whichbacktrack points can be moved deeper in the search space, thereby avoiding thisdifficulty. The technique developed is a variant of dependency-directedbacktracking that uses only polynomial space while still providing usefulcontrol information and retaining the completeness guarantees provided byearlier approaches.;https://arxiv.org/abs/cs/9308101
arXiv:cs/9403101;[cs.AI];Computer Science > Artificial Intelligence;Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction;[Submitted on 1 Mar 1994];P. M. Murphy;M. J. Pazzani;;;;We report on a series of experiments in which all decision trees consistentwith the training data are constructed. These experiments were run to gain anunderstanding of the properties of the set of consistent decision trees and thefactors that affect the accuracy of individual trees. In particular, weinvestigated the relationship between the size of a decision tree consistentwith some training data and the accuracy of the tree on test data. Theexperiments were performed on a massively parallel Maspar computer. The resultsof the experiments on several artificial and two real world problems indicatethat, for many of the problems investigated, smaller consistent decision treesare on average less accurate than the average accuracy of slightly largertrees.;https://arxiv.org/abs/cs/9403101
arXiv:cs/9402103;[cs.AI];Computer Science > Artificial Intelligence;Bias-Driven Revision of Logical Domain Theories;[Submitted on 1 Feb 1994];M. Koppel;R. Feldman;A. M. Segre;;;The theory revision problem is the problem of how best to go about revising adeficient domain theory using information contained in examples that exposeinaccuracies. In this paper we present our approach to the theory revisionproblem for propositional domain theories. The approach described here, calledPTR, uses probabilities associated with domain theory elements to numericallytrack the ``flow'' of proof through the theory. This allows us to measure theprecise role of a clause or literal in allowing or preventing a (desired orundesired) derivation for a given example. This information is used toefficiently locate and repair flawed elements of the theory. PTR is proved toconverge to a theory which correctly classifies all examples, and shownexperimentally to be fast and accurate even for deep theories.;https://arxiv.org/abs/cs/9402103
arXiv:cs/9402102;[cs.AI];Computer Science > Artificial Intelligence;Substructure Discovery Using Minimum Description Length and Background Knowledge;[Submitted on 1 Feb 1994];D. J. Cook;L. B. Holder;;;;The ability to identify interesting and repetitive substructures is anessential component to discovering knowledge in structural data. We describe anew version of our SUBDUE substructure discovery system based on the minimumdescription length principle. The SUBDUE system discovers substructures thatcompress the original data and represent structural concepts in the data. Byreplacing previously-discovered substructures in the data, multiple passes ofSUBDUE produce a hierarchical description of the structural regularities in thedata. SUBDUE uses a computationally-bounded inexact graph match that identifiessimilar, but not identical, instances of a substructure and finds anapproximate measure of closeness of two substructures when under computationalconstraints. In addition to the minimum description length principle, otherbackground knowledge can be used by SUBDUE to guide the search towards moreappropriate substructures. Experiments in a variety of domains demonstrateSUBDUE's ability to find substructures capable of compressing the original dataand to discover structural concepts important to the domain. Description ofOnline Appendix: This is a compressed tar file containing the SUBDUE discoverysystem, written in C. The program accepts as input databases represented ingraph form, and will output discovered substructures with their correspondingvalue.;https://arxiv.org/abs/cs/9402102
arXiv:cs/9402101;[cs.AI];Computer Science > Artificial Intelligence;Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models;[Submitted on 1 Feb 1994];C. X. Ling;;;;;Learning the past tense of English verbs - a seemingly minor aspect oflanguage acquisition - has generated heated debates since 1986, and has becomea landmark task for testing the adequacy of cognitive modeling. Severalartificial neural networks (ANNs) have been implemented, and a challenge forbetter symbolic models has been posed. In this paper, we present ageneral-purpose Symbolic Pattern Associator (SPA) based upon the decision-treelearning algorithm ID3. We conduct extensive head-to-head comparisons on thegeneralization ability between ANN models and the SPA under differentrepresentations. We conclude that the SPA generalizes the past tense of unseenverbs better than ANN models by a wide margin, and we offer insights as to whythis should be the case. We also discuss a new default strategy fordecision-tree learning algorithms.;https://arxiv.org/abs/cs/9402101
arXiv:cs/9311102;[cs.AI];Computer Science > Artificial Intelligence;Software Agents: Completing Patterns and Constructing User Interfaces;[Submitted on 1 Nov 1993];J. C. Schlimmer;L. A. Hermens;;;;To support the goal of allowing users to record and retrieve information,this paper describes an interactive note-taking system for pen-based computerswith two distinctive features. First, it actively predicts what the user isgoing to write. Second, it automatically constructs a custom, button-box userinterface on request. The system is an example of a learning-apprenticesoftware- agent. A machine learning component characterizes the syntax andsemantics of the user's information. A performance system uses this learnedinformation to generate completion strings and construct a user interface.Description of Online Appendix: People like to record information. Doing thison paper is initially efficient, but lacks flexibility. Recording informationon a computer is less efficient but more powerful. In our new note takingsoftwre, the user records information directly on a computer. Behind theinterface, an agent acts for the user. To help, it provides defaults andconstructs a custom user interface. The demonstration is a QuickTime movie ofthe note taking agent in action. The file is a binhexed self-extractingarchive. Macintosh utilities for binhex are available from;https://arxiv.org/abs/cs/9311102
arXiv:cs/9309101;[cs.AI];Computer Science > Artificial Intelligence;An Empirical Analysis of Search in GSAT;[Submitted on 1 Sep 1993];I. P. Gent;T. Walsh;;;;We describe an extensive study of search in GSAT, an approximation procedurefor propositional satisfiability. GSAT performs greedy hill-climbing on thenumber of satisfied clauses in a truth assignment. Our experiments provide amore complete picture of GSAT's search than previous accounts. We describe indetail the two phases of search: rapid hill-climbing followed by a long plateausearch. We demonstrate that when applied to randomly generated 3SAT problems,there is a very simple scaling with problem size for both the mean number ofsatisfied clauses and the mean branching rate. Our results allow us to makedetailed numerical conjectures about the length of the hill-climbing phase, theaverage gradient of this phase, and to conjecture that both the average scoreand average branching rate decay exponentially during plateau search. We end byshowing how these results can be used to direct future theoretical analysis.This work provides a case study of how computer experiments can be used toimprove understanding of the theoretical properties of algorithms.;https://arxiv.org/abs/cs/9309101
arXiv:cs/9311101;[cs.AI];Computer Science > Artificial Intelligence;The Difficulties of Learning Logic Programs with Cut;[Submitted on 1 Nov 1993];F. Bergadano;D. Gunetti;U. Trinchero;;;As real logic programmers normally use cut (!), an effective learningprocedure for logic programs should be able to deal with it. Because the cutpredicate has only a procedural meaning, clauses containing cut cannot belearned using an extensional evaluation method, as is done in most learningsystems. On the other hand, searching a space of possible programs (instead ofa space of independent clauses) is unfeasible. An alternative solution is togenerate first a candidate base program which covers the positive examples, andthen make it consistent by inserting cut where appropriate. The problem oflearning programs with cut has not been investigated before and this seems tobe a natural and reasonable approach. We generalize this scheme and investigatethe difficulties that arise. Some of the major shortcomings are actuallycaused, in general, by the need for intensional evaluation. As a conclusion,the analysis of this paper suggests, on precise and technical grounds, thatlearning cut is difficult, and current induction techniques should probably berestricted to purely declarative logic languages.;https://arxiv.org/abs/cs/9311101
arXiv:cs/9312101;[cs.AI];Computer Science > Artificial Intelligence;Decidable Reasoning in Terminological Knowledge Representation Systems;[Submitted on 1 Dec 1993];M. Buchheit;F. M. Donini;A. Schaerf;;;Terminological knowledge representation systems (TKRSs) are tools fordesigning and using knowledge bases that make use of terminological languages(or concept languages). We analyze from a theoretical point of view a TKRSwhose capabilities go beyond the ones of presently available TKRSs. The newfeatures studied, often required in practical applications, can be summarizedin three main points. First, we consider a highly expressive terminologicallanguage, called ALCNR, including general complements of concepts, numberrestrictions and role conjunction. Second, we allow to express inclusionstatements between general concepts, and terminological cycles as a particularcase. Third, we prove the decidability of a number of desirable TKRS-deductionservices (like satisfiability, subsumption and instance checking) through asound, complete and terminating calculus for reasoning in ALCNR-knowledgebases. Our calculus extends the general technique of constraint systems. As abyproduct of the proof, we get also the result that inclusion statements inALCNR can be simulated by terminological cycles, if descriptive semantics isadopted.;https://arxiv.org/abs/cs/9312101
arXiv:cs/9409101;[cs.AI];Computer Science > Artificial Intelligence;On Planning while Learning;[Submitted on 1 Sep 1994];S. Safra;M. Tennenholtz;;;;This paper introduces a framework for Planning while Learning where an agentis given a goal to achieve in an environment whose behavior is only partiallyknown to the agent. We discuss the tractability of various plan-designprocesses. We show that for a large natural class of Planning while Learningsystems, a plan can be presented and verified in a reasonable time. However,coming up algorithmically with a plan, even for simple classes of systems isapparently intractable. We emphasize the role of off-line plan-designprocesses, and show that, in most natural cases, the verification (projection)part can be carried out in an efficient algorithmic manner.;https://arxiv.org/abs/cs/9409101
arXiv:cs/9412102;[cs.AI];Computer Science > Artificial Intelligence;Operations for Learning with Graphical Models;[Submitted on 1 Dec 1994];W. L. Buntine;;;;;This paper is a multidisciplinary review of empirical, statistical learningfrom a graphical model perspective. Well-known examples of graphical modelsinclude Bayesian networks, directed graphs representing a Markov chain, andundirected networks representing a Markov field. These graphical models areextended to model data analysis and empirical learning using the notation ofplates. Graphical operations for simplifying and manipulating a problem areprovided including decomposition, differentiation, and the manipulation ofprobability models from the exponential family. Two standard algorithm schemasfor learning are reviewed in a graphical framework: Gibbs sampling and theexpectation maximization algorithm. Using these operations and schemas, somepopular algorithms can be synthesized from their graphical specification. Thisincludes versions of linear regression, techniques for feed-forward networks,and learning Gaussian and discrete Bayesian networks from data. The paperconcludes by sketching some implications for data analysis and summarizing howsome popular algorithms fall within the framework presented. The main originalcontributions here are the decomposition techniques and the demonstration thatgraphical models provide a framework for understanding and developing complexlearning algorithms.;https://arxiv.org/abs/cs/9412102
arXiv:cs/9412103;[cs.AI];Computer Science > Artificial Intelligence;Total-Order and Partial-Order Planning: A Comparative Analysis;[Submitted on 1 Dec 1994];S. Minton;J. Bresina;M. Drummond;;;For many years, the intuitions underlying partial-order planning were largelytaken for granted. Only in the past few years has there been renewed interestin the fundamental principles underlying this paradigm. In this paper, wepresent a rigorous comparative analysis of partial-order and total-orderplanning by focusing on two specific planners that can be directly compared. Weshow that there are some subtle assumptions that underly the wide-spreadintuitions regarding the supposed efficiency of partial-order planning. Forinstance, the superiority of partial-order planning can depend critically uponthe search strategy and the structure of the search space. Understanding theunderlying assumptions is crucial for constructing efficient planners.;https://arxiv.org/abs/cs/9412103
arXiv:cs/9412101;[cs.AI];Computer Science > Artificial Intelligence;Wrap-Up: a Trainable Discourse Module for Information Extraction;[Submitted on 1 Dec 1994];S. Soderland;Lehnert. W;;;;The vast amounts of on-line text now available have led to renewed interestin information extraction (IE) systems that analyze unrestricted text,producing a structured representation of selected information from the text.This paper presents a novel approach that uses machine learning to acquireknowledge for some of the higher level IE processing. Wrap-Up is a trainable IEdiscourse component that makes intersentential inferences and identifieslogical relations among information extracted from the text. Previouscorpus-based approaches were limited to lower level processing such aspart-of-speech tagging, lexical disambiguation, and dictionary construction.Wrap-Up is fully trainable, and not only automatically decides what classifiersare needed, but even derives the feature set for each classifier automatically.Performance equals that of a partially trainable discourse module requiringmanual customization for each domain.;https://arxiv.org/abs/cs/9412101
arXiv:cs/9408103;[cs.AI];Computer Science > Artificial Intelligence;A System for Induction of Oblique Decision Trees;[Submitted on 1 Aug 1994];S. K. Murthy;S. Kasif;S. Salzberg;;;This article describes a new system for induction of oblique decision trees.This system, OC1, combines deterministic hill-climbing with two forms ofrandomization to find a good oblique split (in the form of a hyperplane) ateach node of a decision tree. Oblique decision tree methods are tunedespecially for domains in which the attributes are numeric, although they canbe adapted to symbolic or mixed symbolic/numeric attributes. We presentextensive empirical studies, using both real and artificial data, that analyzeOC1's ability to construct oblique trees that are smaller and more accuratethan their axis-parallel counterparts. We also examine the benefits ofrandomization for the construction of oblique decision trees.;https://arxiv.org/abs/cs/9408103
arXiv:cs/9408102;[cs.AI];Computer Science > Artificial Intelligence;Pattern Matching and Discourse Processing in Information Extraction from Japanese Text;[Submitted on 1 Aug 1994];T. Kitani;Y. Eriguchi;M. Hara;;;"Information extraction is the task of automatically picking up information ofinterest from an unconstrained text. Information of interest is usuallyextracted in two steps. First, sentence level processing locates relevantpieces of information scattered throughout the text; second, discourseprocessing merges coreferential information to generate the output. In thefirst step, pieces of information are locally identified without recognizingany relationships among them. A key word search or simple pattern search canachieve this purpose. The second step requires deeper knowledge in order tounderstand relationships among separately identified pieces of information.Previous information extraction systems focused on the first step, partlybecause they were not required to link up each piece of information with otherpieces. To link the extracted pieces of information and map them onto astructured output format, complex discourse processing is essential. This paperreports on a Japanese information extraction system that merges informationusing a pattern matcher and discourse processor. Evaluation results show a highlevel of system performance which approaches human performance.";https://arxiv.org/abs/cs/9408102
arXiv:cs/9501103;[cs.AI];Computer Science > Artificial Intelligence;Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning;[Submitted on 1 Jan 1995];P. Cichosz;;;;;Temporal difference (TD) methods constitute a class of methods for learningpredictions in multi-step prediction problems, parameterized by a recencyfactor lambda. Currently the most important application of these methods is totemporal credit assignment in reinforcement learning. Well known reinforcementlearning algorithms, such as AHC or Q-learning, may be viewed as instances ofTD learning. This paper examines the issues of the efficient and generalimplementation of TD(lambda) for arbitrary lambda, for use with reinforcementlearning algorithms optimizing the discounted sum of rewards. The traditionalapproach, based on eligibility traces, is argued to suffer from bothinefficiency and lack of generality. The TTD (Truncated Temporal Differences)procedure is proposed as an alternative, that indeed only approximatesTD(lambda), but requires very little computation per action and can be usedwith arbitrary function representation methods. The idea from which it isderived is fairly simple and not new, but probably unexplored so far.Encouraging experimental results are presented, suggesting that using lambda&gt 0 with the TTD procedure allows one to obtain a significant learningspeedup at essentially the same cost as usual TD(0) learning.;https://arxiv.org/abs/cs/9501103
arXiv:cs/9408101;[cs.AI];Computer Science > Artificial Intelligence;Random Worlds and Maximum Entropy;[Submitted on 1 Aug 1994];A. J. Grove;J. Y. Halpern;D. Koller;;;Given a knowledge base KB containing first-order and statistical facts, weconsider a principled method, called the random-worlds method, for computing adegree of belief that some formula Phi holds given KB. If we are reasoningabout a world or system consisting of N individuals, then we can consider allpossible worlds, or first-order models, with domain {1,...,N} that satisfy KB,and compute the fraction of them in which Phi is true. We define the degree ofbelief to be the asymptotic value of this fraction as N grows large. We showthat when the vocabulary underlying Phi and KB uses constants and unarypredicates only, we can naturally associate an entropy with each world. As Ngrows larger, there are many more worlds with higher entropy. Therefore, we canuse a maximum-entropy computation to compute the degree of belief. This resultis in a similar spirit to previous work in physics and artificial intelligence,but is far more general. Of equal interest to the result itself are thelimitations on its scope. Most importantly, the restriction to unary predicatesseems necessary. Although the random-worlds method makes sense in general, theconnection to maximum entropy seems to disappear in the non-unary case. Theseobservations suggest unexpected limitations to the applicability ofmaximum-entropy methods.;https://arxiv.org/abs/cs/9408101
arXiv:cs/9504101;[cs.AI];Computer Science > Artificial Intelligence;Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach;[Submitted on 1 Apr 1995];S. K. Donoho;L. A. Rendell;;;;Theory revision integrates inductive learning and background knowledge bycombining training examples with a coarse domain theory to produce a moreaccurate theory. There are two challenges that theory revision and othertheory-guided systems face. First, a representation language appropriate forthe initial theory may be inappropriate for an improved theory. While theoriginal representation may concisely express the initial theory, a moreaccurate theory forced to use that same representation may be bulky,cumbersome, and difficult to reach. Second, a theory structure suitable for acoarse domain theory may be insufficient for a fine-tuned theory. Systems thatproduce only small, local changes to a theory have limited value foraccomplishing complex structural alterations that may be required.Consequently, advanced theory-guided learning systems require flexiblerepresentation and flexible structure. An analysis of various theory revisionsystems and theory-guided learning systems reveals specific strengths andweaknesses in terms of these two desired properties. Designed to capture theunderlying qualities of each system, a new system uses theory-guidedconstructive induction. Experiments in three domains show improvement overprevious theory-guided systems. This leads to a study of the behavior,limitations, and potential of theory-guided constructive induction.;https://arxiv.org/abs/cs/9504101
arXiv:cs/9505105;[cs.AI];Computer Science > Artificial Intelligence;Pac-learning Recursive Logic Programs: Negative Results;[Submitted on 1 May 1995];W. W. Cohen;;;;;"In a companion paper it was shown that the class of constant-depthdeterminate k-ary recursive clauses is efficiently learnable. In this paper wepresent negative results showing that any natural generalization of this classis hard to learn in Valiant's model of pac-learnability. In particular, we showthat the following program classes are cryptographically hard to learn:programs with an unbounded number of constant-depth linear recursive clauses;programs with one constant-depth determinate clause containing an unboundednumber of recursive calls; and programs with one linear recursive clause ofconstant locality. These results immediately imply the non-learnability of anymore general class of programs. We also show that learning a constant-depthdeterminate program with either two linear recursive clauses or one linearrecursive clause and one non-recursive clause is as hard as learning booleanDNF. Together with positive results from the companion paper, these negativeresults establish a boundary of efficient learnability for recursivefunction-free clauses.";https://arxiv.org/abs/cs/9505105
arXiv:cs/9505104;[cs.AI];Computer Science > Artificial Intelligence;Pac-Learning Recursive Logic Programs: Efficient Algorithms;[Submitted on 1 May 1995];W. W. Cohen;;;;;We present algorithms that learn certain classes of function-free recursivelogic programs in polynomial time from equivalence queries. In particular, weshow that a single k-ary recursive constant-depth determinate clause islearnable. Two-clause programs consisting of one learnable recursive clause andone constant-depth determinate non-recursive clause are also learnable, if anadditional ``basecase'' oracle is assumed. These results immediately imply thepac-learnability of these classes. Although these classes of learnablerecursive programs are very constrained, it is shown in a companion paper thatthey are maximally general, in that generalizing either class in any naturalway leads to a computationally difficult learning problem. Thus, taken togetherwith its companion paper, this paper establishes a boundary of efficientlearnability for recursive logic programs.;https://arxiv.org/abs/cs/9505104
arXiv:cs/9505102;[cs.AI];Computer Science > Artificial Intelligence;Adaptive Load Balancing: A Study in Multi-Agent Learning;[Submitted on 1 May 1995];A. Schaerf;Y. Shoham;M. Tennenholtz;;;We study the process of multi-agent reinforcement learning in the context ofload balancing in a distributed system, without use of either centralcoordination or explicit communication. We first define a precise framework inwhich to study adaptive load balancing, important features of which are itsstochastic nature and the purely local information available to individualagents. Given this framework, we show illuminating results on the interplaybetween basic adaptive behavior parameters and their effect on systemefficiency. We then investigate the properties of adaptive load balancing inheterogeneous populations, and address the issue of exploration vs.exploitation in that context. Finally, we show that naive use of communicationmay not improve, and might even harm system efficiency.;https://arxiv.org/abs/cs/9505102
arXiv:cs/9505103;[cs.AI];Computer Science > Artificial Intelligence;Provably Bounded-Optimal Agents;[Submitted on 1 May 1995];S. J. Russell;D. Subramanian;;;;Since its inception, artificial intelligence has relied upon a theoreticalfoundation centered around perfect rationality as the desired property ofintelligent systems. We argue, as others have done, that this foundation isinadequate because it imposes fundamentally unsatisfiable requirements. As aresult, there has arisen a wide gap between theory and practice in AI,hindering progress in the field. We propose instead a property called boundedoptimality. Roughly speaking, an agent is bounded-optimal if its program is asolution to the constrained optimization problem presented by its architectureand the task environment. We show how to construct agents with this propertyfor a simple class of machine architectures in a broad class of real-timeenvironments. We illustrate these results using a simple model of an automatedmail sorting facility. We also define a weaker property, asymptotic boundedoptimality (ABO), that generalizes the notion of optimality in classicalcomplexity theory. We then construct universal ABO programs, i.e., programsthat are ABO no matter what real-time constraints are applied. Universal ABOprograms can be used as building blocks for more complex systems. We concludewith a discussion of the prospects for bounded optimality as a theoreticalbasis for AI, and relate it to similar trends in philosophy, economics, andgame theory.;https://arxiv.org/abs/cs/9505103
arXiv:cs/9503102;[cs.AI];Computer Science > Artificial Intelligence;Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm;[Submitted on 1 Mar 1995];P. D. Turney;;;;;This paper introduces ICET, a new algorithm for cost-sensitiveclassification. ICET uses a genetic algorithm to evolve a population of biasesfor a decision tree induction algorithm. The fitness function of the geneticalgorithm is the average cost of classification when using the decision tree,including both the costs of tests (features, measurements) and the costs ofclassification errors. ICET is compared here with three other algorithms forcost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,which classifies without regard to cost. The five algorithms are evaluatedempirically on five real-world medical datasets. Three sets of experiments areperformed. The first set examines the baseline performance of the fivealgorithms on the five datasets and establishes that ICET performssignificantly better than its competitors. The second set tests the robustnessof ICET under a variety of conditions and shows that ICET maintains itsadvantage. The third set looks at ICET's search in bias space and discovers away to improve the search.;https://arxiv.org/abs/cs/9503102
arXiv:cs/9505101;[cs.AI];Computer Science > Artificial Intelligence;Using Pivot Consistency to Decompose and Solve Functional CSPs;[Submitted on 1 May 1995];P. David;;;;;"Many studies have been carried out in order to increase the search efficiencyof constraint satisfaction problems; among them, some make use of structuralproperties of the constraint network; others take into account semanticproperties of the constraints, generally assuming that all the constraintspossess the given property. In this paper, we propose a new decompositionmethod benefiting from both semantic properties of functional constraints (notbijective constraints) and structural properties of the network; furthermore,not all the constraints need to be functional. We show that under someconditions, the existence of solutions can be guaranteed. We first characterizea particular subset of the variables, which we name a root set. We thenintroduce pivot consistency, a new local consistency which is a weak form ofpath consistency and can be achieved in O(n^2d^2) complexity (instead ofO(n^3d^3) for path consistency), and we present associated properties; inparticular, we show that any consistent instantiation of the root set can belinearly extended to a solution, which leads to the presentation of theaforementioned new method for solving by decomposing functional CSPs.";https://arxiv.org/abs/cs/9505101
arXiv:cs/9503101;[cs.AI];Computer Science > Artificial Intelligence;On the Informativeness of the DNA Promoter Sequences Domain Theory;[Submitted on 1 Mar 1995];J. Ortega;;;;;The DNA promoter sequences domain theory and database have become popular fortesting systems that integrate empirical and analytical learning. This notereports a simple change and reinterpretation of the domain theory in terms ofM-of-N concepts, involving no learning, that results in an accuracy of 93.4% onthe 106 items of the database. Moreover, an exhaustive search of the space ofM-of-N domain theory interpretations indicates that the expected accuracy of arandomly chosen interpretation is 76.5%, and that a maximum accuracy of 97.2%is achieved in 12 cases. This demonstrates the informativeness of the domaintheory, without the complications of understanding the interactions betweenvarious learning algorithms and the theory. In addition, our results helpcharacterize the difficulty of learning using the DNA promoters theory.;https://arxiv.org/abs/cs/9503101
arXiv:cs/9501102;[cs.AI];Computer Science > Artificial Intelligence;A Domain-Independent Algorithm for Plan Adaptation;[Submitted on 1 Jan 1995];S. Hanks;D. S. Weld;;;;The paradigms of transformational planning, case-based planning, and plandebugging all involve a process known as plan adaptation - modifying orrepairing an old plan so it solves a new problem. In this paper we provide adomain-independent algorithm for plan adaptation, demonstrate that it is sound,complete, and systematic, and compare it to other adaptation algorithms in theliterature. Our approach is based on a view of planning as searching a graph ofpartial plans. Generative planning starts at the graph's root and moves fromnode to node using plan-refinement operators. In planning by adaptation, alibrary plan - an arbitrary node in the plan graph - is the starting point forthe search, and the plan-adaptation algorithm can apply both the samerefinement operators available to a generative planner and can also retractconstraints and steps from the plan. Our algorithm's completeness ensures thatthe adaptation algorithm will eventually search the entire graph and itssystematicity ensures that it will do so without redundantly searching anyparts of the graph.;https://arxiv.org/abs/cs/9501102
arXiv:cs/9501101;[cs.AI];Computer Science > Artificial Intelligence;Solving Multiclass Learning Problems via Error-Correcting Output Codes;[Submitted on 1 Jan 1995];T. G. Dietterich;G. Bakiri;;;;Multiclass learning problems involve finding a definition for an unknownfunction f(x) whose range is a discrete set containing k &gt 2 values (i.e., k``classes''). The definition is acquired by studying collections of trainingexamples of the form [x_i, f (x_i)]. Existing approaches to multiclass learningproblems include direct application of multiclass algorithms such as thedecision-tree algorithms C4.5 and CART, application of binary concept learningalgorithms to learn individual binary functions for each of the k classes, andapplication of binary concept learning algorithms with distributed outputrepresentations. This paper compares these three approaches to a new techniquein which error-correcting codes are employed as a distributed outputrepresentation. We show that these output representations improve thegeneralization performance of both C4.5 and backpropagation on a wide range ofmulticlass learning tasks. We also demonstrate that this approach is robustwith respect to changes in the size of the training sample, the assignment ofdistributed representations to particular classes, and the application ofoverfitting avoidance techniques such as decision-tree pruning. Finally, weshow that---like the other methods---the error-correcting code technique canprovide reliable class probability estimates. Taken together, these resultsdemonstrate that error-correcting output codes provide a general-purpose methodfor improving the performance of inductive learning programs on multiclassproblems.;https://arxiv.org/abs/cs/9501101
arXiv:cs/9406102;[cs.AI];Computer Science > Artificial Intelligence;Applying GSAT to Non-Clausal Formulas;[Submitted on 1 Jun 1994];R. Sebastiani;;;;;In this paper we describe how to modify GSAT so that it can be applied tonon-clausal formulas. The idea is to use a particular ``score'' function whichgives the number of clauses of the CNF conversion of a formula which are falseunder a given truth assignment. Its value is computed in linear time, withoutconstructing the CNF conversion itself. The proposed methodology applies tomost of the variants of GSAT proposed so far.;https://arxiv.org/abs/cs/9406102
arXiv:cs/9406101;[cs.AI];Computer Science > Artificial Intelligence;A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic;[Submitted on 1 Jun 1994];A. Borgida;P. F. Patel-Schneider;;;;This paper analyzes the correctness of the subsumption algorithm used inCLASSIC, a description logic-based knowledge representation system that isbeing used in practical applications. In order to deal efficiently withindividuals in CLASSIC descriptions, the developers have had to use analgorithm that is incomplete with respect to the standard, model-theoreticsemantics for description logics. We provide a variant semantics fordescriptions with respect to which the current implementation is complete, andwhich can be independently motivated. The soundness and completeness of thepolynomial-time subsumption algorithm is established using description graphs,which are an abstracted version of the implementation structures used inCLASSIC, and are of independent interest.;https://arxiv.org/abs/cs/9406101
arXiv:cs/9401101;[cs.AI];Computer Science > Artificial Intelligence;Teleo-Reactive Programs for Agent Control;[Submitted on 1 Jan 1994];N. Nilsson;;;;;"A formalism is presented for computing and organizing actions for autonomousagents in dynamic environments. We introduce the notion of teleo-reactive (T-R)programs whose execution entails the construction of circuitry for thecontinuous computation of the parameters and conditions on which agent actionis based. In addition to continuous feedback, T-R programs support parameterbinding and recursion. A primary difference between T-R programs and many othercircuit-based systems is that the circuitry of T-R programs is more compact; itis constructed at run time and thus does not have to anticipate all thecontingencies that might arise over all possible runs. In addition, T-Rprograms are intuitive and easy to write and are written in a form that iscompatible with automatic planning and learning methods. We briefly describesome experimental applications of T-R programs in the control of simulated andactual mobile robots.";https://arxiv.org/abs/cs/9401101
arXiv:cs/9508102;[cs.AI];Computer Science > Artificial Intelligence;An Integrated Framework for Learning and Reasoning;[Submitted on 1 Aug 1995];C. G. Giraud-Carrier;T. R. Martinez;;;;Learning and reasoning are both aspects of what is considered to beintelligence. Their studies within AI have been separated historically,learning being the topic of machine learning and neural networks, and reasoningfalling under classical (or symbolic) AI. However, learning and reasoning arein many ways interdependent. This paper discusses the nature of some of theseinterdependencies and proposes a general framework called FLARE, that combinesinductive learning using prior knowledge together with reasoning in apropositional setting. Several examples that test the framework are presented,including classical induction, many important reasoning protocols and twosimple expert systems.;https://arxiv.org/abs/cs/9508102
arXiv:cs/9508101;[cs.AI];Computer Science > Artificial Intelligence;Using Qualitative Hypotheses to Identify Inaccurate Data;[Submitted on 1 Aug 1995];Q. Zhao;T. Nishida;;;;Identifying inaccurate data has long been regarded as a significant anddifficult problem in AI. In this paper, we present a new method for identifyinginaccurate data on the basis of qualitative correlations among related data.First, we introduce the definitions of related data and qualitativecorrelations among related data. Then we put forward a new concept calledsupport coefficient function (SCF). SCF can be used to extract, represent, andcalculate qualitative correlations among related data within a dataset. Wepropose an approach to determining dynamic shift intervals of inaccurate data,and an approach to calculating possibility of identifying inaccurate data,respectively. Both of the approaches are based on SCF. Finally we present analgorithm for identifying inaccurate data by using qualitative correlationsamong related data as confirmatory or disconfirmatory evidence. We havedeveloped a practical system for interpreting infrared spectra by applying themethod, and have fully tested the system against several hundred real spectra.The experimental results show that the method is significantly better than theconventional methods used in many similar systems.;https://arxiv.org/abs/cs/9508101
arXiv:cs/9510103;[cs.AI];Computer Science > Artificial Intelligence;Learning Membership Functions in a Function-Based Object Recognition System;[Submitted on 1 Oct 1995];K. Woods;D. Cook;L. Hall;K. Bowyer;L. Stark;Functionality-based recognition systems recognize objects at the categorylevel by reasoning about how well the objects support the expected function.Such systems naturally associate a ``measure of goodness'' or ``membershipvalue'' with a recognized object. This measure of goodness is the result ofcombining individual measures, or membership values, from potentially manyprimitive evaluations of different properties of the object's shape. Amembership function is used to compute the membership value when evaluating aprimitive of a particular physical property of an object. In previous versionsof a recognition system known as Gruff, the membership function for each of theprimitive evaluations was hand-crafted by the system designer. In this paper,we provide a learning component for the Gruff system, called Omlet, thatautomatically learns membership functions given a set of example objectslabeled with their desired category measure. The learning algorithm isgenerally applicable to any problem in which low-level membership values arecombined through an and-or tree structure to give a final overall membershipvalue.;https://arxiv.org/abs/cs/9510103
arXiv:cs/9511101;[cs.AI];Computer Science > Artificial Intelligence;Flexibly Instructable Agents;[Submitted on 1 Nov 1995];S. B. Huffman;J. E. Laird;;;;"This paper presents an approach to learning from situated, interactivetutorial instruction within an ongoing agent. Tutorial instruction is aflexible (and thus powerful) paradigm for teaching tasks because it allows aninstructor to communicate whatever types of knowledge an agent might need inwhatever situations might arise. To support this flexibility, however, theagent must be able to learn multiple kinds of knowledge from a broad range ofinstructional interactions. Our approach, called situated explanation, achievessuch learning through a combination of analytic and inductive techniques. Itcombines a form of explanation-based learning that is situated for eachinstruction with a full suite of contextually guided responses to incompleteexplanations. The approach is implemented in an agent called Instructo-Soarthat learns hierarchies of new tasks and other domain knowledge frominteractive natural language instructions. Instructo-Soar meets three keyrequirements of flexible instructability that distinguish it from previoussystems: (1) it can take known or unknown commands at any instruction point;(2) it can handle instructions that apply to either its current situation or toa hypothetical situation specified in language (as in, for instance,conditional instructions); and (3) it can learn, from instructions, each classof knowledge it uses to perform tasks.";https://arxiv.org/abs/cs/9511101
arXiv:cs/9512107;[cs.AI];Computer Science > Artificial Intelligence;Rule-based Machine Learning Methods for Functional Prediction;[Submitted on 1 Dec 1995];S. M. Weiss;N. Indurkhya;;;;We describe a machine learning method for predicting the value of areal-valued function, given the values of multiple input variables. The methodinduces solutions from samples in the form of ordered disjunctive normal form(DNF) decision rules. A central objective of the method and representation isthe induction of compact, easily interpretable solutions. This rule-baseddecision model can be extended to search efficiently for similar cases prior toapproximating function values. Experimental results on real-world datademonstrate that the new techniques are competitive with existing machinelearning and statistical methods and can sometimes yield superior regressionperformance.;https://arxiv.org/abs/cs/9512107
arXiv:cs/9512104;[cs.AI];Computer Science > Artificial Intelligence;Decision-Theoretic Foundations for Causal Reasoning;[Submitted on 1 Dec 1995];D. Heckerman;R. Shachter;;;;We present a definition of cause and effect in terms of decision-theoreticprimitives and thereby provide a principled foundation for causal reasoning.Our definition departs from the traditional view of causation in that causalassertions may vary with the set of decisions available. We argue that thisapproach provides added clarity to the notion of cause. Also in this paper, weexamine the encoding of causal relationships in directed acyclic graphs. Wedescribe a special class of influence diagrams, those in canonical form, andshow its relationship to Pearl's representation of cause and effect. Finally,we show how canonical form facilitates counterfactual reasoning.;https://arxiv.org/abs/cs/9512104
arXiv:cs/9512105;[cs.AI];Computer Science > Artificial Intelligence;Translating between Horn Representations and their Characteristic Models;[Submitted on 1 Dec 1995];R. Khardon;;;;;Characteristic models are an alternative, model based, representation forHorn expressions. It has been shown that these two representations areincomparable and each has its advantages over the other. It is thereforenatural to ask what is the cost of translating, back and forth, between theserepresentations. Interestingly, the same translation questions arise indatabase theory, where it has applications to the design of relationaldatabases. This paper studies the computational complexity of these problems.Our main result is that the two translation problems are equivalent underpolynomial reductions, and that they are equivalent to the correspondingdecision problem. Namely, translating is equivalent to deciding whether a givenset of models is the set of characteristic models for a given Horn expression.We also relate these problems to the hypergraph transversal problem, a wellknown problem which is related to other applications in AI and for which nopolynomial time algorithm is known. It is shown that in general our translationproblems are at least as hard as the hypergraph transversal problem, and in aspecial case they are equivalent to it.;https://arxiv.org/abs/cs/9512105
arXiv:cs/9512106;[cs.AI];Computer Science > Artificial Intelligence;Statistical Feature Combination for the Evaluation of Game Positions;[Submitted on 1 Dec 1995];M. Buro;;;;;This article describes an application of three well-known statistical methodsin the field of game-tree search: using a large number of classified Othellopositions, feature weights for evaluation functions with agame-phase-independent meaning are estimated by means of logistic regression,Fisher's linear discriminant, and the quadratic discriminant function fornormally distributed features. Thereafter, the playing strengths are comparedby means of tournaments between the resulting versions of a world-class Othelloprogram. In this application, logistic regression - which is used here for thefirst time in the context of game playing - leads to better results than theother approaches.;https://arxiv.org/abs/cs/9512106
arXiv:cs/9604103;[cs.AI];Computer Science > Artificial Intelligence;Iterative Optimization and Simplification of Hierarchical Clusterings;[Submitted on 1 Apr 1996];D. Fisher;;;;;Clustering is often used for discovering structure in data. Clusteringsystems differ in the objective function used to evaluate clustering qualityand the control strategy used to search the space of clusterings. Ideally, thesearch strategy should consistently construct clusterings of high quality, butbe computationally inexpensive as well. In general, we cannot have it bothways, but we can partition the search so that a system inexpensively constructsa `tentative' clustering for initial examination, followed by iterativeoptimization, which continues to search in background for improved clusterings.Given this motivation, we evaluate an inexpensive strategy for creating initialclusterings, coupled with several control strategies for iterativeoptimization, each of which repeatedly modifies an initial clustering in searchof a better one. One of these methods appears novel as an iterativeoptimization strategy in clustering contexts. Once a clustering has beenconstructed it is judged by analysts -- often according to task-specificcriteria. Several authors have abstracted these criteria and posited a genericperformance task akin to pattern completion, where the error rate overcompleted patterns is used to `externally' judge clustering utility. Given thisperformance task, we adapt resampling-based pruning strategies used bysupervised learning systems to the task of simplifying hierarchicalclusterings, thus promising to ease post-clustering analysis. Finally, wepropose a number of objective functions, based on attribute-selection measuresfor decision-tree induction, that might perform well on the error rate andsimplicity dimensions.;https://arxiv.org/abs/cs/9604103
arXiv:cs/9604102;[cs.AI];Computer Science > Artificial Intelligence;Practical Methods for Proving Termination of General Logic Programs;[Submitted on 1 Apr 1996];E. Marchiori;;;;;Termination of logic programs with negated body atoms (here called generallogic programs) is an important topic. One reason is that many computationalmechanisms used to process negated atoms, like Clark's negation as failure andChan's constructive negation, are based on termination conditions. This paperintroduces a methodology for proving termination of general logic programsw.r.t. the Prolog selection rule. The idea is to distinguish parts of theprogram depending on whether or not their termination depends on the selectionrule. To this end, the notions of low-, weakly up-, and up-acceptable programare introduced. We use these notions to develop a methodology for provingtermination of general logic programs, and show how interesting problems innon-monotonic reasoning can be formalized and implemented by means ofterminating general logic programs.;https://arxiv.org/abs/cs/9604102
arXiv:cs/9606102;[cs.AI];Computer Science > Artificial Intelligence;On Partially Controlled Multi-Agent Systems;[Submitted on 1 Jun 1996];R. I. Brafman;M. Tennenholtz;;;;Motivated by the control theoretic distinction between controllable anduncontrollable events, we distinguish between two types of agents within amulti-agent system: controllable agents, which are directly controlled by thesystem's designer, and uncontrollable agents, which are not under thedesigner's direct control. We refer to such systems as partially controlledmulti-agent systems, and we investigate how one might influence the behavior ofthe uncontrolled agents through appropriate design of the controlled agents. Inparticular, we wish to understand which problems are naturally described inthese terms, what methods can be applied to influence the uncontrollableagents, the effectiveness of such methods, and whether similar methods workacross different domains. Using a game-theoretic framework, this paper studiesthe design of partially controlled multi-agent systems in two contexts: in onecontext, the uncontrollable agents are expected utility maximizers, while inthe other they are reinforcement learners. We suggest different techniques forcontrolling agents' behavior in each domain, assess their success, and examinetheir relationship.;https://arxiv.org/abs/cs/9606102
arXiv:cs/9606101;[cs.AI];Computer Science > Artificial Intelligence;A Principled Approach Towards Symbolic Geometric Constraint Satisfaction;[Submitted on 1 Jun 1996];S. Bhansali;G. A. Kramer;T. J. Hoar;;;An important problem in geometric reasoning is to find the configuration of acollection of geometric bodies so as to satisfy a set of given constraints.Recently, it has been suggested that this problem can be solved efficiently bysymbolically reasoning about geometry. This approach, called degrees of freedomanalysis, employs a set of specialized routines called plan fragments thatspecify how to change the configuration of a set of bodies to satisfy a newconstraint while preserving existing constraints. A potential drawback, whichlimits the scalability of this approach, is concerned with the difficulty ofwriting plan fragments. In this paper we address this limitation by showing howthese plan fragments can be automatically synthesized using first principlesabout geometric bodies, actions, and topology.;https://arxiv.org/abs/cs/9606101
arXiv:cs/9609102;[cs.AI];Computer Science > Artificial Intelligence;Cue Phrase Classification Using Machine Learning;[Submitted on 1 Sep 1996];D. J. Litman;;;;;Cue phrases may be used in a discourse sense to explicitly signal discoursestructure, but also in a sentential sense to convey semantic rather thanstructural information. Correctly classifying cue phrases as discourse orsentential is critical in natural language processing systems that exploitdiscourse structure, e.g., for performing tasks such as anaphora resolution andplan recognition. This paper explores the use of machine learning forclassifying cue phrases as discourse or sentential. Two machine learningprograms (Cgrendel and C4.5) are used to induce classification models from setsof pre-classified cue phrases and their features in text and speech. Machinelearning is shown to be an effective technique for not only automating thegeneration of classification models, but also for improving upon previousresults. When compared to manually derived classification models already in theliterature, the learned models often perform with higher accuracy and containnew linguistic insights into the data. In addition, the ability toautomatically construct classification models makes it easier to comparativelyanalyze the utility of alternative feature representations of the data.Finally, the ease of retraining makes the learning approach more scalable andflexible than manual methods.;https://arxiv.org/abs/cs/9609102
arXiv:cs/9609101;[cs.AI];Computer Science > Artificial Intelligence;Accelerating Partial-Order Planners: Some Techniques for Effective Search Control and Pruning;[Submitted on 1 Sep 1996];A. Gerevini;L. Schubert;;;;We propose some domain-independent techniques for bringing well-foundedpartial-order planners closer to practicality. The first two techniques areaimed at improving search control while keeping overhead costs low. One isbased on a simple adjustment to the default A* heuristic used by UCPOP toselect plans for refinement. The other is based on preferring ``zerocommitment'' (forced) plan refinements whenever possible, and using LIFOprioritization otherwise. A more radical technique is the use of operatorparameter domains to prune search. These domains are initially computed fromthe definitions of the operators and the initial and goal conditions, using apolynomial-time algorithm that propagates sets of constants through theoperator graph, starting in the initial conditions. During planning, parameterdomains can be used to prune nonviable operator instances and to removespurious clobbering threats. In experiments based on modifications of UCPOP,our improved plan and goal selection strategies gave speedups by factorsranging from 5 to more than 1000 for a variety of problems that are nontrivialfor the unmodified version. Crucially, the hardest problems gave the greatestimprovements. The pruning technique based on parameter domains often gavespeedups by an order of magnitude or more for difficult problems, both with thedefault UCPOP search strategy and with our improved strategy. The Lisp code forour techniques and for the test problems is provided in on-line appendices.;https://arxiv.org/abs/cs/9609101
arXiv:cs/9608104;[cs.AI];Computer Science > Artificial Intelligence;A Hierarchy of Tractable Subsets for Computing Stable Models;[Submitted on 1 Aug 1996];R. Ben-Eliyahu;;;;;"Finding the stable models of a knowledge base is a significant computationalproblem in artificial intelligence. This task is at the computational heart oftruth maintenance systems, autoepistemic logic, and default logic.Unfortunately, it is NP-hard. In this paper we present a hierarchy of classesof knowledge bases, Omega_1,Omega_2,..., with the following properties: first,Omega_1 is the class of all stratified knowledge bases; second, if a knowledgebase Pi is in Omega_k, then Pi has at most k stable models, and all of them maybe found in time O(lnk), where l is the length of the knowledge base and n thenumber of atoms in Pi; third, for an arbitrary knowledge base Pi, we can findthe minimum k such that Pi belongs to Omega_k in time polynomial in the size ofPi; and, last, where K is the class of all knowledge bases, it is the case thatunion{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to someclass in the hierarchy.";https://arxiv.org/abs/cs/9608104
arXiv:cs/9612103;[cs.AI];Computer Science > Artificial Intelligence;Characterizations of Decomposable Dependency Models;[Submitted on 1 Dec 1996];L. M. deCampos;;;;;Decomposable dependency models possess a number of interesting and usefulproperties. This paper presents new characterizations of decomposable models interms of independence relationships, which are obtained by adding a singleaxiom to the well-known set characterizing dependency models that areisomorphic to undirected graphs. We also briefly discuss a potentialapplication of our results to the problem of learning graphical models fromdata.;https://arxiv.org/abs/cs/9612103
arXiv:cs/9611101;[cs.AI];Computer Science > Artificial Intelligence;MUSE CSP: An Extension to the Constraint Satisfaction Problem;[Submitted on 1 Nov 1996];R. A Helzerman;M. P. Harper;;;;This paper describes an extension to the constraint satisfaction problem(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).This extension is especially useful for those problems which segment intomultiple sets of partially shared variables. Such problems arise naturally insignal processing applications including computer vision, speech processing,and handwriting recognition. For these applications, it is often difficult tosegment the data in only one way given the low-level information utilized bythe segmentation algorithms. MUSE CSP can be used to compactly representseveral similar instances of the constraint satisfaction problem. If multipleinstances of a CSP have some common variables which have the same domains andconstraints, then they can be combined into a single instance of a MUSE CSP,reducing the work required to apply the constraints. We introduce the conceptsof MUSE node consistency, MUSE arc consistency, and MUSE path consistency. Wethen demonstrate how MUSE CSP can be used to compactly represent lexicallyambiguous sentences and the multiple sentence hypotheses that are oftengenerated by speech recognition algorithms so that grammar constraints can beused to provide parses for all syntactically correct sentences. Algorithms forMUSE arc and path consistency are provided. Finally, we discuss how to create aMUSE CSP from a set of CSPs which are labeled to indicate when the samevariable is shared by more than a single CSP.;https://arxiv.org/abs/cs/9611101
arXiv:cs/9610102;[cs.AI];Computer Science > Artificial Intelligence;Learning First-Order Definitions of Functions;[Submitted on 1 Oct 1996];J. R. Quinlan;;;;;First-order learning involves finding a clause-form definition of a relationfrom examples of the relation and relevant background information. In thispaper, a particular first-order learning system is modified to customize it forfinding definitions of functional relations. This restriction leads to fasterlearning times and, in some cases, to definitions that have higher predictiveaccuracy. Other first-order learning systems might benefit from similarspecialization.;https://arxiv.org/abs/cs/9610102
arXiv:cs/9610101;[cs.AI];Computer Science > Artificial Intelligence;Mechanisms for Automated Negotiation in State Oriented Domains;[Submitted on 1 Oct 1996];G. Zlotkin;J. S. Rosenschein;;;;This paper lays part of the groundwork for a domain theory of negotiation,that is, a way of classifying interactions so that it is clear, given a domain,which negotiation mechanisms and strategies are appropriate. We define StateOriented Domains, a general category of interaction. Necessary and sufficientconditions for cooperation are outlined. We use the notion of worth in analtered definition of utility, thus enabling agreements in a wider class ofjoint-goal reachable situations. An approach is offered for conflictresolution, and it is shown that even in a conflict situation, partialcooperative steps can be taken by interacting agents (that is, agents infundamental conflict might still agree to cooperate up to a certain point). AUnified Negotiation Protocol (UNP) is developed that can be used in all typesof encounters. It is shown that in certain borderline cooperative situations, apartial cooperative agreement (i.e., one that does not achieve all agents'goals) might be preferred by all agents, even though there exists a rationalagreement that would achieve all their goals. Finally, we analyze cases whereagents have incomplete information on the goals and worth of other agents.First we consider the case where agents' goals are private information, and weanalyze what goal declaration strategies the agents might adopt to increasetheir utility. Then, we consider the situation where the agents' goals (andtherefore stand-alone costs) are common knowledge, but the worth they attach totheir goals is private information. We introduce two mechanisms, one 'strict',the other 'tolerant', and analyze their affects on the stability and efficiencyof negotiation outcomes.;https://arxiv.org/abs/cs/9610101
arXiv:cs/9701101;[cs.AI];Computer Science > Artificial Intelligence;Improved Heterogeneous Distance Functions;[Submitted on 1 Jan 1997];D. R. Wilson;T. R. Martinez;;;;Instance-based learning techniques typically handle continuous and linearinput values well, but often do not handle nominal input attributesappropriately. The Value Difference Metric (VDM) was designed to findreasonable distance values between nominal attribute values, but it largelyignores continuous attributes, requiring discretization to map continuousvalues into nominal values. This paper proposes three new heterogeneousdistance functions, called the Heterogeneous Value Difference Metric (HVDM),the Interpolated Value Difference Metric (IVDM), and the Windowed ValueDifference Metric (WVDM). These new distance functions are designed to handleapplications with nominal attributes, continuous attributes, or both. Inexperiments on 48 applications the new distance metrics achieve higherclassification accuracy on average than three previous distance functions onthose datasets that have both nominal and continuous attributes.;https://arxiv.org/abs/cs/9701101
arXiv:cs/9701102;[cs.AI];Computer Science > Artificial Intelligence;SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks;[Submitted on 1 Jan 1997];S. Wermter;V. Weber;;;;Previous approaches of analyzing spontaneously spoken language often havebeen based on encoding syntactic and semantic knowledge manually andsymbolically. While there has been some progress using statistical orconnectionist language models, many current spoken- language systems still usea relatively brittle, hand-coded symbolic grammar or symbolic semanticcomponent. In contrast, we describe a so-called screening approach for learningrobust processing of spontaneously spoken language. A screening approach is aflat analysis which uses shallow sequences of category representations foranalyzing an utterance at various syntactic, semantic and dialog levels. Ratherthan using a deeply structured symbolic analysis, we use a flat connectionistanalysis. This screening approach aims at supporting speech and languageprocessing by using (1) data-driven learning and (2) robustness ofconnectionist networks. In order to test this approach, we have developed theSCREEN system which is based on this new robust, learned and flat analysis. Inthis paper, we focus on a detailed description of SCREEN's architecture, theflat syntactic and semantic analysis, the interaction with a speech recognizer,and a detailed evaluation analysis of the robustness under the influence ofnoisy or incomplete input. The main result of this paper is that flatrepresentations allow more robust processing of spontaneous spoken languagethan deeply structured representations. In particular, we show how thefault-tolerance and learning capability of connectionist networks can support aflat analysis for providing more robust spoken-language processing within anoverall hybrid symbolic/connectionist framework.;https://arxiv.org/abs/cs/9701102
arXiv:cs/9612102;[cs.AI];Computer Science > Artificial Intelligence;Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer;[Submitted on 1 Dec 1996];J. C. Schlimmer;P. C. Wells;;;;Efficiently entering information into a computer is key to enjoying thebenefits of computing. This paper describes three intelligent user interfaces:handwriting recognition, adaptive menus, and predictive fillin. In the contextof adding a personUs name and address to an electronic organizer, tests showhandwriting recognition is slower than typing on an on-screen, soft keyboard,while adaptive menus and predictive fillin can be twice as fast. This paperalso presents strategies for applying these three interfaces to otherinformation collection domains.;https://arxiv.org/abs/cs/9612102
arXiv:cs/9608103;[cs.AI];Computer Science > Artificial Intelligence;Spatial Aggregation: Theory and Applications;[Submitted on 1 Aug 1996];K. Yip;F. Zhao;;;;Visual thinking plays an important role in scientific reasoning. Based on theresearch in automating diverse reasoning tasks about dynamical systems,nonlinear controllers, kinematic mechanisms, and fluid motion, we haveidentified a style of visual thinking, imagistic reasoning. Imagistic reasoningorganizes computations around image-like, analogue representations so thatperceptual and symbolic operations can be brought to bear to infer structureand behavior. Programs incorporating imagistic reasoning have been shown toperform at an expert level in domains that defy current analytic or numericalmethods. We have developed a computational paradigm, spatial aggregation, tounify the description of a class of imagistic problem solvers. A programwritten in this paradigm has the following properties. It takes a continuousfield and optional objective functions as input, and produces high-leveldescriptions of structure, behavior, or control actions. It computes amulti-layer of intermediate representations, called spatial aggregates, byforming equivalence classes and adjacency relations. It employs a small set ofgeneric operators such as aggregation, classification, and localization toperform bidirectional mapping between the information-rich field andsuccessively more abstract spatial aggregates. It uses a data structure, theneighborhood graph, as a common interface to modularize computations. Toillustrate our theory, we describe the computational structure of threeimplemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of thespatial aggregation generic operators by mixing and matching a library ofcommonly used routines.;https://arxiv.org/abs/cs/9608103
arXiv:cs/9612101;[cs.AI];Computer Science > Artificial Intelligence;Exploiting Causal Independence in Bayesian Network Inference;[Submitted on 1 Dec 1996];N. L. Zhang;D. Poole;;;;A new method is proposed for exploiting causal independencies in exactBayesian network inference. A Bayesian network can be viewed as representing afactorization of a joint probability into the multiplication of a set ofconditional probabilities. We present a notion of causal independence thatenables one to further factorize the conditional probabilities into acombination of even smaller factors and consequently obtain a finer-grainfactorization of the joint probability. The new formulation of causalindependence lets us specify the conditional probability of a variable givenits parents in terms of an associative and commutative operator, such as``or'', ``sum'' or ``max'', on the contribution of each parent. We start with asimple algorithm VE for Bayesian network inference that, given evidence and aquery variable, uses the factorization to find the posterior distribution ofthe query. We show how this algorithm can be extended to exploit causalindependence. Empirical studies, based on the CPCS networks for medicaldiagnosis, show that this method is more efficient than previous methods andallows for inference in larger networks than previous algorithms.;https://arxiv.org/abs/cs/9612101
arXiv:cs/9604101;[cs.AI];Computer Science > Artificial Intelligence;A Divergence Critic for Inductive Proof;[Submitted on 1 Apr 1996];T. Walsh;;;;;Inductive theorem provers often diverge. This paper describes a simplecritic, a computer program which monitors the construction of inductive proofsattempting to identify diverging proof attempts. Divergence is recognized bymeans of a ``difference matching'' procedure. The critic then proposes lemmasand generalizations which ``ripple'' these differences away so that the proofcan go through without divergence. The critic enables the theorem prover Spiketo prove many theorems completely automatically from the definitions alone.;https://arxiv.org/abs/cs/9604101
arXiv:cs/9605106;[cs.AI];Computer Science > Artificial Intelligence;2Planning for Contingencies: A Decision-based Approach;[Submitted on 1 May 1996];L. Pryor;G. Collins;;;;A fundamental assumption made by classical AI planners is that there is nouncertainty in the world: the planner has full knowledge of the conditionsunder which the plan will be executed and the outcome of every action is fullypredictable. These planners cannot therefore construct contingency plans, i.e.,plans in which different actions are performed in different circumstances. Inthis paper we discuss some issues that arise in the representation andconstruction of contingency plans and describe Cassandra, a partial-ordercontingency planner. Cassandra uses explicit decision-steps that enable theagent executing the plan to decide which plan branch to follow. Thedecision-steps in a plan result in subgoals to acquire knowledge, which areplanned for in the same way as any other subgoals. Cassandra thus distinguishesthe process of gathering information from the process of making decisions. Theexplicit representation of decisions in Cassandra allows a coherent approach tothe problems of contingent planning, and provides a solid base for extensionssuch as the use of different decision-making procedures.;https://arxiv.org/abs/cs/9605106
arXiv:cs/9605105;[cs.AI];Computer Science > Artificial Intelligence;A Formal Framework for Speedup Learning from Problems and Solutions;[Submitted on 1 May 1996];P. Tadepalli;B. K. Natarajan;;;;Speedup learning seeks to improve the computational efficiency of problemsolving with experience. In this paper, we develop a formal framework forlearning efficient problem solving from random problems and their solutions. Weapply this framework to two different representations of learned knowledge,namely control rules and macro-operators, and prove theorems that identifysufficient conditions for learning in each representation. Our proofs areconstructive in that they are accompanied with learning algorithms. Ourframework captures both empirical and explanation-based speedup learning in aunified fashion. We illustrate our framework with implementations in twodomains: symbolic integration and Eight Puzzle. This work integrates manystrands of experimental and theoretical work in machine learning, includingempirical learning of control rules, macro-operator learning, Explanation-BasedLearning (EBL), and Probably Approximately Correct (PAC) Learning.;https://arxiv.org/abs/cs/9605105
arXiv:cs/9605104;[cs.AI];Computer Science > Artificial Intelligence;Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study;[Submitted on 1 May 1996];J. Gratch;S. Chien;;;;Although most scheduling problems are NP-hard, domain specific techniquesperform well in practice but are quite expensive to construct. In adaptiveproblem-solving solving, domain specific knowledge is acquired automaticallyfor a general problem solver with a flexible control architecture. In thisapproach, a learning system explores a space of possible heuristic methods forone well-suited to the eccentricities of the given domain and problemdistribution. In this article, we discuss an application of the approach toscheduling satellite communications. Using problem distributions based onactual mission requirements, our approach identifies strategies that not onlydecrease the amount of CPU time required to produce schedules, but alsoincrease the percentage of problems that are solvable within computationalresource limitations.;https://arxiv.org/abs/cs/9605104
arXiv:cs/9605103;[cs.AI];Computer Science > Artificial Intelligence;Reinforcement Learning: A Survey;[Submitted on 1 May 1996];L. P. Kaelbling;M. L. Littman;A. W. Moore;;;This paper surveys the field of reinforcement learning from acomputer-science perspective. It is written to be accessible to researchersfamiliar with machine learning. Both the historical basis of the field and abroad selection of current work are summarized. Reinforcement learning is theproblem faced by an agent that learns behavior through trial-and-errorinteractions with a dynamic environment. The work described here has aresemblance to work in psychology, but differs considerably in the details andin the use of the word ``reinforcement.'' The paper discusses central issues ofreinforcement learning, including trading off exploration and exploitation,establishing the foundations of the field via Markov decision theory, learningfrom delayed reinforcement, constructing empirical models to acceleratelearning, making use of generalization and hierarchy, and coping with hiddenstate. It concludes with a survey of some implemented systems and an assessmentof the practical utility of current methods for reinforcement learning.;https://arxiv.org/abs/cs/9605103
arXiv:cs/9605102;[cs.AI];Computer Science > Artificial Intelligence;Least Generalizations and Greatest Specializations of Sets of Clauses;[Submitted on 1 May 1996];S. H. Nienhuys-Cheng;R. deWolf;;;;The main operations in Inductive Logic Programming (ILP) are generalizationand specialization, which only make sense in a generality order. In ILP, thethree most important generality orders are subsumption, implication andimplication relative to background knowledge. The two languages used most oftenare languages of clauses and languages of only Horn clauses. This gives a totalof six different ordered languages. In this paper, we give a systematictreatment of the existence or non-existence of least generalizations andgreatest specializations of finite sets of clauses in each of these six orderedsets. We survey results already obtained by others and also contribute someanswers of our own. Our main new results are, firstly, the existence of acomputable least generalization under implication of every finite set ofclauses containing at least one non-tautologous function-free clause (amongother, not necessarily function-free clauses). Secondly, we show that such aleast generalization need not exist under relative implication, not even ifboth the set that is to be generalized and the background knowledge arefunction-free. Thirdly, we give a complete discussion of existence andnon-existence of greatest specializations in each of the six ordered languages.;https://arxiv.org/abs/cs/9605102
arXiv:cs/9605101;[cs.AI];Computer Science > Artificial Intelligence;Further Experimental Evidence against the Utility of Occam's Razor;[Submitted on 1 May 1996];G. I. Webb;;;;;This paper presents new experimental evidence against the utility of Occam'srazor. A~systematic procedure is presented for post-processing decision treesproduced by C4.5. This procedure was derived by rejecting Occam's razor andinstead attending to the assumption that similar objects are likely to belongto the same class. It increases a decision tree's complexity without alteringthe performance of that tree on the training data from which it is inferred.The resulting more complex decision trees are demonstrated to have, on average,for a variety of common learning tasks, higher predictive accuracy than theless complex original decision trees. This result raises considerable doubtabout the utility of Occam's razor as it is commonly applied in modern machinelearning.;https://arxiv.org/abs/cs/9605101
arXiv:cs/9603104;[cs.AI];Computer Science > Artificial Intelligence;Active Learning with Statistical Models;[Submitted on 1 Mar 1996];D. A. Cohn;Z. Ghahramani;M. I. Jordan;;;For many types of machine learning algorithms, one can compute thestatistically `optimal' way to select training data. In this paper, we reviewhow optimal data selection techniques have been used with feedforward neuralnetworks. We then show how the same principles may be used to select data fortwo alternative, statistically-based learning architectures: mixtures ofGaussians and locally weighted regression. While the techniques for neuralnetworks are computationally expensive and approximate, the techniques formixtures of Gaussians and locally weighted regression are both efficient andaccurate. Empirically, we observe that the optimality criterion sharplydecreases the number of training examples the learner needs in order to achievegood performance.;https://arxiv.org/abs/cs/9603104
arXiv:cs/9603103;[cs.AI];Computer Science > Artificial Intelligence;Improved Use of Continuous Attributes in C4.5;[Submitted on 1 Mar 1996];J. R. Quinlan;;;;;A reported weakness of C4.5 in domains with continuous attributes isaddressed by modifying the formation and evaluation of tests on continuousattributes. An MDL-inspired penalty is applied to such tests, eliminating someof them from consideration and altering the relative desirability of all tests.Empirical trials show that the modifications lead to smaller decision treeswith higher predictive accuracies. Results also confirm that a new version ofC4.5 incorporating these changes is superior to recent approaches that useglobal discretization and that construct small trees with multi-intervalsplits.;https://arxiv.org/abs/cs/9603103
arXiv:cs/9603102;[cs.AI];Computer Science > Artificial Intelligence;Mean Field Theory for Sigmoid Belief Networks;[Submitted on 1 Mar 1996];L. K. Saul;T. Jaakkola;M. I. Jordan;;;"We develop a mean field theory for sigmoid belief networks based on ideasfrom statistical mechanics. Our mean field theory provides a tractableapproximation to the true probability distribution in these networks; it alsoyields a lower bound on the likelihood of evidence. We demonstrate the utilityof this framework on a benchmark problem in statistical patternrecognition---the classification of handwritten digits.";https://arxiv.org/abs/cs/9603102
arXiv:cs/9603101;[cs.AI];Computer Science > Artificial Intelligence;Quantum Computing and Phase Transitions in Combinatorial Search;[Submitted on 1 Mar 1996];T. Hogg;;;;;We introduce an algorithm for combinatorial search on quantum computers thatis capable of significantly concentrating amplitude into solutions for some NPsearch problems, on average. This is done by exploiting the same aspects ofproblem structure as used by classical backtrack methods to avoid unproductivesearch choices. This quantum algorithm is much more likely to find solutionsthan the simple direct use of quantum parallelism. Furthermore, empiricalevaluation on small problems shows this quantum algorithm displays the samephase transition behavior, and at the same location, as seen in many previouslystudied classical search methods. Specifically, difficult problem instances areconcentrated near the abrupt change from underconstrained to overconstrainedproblems.;https://arxiv.org/abs/cs/9603101
arXiv:cs/9602102;[cs.AI];Computer Science > Artificial Intelligence;Logarithmic-Time Updates and Queries in Probabilistic Networks;[Submitted on 1 Feb 1996];A. L. Delcher;A. J. Grove;S. Kasif;J. Pearl;;Traditional databases commonly support efficient query and update proceduresthat operate in time which is sublinear in the size of the database. Our goalin this paper is to take a first step toward dynamic reasoning in probabilisticdatabases with comparable efficiency. We propose a dynamic data structure thatsupports efficient algorithms for updating and querying singly connectedBayesian networks. In the conventional algorithm, new evidence is absorbed inO(1) time and queries are processed in time O(N), where N is the size of thenetwork. We propose an algorithm which, after a preprocessing phase, allows usto answer queries in time O(log N) at the expense of O(log N) time per evidenceabsorption. The usefulness of sub-linear processing time manifests itself inapplications requiring (near) real-time response over large probabilisticdatabases. We briefly discuss a potential application of dynamic probabilisticreasoning in computational biology.;https://arxiv.org/abs/cs/9602102
arXiv:cs/9602101;[cs.AI];Computer Science > Artificial Intelligence;Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences;[Submitted on 1 Feb 1996];G. Brewka;;;;;The paper describes an extension of well-founded semantics for logic programswith two types of negation. In this extension information about preferencesbetween rules can be expressed in the logical language and derived dynamically.This is achieved by using a reserved predicate symbol and a naming technique.Conflicts among rules are resolved whenever possible on the basis of derivedpreference information. The well-founded conclusions of prioritized logicprograms can be computed in polynomial time. A legal reasoning exampleillustrates the usefulness of the approach.;https://arxiv.org/abs/cs/9602101
arXiv:cs/9512102;[cs.AI];Computer Science > Artificial Intelligence;Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach;[Submitted on 1 Dec 1995];A. Broggi;S. Berte;;;;"The main aim of this work is the development of a vision-based road detectionsystem fast enough to cope with the difficult real-time constraints imposed bymoving vehicle applications. The hardware platform, a special-purpose massivelyparallel system, has been chosen to minimize system production and operationalcosts. This paper presents a novel approach to expectation-driven low-levelimage segmentation, which can be mapped naturally onto mesh-connected massivelyparallel SIMD architectures capable of handling hierarchical data structures.The input image is assumed to contain a distorted version of a given template;a multiresolution stretching process is used to reshape the original templatein accordance with the acquired image content, minimizing a potential function.The distorted template is the process output.";https://arxiv.org/abs/cs/9512102
arXiv:cs/9512103;[cs.AI];Computer Science > Artificial Intelligence;Generalization of Clauses under Implication;[Submitted on 1 Dec 1995];P. Idestam-Almquist;;;;;In the area of inductive learning, generalization is a main operation, andthe usual definition of induction is based on logical implication. Recentlythere has been a rising interest in clausal representation of knowledge inmachine learning. Almost all inductive learning systems that performgeneralization of clauses use the relation theta-subsumption instead ofimplication. The main reason is that there is a well-known and simple techniqueto compute least general generalizations under theta-subsumption, but not underimplication. However generalization under theta-subsumption is inappropriatefor learning recursive clauses, which is a crucial problem since recursion isthe basic program structure of logic programs. We note that implication betweenclauses is undecidable, and we therefore introduce a stronger form ofimplication, called T-implication, which is decidable between clauses. We showthat for every finite set of clauses there exists a least generalgeneralization under T-implication. We describe a technique to reducegeneralizations under implication of a clause to generalizations undertheta-subsumption of what we call an expansion of the original clause. Moreoverwe show that for every non-tautological clause there exists a T-completeexpansion, which means that every generalization under T-implication of theclause is reduced to a generalization under theta-subsumption of the expansion.;https://arxiv.org/abs/cs/9512103
arXiv:cs/9512101;[cs.AI];Computer Science > Artificial Intelligence;OPUS: An Efficient Admissible Algorithm for Unordered Search;[Submitted on 1 Dec 1995];G. I. Webb;;;;;OPUS is a branch and bound search algorithm that enables efficient admissiblesearch through spaces for which the order of search operator application is notsignificant. The algorithm's search efficiency is demonstrated with respect tovery large machine learning search spaces. The use of admissible search is ofpotential value to the machine learning community as it means that the exactlearning biases to be employed for complex learning tasks can be preciselyspecified and manipulated. OPUS also has potential for application in otherareas of artificial intelligence, notably, truth maintenance.;https://arxiv.org/abs/cs/9512101
arXiv:cs/9601101;[cs.AI];Computer Science > Artificial Intelligence;The Design and Experimental Analysis of Algorithms for Temporal Reasoning;[Submitted on 1 Jan 1996];P. vanBeek;D. W. Manchak;;;;Many applications -- from planning and scheduling to problems in molecularbiology -- rely heavily on a temporal reasoning component. In this paper, wediscuss the design and empirical analysis of algorithms for a temporalreasoning system based on Allen's influential interval-based framework forrepresenting temporal information. At the core of the system are algorithms fordetermining whether the temporal information is consistent, and, if so, findingone or more scenarios that are consistent with the temporal information. Twoimportant algorithms for these tasks are a path consistency algorithm and abacktracking algorithm. For the path consistency algorithm, we developtechniques that can result in up to a ten-fold speedup over an already highlyoptimized implementation. For the backtracking algorithm, we develop variableand value ordering heuristics that are shown empirically to dramaticallyimprove the performance of the algorithm. As well, we show that a previouslysuggested reformulation of the backtracking search problem can reduce the timeand space requirements of the backtracking search. Taken together, thetechniques we develop allow a temporal reasoning component to solve problemsthat are of practical size.;https://arxiv.org/abs/cs/9601101
arXiv:cs/9510102;[cs.AI];Computer Science > Artificial Intelligence;Improving Connectionist Energy Minimization;[Submitted on 1 Oct 1995];G. Pinkas;R. Dechter;;;;Symmetric networks designed for energy minimization such as Boltzman machinesand Hopfield nets are frequently investigated for use in optimization,constraint satisfaction and approximation of NP-hard problems. Nevertheless,finding a global solution (i.e., a global minimum for the energy function) isnot guaranteed and even a local solution may take an exponential number ofsteps. We propose an improvement to the standard local activation function usedfor such networks. The improved algorithm guarantees that a global minimum isfound in linear time for tree-like subnetworks. The algorithm, called activate,is uniform and does not assume that the network is tree-like. It can identifytree-like subnetworks even in cyclic topologies (arbitrary networks) and avoidlocal minima along these trees. For acyclic networks, the algorithm isguaranteed to converge to a global minimum from any initial state of the system(self-stabilization) and remains correct under various types of schedulers. Onthe negative side, we show that in the presence of cycles, no uniform algorithmexists that guarantees optimality even under a sequential asynchronousscheduler. An asynchronous scheduler can activate only one unit at a time whilea synchronous scheduler can activate any number of units in a single time step.In addition, no uniform algorithm exists to optimize even acyclic networks whenthe scheduler is synchronous. Finally, we show how the algorithm can beimproved using the cycle-cutset scheme. The general algorithm, calledactivate-with-cutset, improves over activate and has some performanceguarantees that are related to the size of the network's cycle-cutset.;https://arxiv.org/abs/cs/9510102
arXiv:cs/9510101;[cs.AI];Computer Science > Artificial Intelligence;Diffusion of Context and Credit Information in Markovian Models;[Submitted on 1 Oct 1995];Y. Bengio;P. Frasconi;;;;This paper studies the problem of ergodicity of transition probabilitymatrices in Markovian models, such as hidden Markov models (HMMs), and how itmakes very difficult the task of learning to represent long-term context forsequential data. This phenomenon hurts the forward propagation of long-termcontext information, as well as learning a hidden state representation torepresent long-term context, which depends on propagating credit informationbackwards in time. Using results from Markov chain theory, we show that thisproblem of diffusion of context and credit is reduced when the transitionprobabilities approach 0 or 1, i.e., the transition probability matrices aresparse and the model essentially deterministic. The results found in this paperapply to learning approaches based on continuous optimization, such as gradientdescent and the Baum-Welch algorithm.;https://arxiv.org/abs/cs/9510101
arXiv:cs/9507101;[cs.AI];Computer Science > Artificial Intelligence;Building and Refining Abstract Planning Cases by Change of Representation Language;[Submitted on 1 Jul 1995];R. Bergmann;W. Wilke;;;;ion is one of the most promising approaches to improve the performance ofproblem solvers. In several domains abstraction by dropping sentences of adomain description -- as used in most hierarchical planners -- has provenuseful. In this paper we present examples which illustrate significantdrawbacks of abstraction by dropping sentences. To overcome these drawbacks, wepropose a more general view of abstraction involving the change ofrepresentation language. We have developed a new abstraction methodology and arelated sound and complete learning algorithm that allows the complete changeof representation language of planning cases from concrete to abstract.However, to achieve a powerful change of the representation language, theabstract language itself as well as rules which describe admissible ways ofabstracting states must be provided in the domain model. This new abstractionapproach is the core of Paris (Plan Abstraction and Refinement in an IntegratedSystem), a system in which abstract planning cases are automatically learnedfrom given concrete cases. An empirical study in the domain of process planningin mechanical engineering shows significant advantages of the proposedreasoning from abstract cases over classical hierarchical planning.;https://arxiv.org/abs/cs/9507101
arXiv:cs/9506102;[cs.AI];Computer Science > Artificial Intelligence;Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs;[Submitted on 1 Jun 1995];R. J. Mooney;M. E. Califf;;;;This paper presents a method for inducing logic programs from examples thatlearns a new class of concepts called first-order decision lists, defined asordered lists of clauses each ending in a cut. The method, called FOIDL, isbased on FOIL (Quinlan, 1990) but employs intensional background knowledge andavoids the need for explicit negative examples. It is particularly useful forproblems that involve rules with specific exceptions, such as learning thepast-tense of English verbs, a task widely studied in the context of thesymbolic/connectionist debate. FOIDL is able to learn concise, accurateprograms for this problem from significantly fewer examples than previousmethods (both connectionist and symbolic).;https://arxiv.org/abs/cs/9506102
arXiv:cs/9506101;[cs.AI];Computer Science > Artificial Intelligence;FLECS: Planning with a Flexible Commitment Strategy;[Submitted on 1 Jun 1995];M. Veloso;P. Stone;;;;"There has been evidence that least-commitment planners can efficiently handleplanning problems that involve difficult goal interactions. This evidence hasled to the common belief that delayed-commitment is the ""best"" possibleplanning strategy. However, we recently found evidence that eager-commitmentplanners can handle a variety of planning problems more efficiently, inparticular those with difficult operator choices. Resigned to the futility oftrying to find a universally successful planning strategy, we devised a plannerthat can be used to study which domains and problems are best for whichplanning strategies. In this article we introduce this new planning algorithm,FLECS, which uses a FLExible Commitment Strategy with respect to plan-steporderings. It is able to use any strategy from delayed-commitment toeager-commitment. The combination of delayed and eager operator-orderingcommitments allows FLECS to take advantage of the benefits of explicitly usinga simulated execution state and reasoning about planning constraints. FLECS canvary its commitment strategy across different problems and domains, and alsoduring the course of a single planning problem. FLECS represents a novelcontribution to planning in that it explicitly provides the choice of whichcommitment strategy to use while planning. FLECS provides a framework toinvestigate the mapping from planning domains and problems to efficientplanning strategies.";https://arxiv.org/abs/cs/9506101
arXiv:cs/9703101;[cs.AI];Computer Science > Artificial Intelligence;A Uniform Framework for Concept Definitions in Description Logics;[Submitted on 1 Mar 1997];G. DeGiacomo;M. Lenzerini;;;;Most modern formalisms used in Databases and Artificial Intelligence fordescribing an application domain are based on the notions of class (or concept)and relationship among classes. One interesting feature of such formalisms isthe possibility of defining a class, i.e., providing a set of properties thatprecisely characterize the instances of the class. Many recent articles pointout that there are several ways of assigning a meaning to a class definitioncontaining some sort of recursion. In this paper, we argue that, instead ofchoosing a single style of semantics, we achieve better results by adopting aformalism that allows for different semantics to coexist. We demonstrate thefeasibility of our argument, by presenting a knowledge representationformalism, the description logic muALCQ, with the above characteristics. Inaddition to the constructs for conjunction, disjunction, negation, quantifiers,and qualified number restrictions, muALCQ includes special fixpoint constructsto express (suitably interpreted) recursive definitions. These constructsenable the usual frame-based descriptions to be combined with definitions ofrecursive data structures such as directed acyclic graphs, lists, streams, etc.We establish several properties of muALCQ, including the decidability and thecomputational complexity of reasoning, by formulating a correspondence with aparticular modal logic of programs called the modal mu-calculus.;https://arxiv.org/abs/cs/9703101
arXiv:cs/9704101;[cs.AI];Computer Science > Artificial Intelligence;Lifeworld Analysis;[Submitted on 1 Apr 1997];P. Agre;I. Horswill;;;;We argue that the analysis of agent/environment interactions should beextended to include the conventions and invariants maintained by agentsthroughout their activity. We refer to this thicker notion of environment as alifeworld and present a partial set of formal tools for describing structuresof lifeworlds and the ways in which they computationally simplify activity. Asone specific example, we apply the tools to the analysis of the Toast systemand show how versions of the system with very different control structures infact implement a common control structure together with different conventionsfor encoding task state in the positions or states of objects in theenvironment.;https://arxiv.org/abs/cs/9704101
arXiv:cs/9705102;[cs.AI];Computer Science > Artificial Intelligence;Connectionist Theory Refinement: Genetically Searching the Space of Network Topologies;[Submitted on 1 May 1997];D. W. Opitz;J. W. Shavlik;;;;"An algorithm that learns from a set of examples should ideally be able toexploit the available resources of (a) abundant computing power and (b)domain-specific knowledge to improve its ability to generalize. Connectionisttheory-refinement systems, which use background knowledge to select a neuralnetwork's topology and initial weights, have proven to be effective atexploiting domain-specific knowledge; however, most do not exploit availablecomputing power. This weakness occurs because they lack the ability to refinethe topology of the neural networks they produce, thereby limitinggeneralization, especially when given impoverished domain theories. We presentthe REGENT algorithm which uses (a) domain-specific knowledge to help create aninitial population of knowledge-based neural networks and (b) genetic operatorsof crossover and mutation (specifically designed for knowledge-based networks)to continually search for better network topologies. Experiments on threereal-world domains indicate that our new algorithm is able to significantlyincrease generalization compared to a standard connectionist theory-refinementsystem, as well as our previous algorithm for growing knowledge-based networks.";https://arxiv.org/abs/cs/9705102
arXiv:cs/9707103;[cs.AI];Computer Science > Artificial Intelligence;Defining Relative Likelihood in Partially-Ordered Preferential Structures;[Submitted on 1 Jul 1997];J. Y. Halpern;;;;;Starting with a likelihood or preference order on worlds, we extend it to alikelihood ordering on sets of worlds in a natural way, and examine theresulting logic. Lewis earlier considered such a notion of relative likelihoodin the context of studying counterfactuals, but he assumed a total preferenceorder on worlds. Complications arise when examining partial orders that are notpresent for total orders. There are subtleties involving the exact approach tolifting the order on worlds to an order on sets of worlds. In addition, theaxiomatization of the logic of relative likelihood in the case of partialorders gives insight into the connection between relative likelihood anddefault reasoning.;https://arxiv.org/abs/cs/9707103
arXiv:cs/9709102;[cs.AI];Computer Science > Artificial Intelligence;Identifying Hierarchical Structure in Sequences: A linear-time algorithm;[Submitted on 1 Sep 1997];C. G. Nevill-Manning;I. H. Witten;;;;SEQUITUR is an algorithm that infers a hierarchical structure from a sequenceof discrete symbols by replacing repeated phrases with a grammatical rule thatgenerates the phrase, and continuing this process recursively. The result is ahierarchical representation of the original sequence, which offers insightsinto its lexical structure. The algorithm is driven by two constraints thatreduce the size of the grammar, and produce structure as a by-product. SEQUITURbreaks new ground by operating incrementally. Moreover, the method's simplestructure permits a proof that it operates in space and time that is linear inthe size of the input. Our implementation can process 50,000 symbols per secondand has been applied to an extensive range of real world sequences.;https://arxiv.org/abs/cs/9709102
arXiv:cs/9710101;[cs.AI];Computer Science > Artificial Intelligence;Analysis of Three-Dimensional Protein Images;[Submitted on 1 Oct 1997];L. Leherte;J. Glasgow;K. Baxter;E. Steeg;S. Fortier;A fundamental goal of research in molecular biology is to understand proteinstructure. Protein crystallography is currently the most successful method fordetermining the three-dimensional (3D) conformation of a protein, yet itremains labor intensive and relies on an expert's ability to derive andevaluate a protein scene model. In this paper, the problem of protein structuredetermination is formulated as an exercise in scene analysis. A computationalmethodology is presented in which a 3D image of a protein is segmented into agraph of critical points. Bayesian and certainty factor approaches aredescribed and used to analyze critical point graphs and identify meaningfulsubstructures, such as alpha-helices and beta-sheets. Results of applying themethodologies to protein images at low and medium resolution are reported. Theresearch is related to approaches to representation, segmentation andclassification in vision, as well as to top-down approaches to proteinstructure prediction.;https://arxiv.org/abs/cs/9710101
arXiv:cs/9711104;[cs.AI];Computer Science > Artificial Intelligence;Dynamic Non-Bayesian Decision Making;[Submitted on 1 Nov 1997];D. Monderer;M. Tennenholtz;;;;The model of a non-Bayesian agent who faces a repeated game with incompleteinformation against Nature is an appropriate tool for modeling generalagent-environment interactions. In such a model the environment state(controlled by Nature) may change arbitrarily, and the feedback/reward functionis initially unknown. The agent is not Bayesian, that is he does not form aprior probability neither on the state selection strategy of Nature, nor on hisreward function. A policy for the agent is a function which assigns an actionto every history of observations and actions. Two basic feedback structures areconsidered. In one of them -- the perfect monitoring case -- the agent is ableto observe the previous environment state as part of his feedback, while in theother -- the imperfect monitoring case -- all that is available to the agent isthe reward obtained. Both of these settings refer to partially observableprocesses, where the current environment state is unknown. Our main resultrefers to the competitive ratio criterion in the perfect monitoring case. Weprove the existence of an efficient stochastic policy that ensures that thecompetitive ratio is obtained at almost all stages with an arbitrarily highprobability, where efficiency is measured in terms of rate of convergence. Itis further shown that such an optimal policy does not exist in the imperfectmonitoring case. Moreover, it is proved that in the perfect monitoring casethere does not exist a deterministic policy that satisfies our long runoptimality criterion. In addition, we discuss the maxmin criterion and provethat a deterministic efficient optimal strategy does exist in the imperfectmonitoring case under this criterion. Finally we show that our approach tolong-run optimality can be viewed as qualitative, which distinguishes it fromprevious work in this area.;https://arxiv.org/abs/cs/9711104
arXiv:cs/9712102;[cs.AI];Computer Science > Artificial Intelligence;Bidirectional Heuristic Search Reconsidered;[Submitted on 1 Dec 1997];H. Kaindl;G. Kainz;;;;The assessment of bidirectional heuristic search has been incorrect since itwas first published more than a quarter of a century ago. For quite a longtime, this search strategy did not achieve the expected results, and there wasa major misunderstanding about the reasons behind it. Although there is stillwide-spread belief that bidirectional heuristic search is afflicted by theproblem of search frontiers passing each other, we demonstrate that thisconjecture is wrong. Based on this finding, we present both a new genericapproach to bidirectional heuristic search and a new approach to dynamicallyimproving heuristic values that is feasible in bidirectional search only. Theseapproaches are put into perspective with both the traditional and more recentlyproposed approaches in order to facilitate a better overall understanding.Empirical results of experiments with our new approaches show thatbidirectional heuristic search can be performed very efficiently and also withlimited memory. These results suggest that bidirectional heuristic searchappears to be better for solving certain difficult problems than correspondingunidirectional search. This provides some evidence for the usefulness of asearch strategy that was long neglected. In summary, we show that bidirectionalheuristic search is viable and consequently propose that it be reconsidered.;https://arxiv.org/abs/cs/9712102
arXiv:cs/9712101;[cs.AI];Computer Science > Artificial Intelligence;When Gravity Fails: Local Search Topology;[Submitted on 1 Dec 1997];J. Frank;P. Cheeseman;J. Stutz;;;"Local search algorithms for combinatorial search problems frequentlyencounter a sequence of states in which it is impossible to improve the valueof the objective function; moves through these regions, called plateau moves,dominate the time spent in local search. We analyze and characterize plateausfor three different classes of randomly generated Boolean Satisfiabilityproblems. We identify several interesting features of plateaus that impact theperformance of local search algorithms. We show that local minima tend to besmall but occasionally may be very large. We also show that local minima can beescaped without unsatisfying a large number of clauses, but that systematicallysearching for an escape route may be computationally expensive if the localminimum is large. We show that plateaus with exits, called benches, tend to bemuch larger than minima, and that some benches have very few exit states whichlocal search can use to escape. We show that the solutions (i.e., globalminima) of randomly generated problem instances form clusters, which behavesimilarly to local minima. We revisit several enhancements of local searchalgorithms and explain their performance in light of our results. Finally wediscuss strategies for creating the next generation of local search algorithms.";https://arxiv.org/abs/cs/9712101
arXiv:cs/9711103;[cs.AI];Computer Science > Artificial Intelligence;A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains;[Submitted on 1 Nov 1997];N. L. Zhang;W. Liu;;;;Partially observable Markov decision processes (POMDPs) are a natural modelfor planning problems where effects of actions are nondeterministic and thestate of the world is not completely observable. It is difficult to solvePOMDPs exactly. This paper proposes a new approximation scheme. The basic ideais to transform a POMDP into another one where additional information isprovided by an oracle. The oracle informs the planning agent that the currentstate of the world is in a certain region. The transformed POMDP isconsequently said to be region observable. It is easier to solve than theoriginal POMDP. We propose to solve the transformed POMDP and use its optimalpolicy to construct an approximate policy for the original POMDP. Bycontrolling the amount of additional information that the oracle provides, itis possible to find a proper tradeoff between computational time andapproximation quality. In terms of algorithmic contributions, we study indetails how to exploit region observability in solving the transformed POMDP.To facilitate the study, we also propose a new exact algorithm for generalPOMDPs. The algorithm is conceptually simple and yet is significantly moreefficient than all previous exact algorithms.;https://arxiv.org/abs/cs/9711103
arXiv:cs/9711102;[cs.AI];Computer Science > Artificial Intelligence;Storing and Indexing Plan Derivations through Explanation-based Analysis of Retrieval Failures;[Submitted on 1 Nov 1997];L. H. Ihrig;S. Kambhampati;;;;Case-Based Planning (CBP) provides a way of scaling up domain-independentplanning to solve large problems in complex domains. It replaces the detailedand lengthy search for a solution with the retrieval and adaptation of previousplanning experiences. In general, CBP has been demonstrated to improveperformance over generative (from-scratch) planning. However, the performanceimprovements it provides are dependent on adequate judgements as to problemsimilarity. In particular, although CBP may substantially reduce planningeffort overall, it is subject to a mis-retrieval problem. The success of CBPdepends on these retrieval errors being relatively rare. This paper describesthe design and implementation of a replay framework for the case-based plannerDERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporatingexplanation-based learning techniques that allow it to explain and learn fromthe retrieval failures it encounters. These techniques are used to refinejudgements about case similarity in response to feedback when a wrong decisionhas been made. The same failure analysis is used in building the case library,through the addition of repairing cases. Large problems are split and stored assingle goal subproblems. Multi-goal problems are stored only when these smallercases fail to be merged into a full solution. An empirical evaluation of thisapproach demonstrates the advantage of learning from experienced retrievalfailure.;https://arxiv.org/abs/cs/9711102
arXiv:cs/9803103;[cs.AI];Computer Science > Artificial Intelligence;Tractability of Theory Patching;[Submitted on 1 Mar 1998];S. Argamon-Engelson;M. Koppel;;;;In this paper we consider the problem of `theory patching', in which we aregiven a domain theory, some of whose components are indicated to be possiblyflawed, and a set of labeled training examples for the domain concept. Thetheory patching problem is to revise only the indicated components of thetheory, such that the resulting theory correctly classifies all the trainingexamples. Theory patching is thus a type of theory revision in which revisionsare made to individual components of the theory. Our concern in this paper isto determine for which classes of logical domain theories the theory patchingproblem is tractable. We consider both propositional and first-order domaintheories, and show that the theory patching problem is equivalent to that ofdetermining what information contained in a theory is `stable' regardless ofwhat revisions might be performed to the theory. We show that determiningstability is tractable if the input theory satisfies two conditions: thatrevisions to each theory component have monotonic effects on the classificationof examples, and that theory components act independently in the classificationof examples in the theory. We also show how the concepts introduced can be usedto determine the soundness and completeness of particular theory patchingalgorithms.;https://arxiv.org/abs/cs/9803103
arXiv:cs/9805101;[cs.AI];Computer Science > Artificial Intelligence;Integrative Windowing;[Submitted on 1 May 1998];J. Frnkranz;;;;;In this paper we re-investigate windowing for rule learning algorithms. Weshow that, contrary to previous results for decision tree learning, windowingcan in fact achieve significant run-time gains in noise-free domains andexplain the different behavior of rule learning algorithms by the fact thatthey learn each rule independently. The main contribution of this paper isintegrative windowing, a new type of algorithm that further exploits thisproperty by integrating good rules into the final theory right after they havebeen discovered. Thus it avoids re-learning these rules in subsequentiterations of the windowing process. Experimental evidence in a variety ofnoise-free domains shows that integrative windowing can in fact achievesubstantial run-time gains. Furthermore, we discuss the problem of noise inwindowing and present an algorithm that is able to achieve run-time gains in aset of experiments in a simple domain with artificial noise.;https://arxiv.org/abs/cs/9805101
arXiv:cs/9806102;[cs.AI];Computer Science > Artificial Intelligence;A Selective Macro-learning Algorithm and its Application to the NxN Sliding-Tile Puzzle;[Submitted on 1 Jun 1998];L. Finkelstein;S. Markovitch;;;;One of the most common mechanisms used for speeding up problem solvers ismacro-learning. Macros are sequences of basic operators acquired during problemsolving. Macros are used by the problem solver as if they were basic operators.The major problem that macro-learning presents is the vast number of macrosthat are available for acquisition. Macros increase the branching factor of thesearch space and can severely degrade problem-solving efficiency. To make macrolearning useful, a program must be selective in acquiring and utilizing macros.This paper describes a general method for selective acquisition of macros.Solvable training problems are generated in increasing order of difficulty. Theonly macros acquired are those that take the problem solver out of a localminimum to a better state. The utility of the method is demonstrated in severaldomains, including the domain of NxN sliding-tile puzzles. After learning onsmall puzzles, the system is able to efficiently solve puzzles of any size.;https://arxiv.org/abs/cs/9806102
arXiv:cs/9806101;[cs.AI];Computer Science > Artificial Intelligence;Model-Based Diagnosis using Structured System Descriptions;[Submitted on 1 Jun 1998];A. Darwiche;;;;;This paper presents a comprehensive approach for model-based diagnosis whichincludes proposals for characterizing and computing preferred diagnoses,assuming that the system description is augmented with a system structure (adirected graph explicating the interconnections between system components).Specifically, we first introduce the notion of a consequence, which is asyntactically unconstrained propositional sentence that characterizes allconsistency-based diagnoses and show that standard characterizations ofdiagnoses, such as minimal conflicts, correspond to syntactic variations on aconsequence. Second, we propose a new syntactic variation on the consequenceknown as negation normal form (NNF) and discuss its merits compared to standardvariations. Third, we introduce a basic algorithm for computing consequences inNNF given a structured system description. We show that if the system structuredoes not contain cycles, then there is always a linear-size consequence in NNFwhich can be computed in linear time. For arbitrary system structures, we showa precise connection between the complexity of computing consequences and thetopology of the underlying system structure. Finally, we present an algorithmthat enumerates the preferred diagnoses characterized by a consequence. Thealgorithm is shown to take linear time in the size of the consequence if thepreference criterion satisfies some general conditions.;https://arxiv.org/abs/cs/9806101
arXiv:cs/9803102;[cs.AI];Computer Science > Artificial Intelligence;Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets;[Submitted on 1 Mar 1998];A. Moore;M. S. Lee;;;;This paper introduces new algorithms and data structures for quick countingfor machine learning datasets. We focus on the counting task of constructingcontingency tables, but our approach is also applicable to counting the numberof records in a dataset that match conjunctive queries. Subject to certainassumptions, the costs of these operations can be shown to be independent ofthe number of records in the dataset and loglinear in the number of non-zeroentries in the contingency table. We provide a very sparse data structure, theADtree, to minimize memory use. We provide analytical worst-case bounds forthis structure for several models of data distribution. We empiricallydemonstrate that tractably-sized data structures can be produced for largereal-world datasets by (a) using a sparse tree structure that never allocatesmemory for counts of zero, (b) never allocating memory for counts that can bededuced from other counts, and (c) not bothering to expand the tree fully nearits leaves. We show how the ADtree can be used to accelerate Bayes netstructure finding algorithms, rule learning algorithms, and feature selectionalgorithms, and we provide a number of empirical results comparing ADtreemethods against traditional direct counting approaches. We also discuss thepossible uses of ADtrees in other machine learning methods, and discuss themerits of ADtrees in comparison with alternative representations such askd-trees, R-trees and Frequent Sets.;https://arxiv.org/abs/cs/9803102
arXiv:cs/9803101;[cs.AI];Computer Science > Artificial Intelligence;Synthesizing Customized Planners from Specifications;[Submitted on 1 Mar 1998];B. Srivastava;S. Kambhampati;;;;Existing plan synthesis approaches in artificial intelligence fall into twocategories -- domain independent and domain dependent. The domain independentapproaches are applicable across a variety of domains, but may not be veryefficient in any one given domain. The domain dependent approaches need to be(re)designed for each domain separately, but can be very efficient in thedomain for which they are designed. One enticing alternative to theseapproaches is to automatically synthesize domain independent planners given theknowledge about the domain and the theory of planning. In this paper, weinvestigate the feasibility of using existing automated software synthesistools to support such synthesis. Specifically, we describe an architecturecalled CLAY in which the Kestrel Interactive Development System (KIDS) is usedto derive a domain-customized planner through a semi-automatic combination of adeclarative theory of planning, and the declarative control knowledge specificto a given domain, to semi-automatically combine them to derivedomain-customized planners. We discuss what it means to write a declarativetheory of planning and control knowledge for KIDS, and illustrate our approachby generating a class of domain-specific planners using state spacerefinements. Our experiments show that the synthesized planners can outperformclassical refinement planners (implemented as instantiations of UCP,Kambhampati & Srivastava, 1995), using the same control knowledge. We willcontrast the costs and benefits of the synthesis approach with conventionalmethods for customizing domain independent planners.;https://arxiv.org/abs/cs/9803101
arXiv:cs/9801102;[cs.AI];Computer Science > Artificial Intelligence;Monotonicity and Persistence in Preferential Logics;[Submitted on 1 Jan 1998];J. Engelfriet;;;;;An important characteristic of many logics for Artificial Intelligence istheir nonmonotonicity. This means that adding a formula to the premises caninvalidate some of the consequences. There may, however, exist formulae thatcan always be safely added to the premises without destroying any of theconsequences: we say they respect monotonicity. Also, there may be formulaethat, when they are a consequence, can not be invalidated when adding anyformula to the premises: we call them conservative. We study these two classesof formulae for preferential logics, and show that they are closely linked tothe formulae whose truth-value is preserved along the (preferential) ordering.We will consider some preferential logics for illustration, and prove syntacticcharacterization results for them. The results in this paper may improve theefficiency of theorem provers for preferential logics.;https://arxiv.org/abs/cs/9801102
arXiv:cs/9801101;[cs.AI];Computer Science > Artificial Intelligence;Incremental Recompilation of Knowledge;[Submitted on 1 Jan 1998];G. Gogic;C. H. Papadimitriou;M. Sideri;;;"Approximating a general formula from above and below by Horn formulas (itsHorn envelope and Horn core, respectively) was proposed by Selman and Kautz(1991, 1996) as a form of ``knowledge compilation,'' supporting rapidapproximate reasoning; on the negative side, this scheme is static in that itsupports no updates, and has certain complexity drawbacks pointed out byKavvadias, Papadimitriou and Sideri (1993). On the other hand, the manyframeworks and schemes proposed in the literature for theory update andrevision are plagued by serious complexity-theoretic impediments, even in theHorn case, as was pointed out by Eiter and Gottlob (1992), and is furtherdemonstrated in the present paper. More fundamentally, these schemes are notinductive, in that they may lose in a single update any positive properties ofthe represented sets of formulas (small size, Horn structure, etc.). In thispaper we propose a new scheme, incremental recompilation, which combines Hornapproximation and model-based updates; this scheme is inductive and veryefficient, free of the problems facing its constituents. A set of formulas isrepresented by an upper and lower Horn approximation. To update, we replace theupper Horn formula by the Horn envelope of its minimum-change update, andsimilarly the lower one by the Horn core of its update; the key fact whichenables this scheme is that Horn envelopes and cores are easy to compute whenthe underlying formula is the result of a minimum-change update of a Hornformula by a clause. We conjecture that efficient algorithms are possible formore complex updates.";https://arxiv.org/abs/cs/9801101
arXiv:cs/9809121;[cs.IR];Computer Science > Information Retrieval;Using Local Optimality Criteria for Efficient Information Retrieval with Redundant Information Filters;[Submitted on 29 Sep 1998];Neil C. Rowe;;;;;"We consider information retrieval when the data, for instance multimedia, iscoputationally expensive to fetch. Our approach uses ""information filters"" toconsiderably narrow the universe of possiblities before retrieval. We areespecially interested in redundant information filters that save time over moregeneral but more costly filters. Efficient retrieval requires that decisionmust be made about the necessity, order, and concurrent processing of proposedfilters (an ""execution plan""). We develop simple polynomial-time local criteriafor optimal execution plans, and show that most forms of concurrency aresuboptimal with information filters. Although the general problem of finding anoptimal execution plan is likely exponential in the number of filters, we showexperimentally that our local optimality criteria, used in a polynomial-timealgorithm, nearly always find the global optimum with 15 filters or less, asufficient number of filters for most applications. Our methods do not requirespecial hardware and avoid the high processor idleness that is characteristicof massive parallelism solutions to this problem. We apply our ideas to animportant application, information retrieval of cpationed data usingnatural-language understanding, a problem for which the natural-languageprocessing can be the bottleneck if not implemented well.";https://arxiv.org/abs/cs/9809121
arXiv:cs/9809110;[cs.CL];Computer Science > Computation and Language;Similarity-Based Models of Word Cooccurrence Probabilities;[Submitted on 27 Sep 1998];Ido Dagan;Lillian Lee;Fernando C. N. Pereira;;;In many applications of natural language processing (NLP) it is necessary todetermine the likelihood of a given word combination. For example, a speechrecognizer may need to determine which of the two word combinations ``eat apeach'' and ``eat a beach'' is more likely. Statistical NLP methods determinethe likelihood of a word combination from its frequency in a training corpus.However, the nature of language is such that many word combinations areinfrequent and do not occur in any given corpus. In this work we propose amethod for estimating the probability of such previously unseen wordcombinations using available information on ``most similar'' words.;https://arxiv.org/abs/cs/9809110
arXiv:cs/9809108;[cs.MA];Computer Science > Multiagent Systems;Learning Nested Agent Models in an Information Economy;[Submitted on 26 Sep 1998];Jose M. Vidal;Edmund H. Durfee;;;;We present our approach to the problem of how an agent, within an economicMulti-Agent System, can determine when it should behave strategically (i.e.learn and use models of other agents), and when it should act as a simpleprice-taker. We provide a framework for the incremental implementation ofmodeling capabilities in agents, and a description of the forms of knowledgerequired. The agents were implemented and different populations simulated inorder to learn more about their behavior and the merits of using and learningagent models. Our results show, among other lessons, how savvy buyers can avoidbeing ``cheated'' by sellers, how price volatility can be used toquantitatively predict the benefits of deeper models, and how specific types ofagent populations influence system behavior.;https://arxiv.org/abs/cs/9809108
arXiv:cs/9809034;[cs.MA];Computer Science > Multiagent Systems;Semantics and Conversations for an Agent Communication Language;[Submitted on 18 Sep 1998];Yannis Labrou;Tim Finin;;;;We address the issues of semantics and conversations for agent communicationlanguages and the Knowledge Query Manipulation Language (KQML) in particular.Based on ideas from speech act theory, we present a semantic description forKQML that associates ``cognitive'' states of the agent with the use of thelanguage's primitives (performatives). We have used this approach to describethe semantics for the whole set of reserved KQML performatives. Building on thesemantics, we devise the conversation policies, i.e., a formal description ofhow KQML performatives may be combined into KQML exchanges (conversations),using a Definite Clause Grammar. Our research offers methods for a speech acttheory-based semantic description of a language of communication acts and forthe specification of the protocols associated with these acts. Languages ofcommunication acts address the issue of communication among softwareapplications at a level of abstraction that is useful to the emerging softwareagents paradigm.;https://arxiv.org/abs/cs/9809034
arXiv:cs/9809032;[cs.LO];Computer Science > Logic in Computer Science;Stable models and an alternative logic programming paradigm;[Submitted on 18 Sep 1998];Victor W. Marek;Miroslaw Truszczynski;;;;In this paper we reexamine the place and role of stable model semantics inlogic programming and contrast it with a least Herbrand model approach to Hornprograms. We demonstrate that inherent features of stable model semanticsnaturally lead to a logic programming system that offers an interestingalternative to more traditional logic programming styles of Horn logicprogramming, stratified logic programming and logic programming withwell-founded semantics. The proposed approach is based on the interpretation ofprogram clauses as constraints. In this setting programs do not describe asingle intended model, but a family of stable models. These stable modelsencode solutions to the constraint satisfaction problem described by theprogram. Our approach imposes restrictions on the syntax of logic programs. Inparticular, function symbols are eliminated from the language. We argue thatthe resulting logic programming system is well-attuned to problems in the classNP, has a well-defined domain of applications, and an emerging methodology ofprogramming. We point out that what makes the whole approach viable is recentprogress in implementations of algorithms to compute stable models ofpropositional logic programs.;https://arxiv.org/abs/cs/9809032
arXiv:cs/9809013;[cs.AI];Computer Science > Artificial Intelligence;Reasoning about Noisy Sensors and Effectors in the Situation Calculus;[Submitted on 9 Sep 1998];Fahiem Bacchus;Joseph Y. Halpern;Hector J. Levesque;;;Agents interacting with an incompletely known world need to be able to reasonabout the effects of their actions, and to gain further information about thatworld they need to use sensors of some sort. Unfortunately, both the effects ofactions and the information returned from sensors are subject to error. To copewith such uncertainties, the agent can maintain probabilistic beliefs about thestate of the world. With probabilistic beliefs the agent will be able toquantify the likelihood of the various outcomes of its actions and is betterable to utilize the information gathered from its error-prone actions andsensors. In this paper, we present a model in which we can reason about anagent's probabilistic degrees of belief and the manner in which these beliefschange as various actions are executed. We build on a general logical theory ofaction developed by Reiter and others, formalized in the situation calculus. Wepropose a simple axiomatization that captures an agent's state of belief andthe manner in which these beliefs change when actions are executed. Our modeldisplays a number of intuitively reasonable properties.;https://arxiv.org/abs/cs/9809013
arXiv:cs/9808001;[cs.CC];Computer Science > Computational Complexity;Chess Pure Strategies are Probably Chaotic;[Submitted on 21 Aug 1998];M. Chaves;;;;;It is odd that chess grandmasters often disagree in their analysis ofpositions, sometimes even of simple ones, and that a grandmaster can hold hisown against an powerful analytic machine such as Deep Blue. The fact that theremust exist pure winning strategies for chess is used to construct a controlstrategy function. It is then shown that chess strategy is equivalent to anautonomous system of differential equations, and conjectured that the system ischaotic. If true the conjecture would explain the forenamed peculiarities andwould also imply that there cannot exist a static evaluator for chess.;https://arxiv.org/abs/cs/9808001
arXiv:cs/9808101;[cs.AI];Computer Science > Artificial Intelligence;The Computational Complexity of Probabilistic Planning;[Submitted on 1 Aug 1998];M. L. Littman;J. Goldsmith;M. Mundhenk;;;"We examine the computational complexity of testing and finding small plans inprobabilistic planning domains with both flat and propositionalrepresentations. The complexity of plan evaluation and existence varies withthe plan type sought; we examine totally ordered plans, acyclic plans, andlooping plans, and partially ordered plans under three natural definitions ofplan value. We show that problems of interest are complete for a variety ofcomplexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In theprocess of proving that certain planning problems are complete for NP^PP, weintroduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes thestandard Boolean satisfiability problem to computations involving probabilisticquantities; our results suggest that the development of good heuristics forE-MAJSAT could be important for the creation of efficient algorithms for a widevariety of problems.";https://arxiv.org/abs/cs/9808101
arXiv:cs/9808006;[cs.AI];Computer Science > Artificial Intelligence;Set-Theoretic Completeness for Epistemic and Conditional Logic;[Submitted on 28 Aug 1998 (;Joseph Y. Halpern;;;;;The standard approach to logic in the literature in philosophy andmathematics, which has also been adopted in computer science, is to define alanguage (the syntax), an appropriate class of models together with aninterpretation of formulas in the language (the semantics), a collection ofaxioms and rules of inference characterizing reasoning (the proof theory), andthen relate the proof theory to the semantics via soundness and completenessresults. Here we consider an approach that is more common in the economicsliterature, which works purely at the semantic, set-theoretic level. We provideset-theoretic completeness results for a number of epistemic and conditionallogics, and contrast the expressive power of the syntactic and set-theoreticapproaches;https://arxiv.org/abs/cs/9808006
arXiv:cs/9808007;[cs.AI];Computer Science > Artificial Intelligence;Plausibility Measures and Default Reasoning;[Submitted on 29 Aug 1998];Nir Friedman;Joseph Y. Halpern;;;;We introduce a new approach to modeling uncertainty based on plausibilitymeasures. This approach is easily seen to generalize other approaches tomodeling uncertainty, such as probability measures, belief functions, andpossibility measures. We focus on one application of plausibility measures inthis paper: default reasoning. In recent years, a number of different semanticsfor defaults have been proposed, such as preferential structures,$\epsilon$-semantics, possibilistic structures, and $\kappa$-rankings, thathave been shown to be characterized by the same set of axioms, known as the KLMproperties. While this was viewed as a surprise, we show here that it is almostinevitable. In the framework of plausibility measures, we can give a necessarycondition for the KLM axioms to be sound, and an additional condition necessaryand sufficient to ensure that the KLM axioms are complete. This additionalcondition is so weak that it is almost always met whenever the axioms aresound. In particular, it is easily seen to hold for all the proposals made inthe literature.;https://arxiv.org/abs/cs/9808007
arXiv:cs/9808005;[cs.AI];Computer Science > Artificial Intelligence;First-Order Conditional Logic Revisited;[Submitted on 28 Aug 1998];Nir Friedman;Joseph Y. Halpern;Daphne Koller;;;Conditional logics play an important role in recent attempts to formulatetheories of default reasoning. This paper investigates first-order conditionallogic. We show that, as for first-order probabilistic logic, it is importantnot to confound statistical conditionals over the domain (such as ``most birdsfly''), and subjective conditionals over possible worlds (such as ``I believethat Tweety is unlikely to fly''). We then address the issue of ascribingsemantics to first-order conditional logic. As in the propositional case, thereare many possible semantics. To study the problem in a coherent way, we useplausibility structures. These provide us with a general framework in whichmany of the standard approaches can be embedded. We show that while thesestandard approaches are all the same at the propositional level, they aresignificantly different in the context of a first-order language. Furthermore,we show that plausibilities provide the most natural extension of conditionallogic to the first-order case: We provide a sound and complete axiomatizationthat contains only the KLM properties and standard axioms of first-order modallogic. We show that most of the other approaches have additional properties,which result in an inappropriate treatment of an infinitary version of thelottery paradox.;https://arxiv.org/abs/cs/9808005
arXiv:cs/9709101;[cs.AI];Computer Science > Artificial Intelligence;Towards Flexible Teamwork;[Submitted on 1 Sep 1997];M. Tambe;;;;;"Many AI researchers are today striving to build agent teams for complex,dynamic multi-agent domains, with intended applications in arenas such aseducation, training, entertainment, information integration, and collectiverobotics. Unfortunately, uncertainties in these complex, dynamic domainsobstruct coherent teamwork. In particular, team members often encounterdiffering, incomplete, and possibly inconsistent views of their environment.Furthermore, team members can unexpectedly fail in fulfilling responsibilitiesor discover unexpected opportunities. Highly flexible coordination andcommunication is key in addressing such uncertainties. Simply fittingindividual agents with precomputed coordination plans will not do, for theirinflexibility can cause severe failures in teamwork, and theirdomain-specificity hinders reusability. Our central hypothesis is that the keyto such flexibility and reusability is providing agents with general models ofteamwork. Agents exploit such models to autonomously reason about coordinationand communication, providing requisite flexibility. Furthermore, the modelsenable reuse across domains, both saving implementation effort and enforcingconsistency. This article presents one general, implemented model of teamwork,called STEAM. The basic building block of teamwork in STEAM is joint intentions(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a(partial) hierarchy of joint intentions (this hierarchy is seen to parallelGrosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team membersmonitor the team's and individual members' performance, reorganizing the teamas necessary. Finally, decision-theoretic communication selectivity in STEAMensures reduction in communication overheads of teamwork, with appropriatesensitivity to the environmental conditions. This article describes STEAM'sapplication in three different complex domains, and presents detailed empiricalresults.";https://arxiv.org/abs/cs/9709101
arXiv:cs/9707102;[cs.AI];Computer Science > Artificial Intelligence;Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time;[Submitted on 1 Jul 1997];T. Drakengren;P. Jonsson;;;;This paper combines two important directions of research in temporalresoning: that of finding maximal tractable subclasses of Allen's intervalalgebra, and that of reasoning with metric temporal information. Eight newmaximal tractable subclasses of Allen's interval algebra are presented, some ofthem subsuming previously reported tractable algebras. The algebras allow formetric temporal constraints on interval starting or ending points, using therecent framework of Horn DLRs. Two of the algebras can express the notion ofsequentiality between intervals, being the first such algebras admitting bothqualitative and metric time.;https://arxiv.org/abs/cs/9707102
arXiv:cs/9707101;[cs.AI];Computer Science > Artificial Intelligence;A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search Difficulty;[Submitted on 1 Jul 1997];D. L. Mammen;T. Hogg;;;;The easy-hard-easy pattern in the difficulty of combinatorial search problemsas constraints are added has been explained as due to a competition between thedecrease in number of solutions and increased pruning. We test the generalityof this explanation by examining one of its predictions: if the number ofsolutions is held fixed by the choice of problems, then increased pruningshould lead to a monotonic decrease in search cost. Instead, we find theeasy-hard-easy pattern in median search cost even when the number of solutionsis held constant, for some search methods. This generalizes previousobservations of this pattern and shows that the existing theory does notexplain the full range of the peak in search cost. In these cases the patternappears to be due to changes in the size of the minimal unsolvable subproblems,rather than changing numbers of solutions.;https://arxiv.org/abs/cs/9707101
arXiv:cs/9706102;[cs.AI];Computer Science > Artificial Intelligence;A Complete Classification of Tractability in RCC-5;[Submitted on 1 Jun 1997];P. Jonsson;T. Drakengren;;;;We investigate the computational properties of the spatial algebra RCC-5which is a restricted version of the RCC framework for spatial reasoning. Thesatisfiability problem for RCC-5 is known to be NP-complete but not much isknown about its approximately four billion subclasses. We provide a completeclassification of satisfiability for all these subclasses into polynomial andNP-complete respectively. In the process, we identify all maximal tractablesubalgebras which are four in total.;https://arxiv.org/abs/cs/9706102
arXiv:cs/9706101;[cs.AI];Computer Science > Artificial Intelligence;Flaw Selection Strategies for Partial-Order Planning;[Submitted on 1 Jun 1997];M. E. Pollack;D. Joslin;M. Paolucci;;;Several recent studies have compared the relative efficiency of alternativeflaw selection strategies for partial-order causal link (POCL) planning. Wereview this literature, and present new experimental results that generalizethe earlier work and explain some of the discrepancies in it. In particular, wedescribe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed byJoslin and Pollack (1994), and compare it with other strategies, includingGerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make verydifferent, and apparently conflicting claims about the most effective way toreduce search-space size in POCL planning. We resolve this conflict, arguingthat much of the benefit that Gerevini and Schubert ascribe to the LIFOcomponent of their ZLIFO strategy is better attributed to other causes. We showthat for many problems, a strategy that combines least-cost flaw selection withthe delay of separable threats will be effective in reducing search-space size,and will do so without excessive computational overhead. Although such astrategy thus provides a good default, we also show that certain domaincharacteristics may reduce its effectiveness.;https://arxiv.org/abs/cs/9706101
arXiv:cs/9705101;[cs.AI];Computer Science > Artificial Intelligence;Query DAGs: A Practical Paradigm for Implementing Belief-Network Inference;[Submitted on 1 May 1997];A. Darwiche;G. Provan;;;;"We describe a new paradigm for implementing inference in belief networks,which consists of two steps: (1) compiling a belief network into an arithmeticexpression called a Query DAG (Q-DAG); and (2) answering queries using a simpleevaluation algorithm. Each node of a Q-DAG represents a numeric operation, anumber, or a symbol for evidence. Each leaf node of a Q-DAG represents theanswer to a network query, that is, the probability of some event of interest.It appears that Q-DAGs can be generated using any of the standard algorithmsfor exact inference in belief networks (we show how they can be generated usingclustering and conditioning algorithms). The time and space complexity of aQ-DAG generation algorithm is no worse than the time complexity of theinference algorithm on which it is based. The complexity of a Q-DAG evaluationalgorithm is linear in the size of the Q-DAG, and such inference amounts to astandard evaluation of the arithmetic expression it represents. The intendedvalue of Q-DAGs is in reducing the software and hardware resources required toutilize belief networks in on-line, real-world applications. The proposedframework also facilitates the development of on-line inference on differentsoftware and hardware platforms due to the simplicity of the Q-DAG evaluationalgorithm. Interestingly enough, Q-DAGs were found to serve other purposes:simple techniques for reducing Q-DAGs tend to subsume relatively complexoptimization techniques for belief-network inference, such as network-pruningand computation-caching.";https://arxiv.org/abs/cs/9705101
arXiv:cs/9811029;[cs.RO];Computer Science > Robotics;A Human - machine interface for teleoperation of arm manipulators in a complex environment;[Submitted on 20 Nov 1998];I. Ivanisevic;V. Lumelsky;;;;This paper discusses the feasibility of using configuration space (C-space)as a means of visualization and control in operator-guided real-time motion ofa robot arm manipulator. The motivation is to improve performance of the humanoperator in tasks involving the manipulator motion in an environment withobstacles. Unlike some other motion planning tasks, operators are known to makeexpensive mistakes in such tasks, even in a simpler two-dimensional case. Theyhave difficulty learning better procedures and their performance improves verylittle with practice. Using an example of a two-dimensional arm manipulator, weshow that translating the problem into C-space improves the operatorperformance rather remarkably, on the order of magnitude compared to the usualwork space control. An interface that makes the transfer possible is described,and an example of its use in a virtual environment is shown.;https://arxiv.org/abs/cs/9811029
arXiv:cs/9811024;[cs.AI];Computer Science > Artificial Intelligence;The Essence of Constraint Propagation;[Submitted on 13 Nov 1998];Krzysztof R. Apt;;;;;We show that several constraint propagation algorithms (also called (local)consistency, consistency enforcing, Waltz, filtering or narrowing algorithms)are instances of algorithms that deal with chaotic iteration. To this end wepropose a simple abstract framework that allows us to classify and comparethese algorithms and to establish in a uniform way their basic properties.;https://arxiv.org/abs/cs/9811024
arXiv:cs/9901016;[cs.LO];Computer Science > Logic in Computer Science;Representation Theory for Default Logic;[Submitted on 28 Jan 1999];Victor Marek;Jan Treur;Miroslaw Truszczynski;;;Default logic can be regarded as a mechanism to represent families of beliefsets of a reasoning agent. As such, it is inherently second-order. In thispaper, we study the problem of representability of a family of theories as theset of extensions of a default theory. We give a complete solution to therepresentability by means of normal default theories. We obtain partial resultson representability by arbitrary default theories. We construct examples ofdenumerable families of non-including theories that are not representable. Wealso study the concept of equivalence between default theories.;https://arxiv.org/abs/cs/9901016
arXiv:cs/9902015;[cs.DL];Computer Science > Digital Libraries;Resource Discovery in Trilogy;[Submitted on 8 Feb 1999];Franck Chevalier;David Harle;Geoffrey Smith;;;"Trilogy is a collaborative project whose key aim is the development of anintegrated virtual laboratory to support research training within eachinstitution and collaborative projects between the partners. In this paper, thearchitecture and underpinning platform of the system is described withparticular emphasis being placed on the structure and the integration of thedistributed database. A key element is the ontology that provides themulti-agent system with a conceptualisation specification of the domain; thisontology is explained, accompanied by a discussion how such a system isintegrated and used within the virtual laboratory. Although in this paper,Telecommunications and in particular Broadband networks are used as exemplars,the underlying system principles are applicable to any domain where acombination of experimental and literature-based resources are required.";https://arxiv.org/abs/cs/9902015
arXiv:cs/9902006;[cs.NE];Computer Science > Neural and Evolutionary Computing;A Discipline of Evolutionary Programming;[Submitted on 2 Feb 1999];Paul Vitanyi;;;;;Genetic fitness optimization using small populations or small populationupdates across generations generally suffers from randomly divergingevolutions. We propose a notion of highly probable fitness optimization throughfeasible evolutionary computing runs on small size populations. Based onrapidly mixing Markov chains, the approach pertains to most types ofevolutionary genetic algorithms, genetic programming and the like. We establishthat for systems having associated rapidly mixing Markov chains and appropriatestationary distributions the new method finds optimal programs (individuals)with probability almost 1. To make the method useful would require a structureddesign methodology where the development of the program and the guarantee ofthe rapidly mixing property go hand in hand. We analyze a simple example toshow that the method is implementable. More significant examples requiretheoretical advances, for example with respect to the Metropolis filter.;https://arxiv.org/abs/cs/9902006
arXiv:cs/9903011;[cs.DS];Computer Science > Data Structures and Algorithms;A complete anytime algorithm for balanced number partitioning;[Submitted on 11 Mar 1999];Stephan Mertens;;;;;Given a set of numbers, the balanced partioning problem is to divide theminto two subsets, so that the sum of the numbers in each subset are as nearlyequal as possible, subject to the constraint that the cardinalities of thesubsets be within one of each other. We combine the balanced largestdifferencing method (BLDM) and Korf's complete Karmarkar-Karp algorithm to geta new algorithm that optimally solves the balanced partitioning problem. Fornumbers with twelve significant digits or less, the algorithm can optimallysolve balanced partioning problems of arbitrary size in practice. For numberswith greater precision, it first returns the BLDM solution, then continues tofind better solutions as time allows.;https://arxiv.org/abs/cs/9903011
arXiv:cs/9905008;[cs.CL];Computer Science > Computation and Language;Inducing a Semantically Annotated Lexicon via EM-Based Clustering;[Submitted on 19 May 1999];Mats Rooth;Stefan Riezler;Detlef Prescher;Glenn Carroll;Franz Beil;We present a technique for automatic induction of slot annotations forsubcategorization frames, based on induction of hidden classes in the EMframework of statistical estimation. The models are empirically evalutated by ageneral decision test. Induction of slot labeling for subcategorization framesis accomplished by a further application of EM, and applied experimentally onframe observations derived from parsing large corpora. We outline aninterpretation of the learned representations as theoretical-linguisticdecompositional lexical entries.;https://arxiv.org/abs/cs/9905008
arXiv:cs/9903002;[cs.SE];Computer Science > Software Engineering;An Algebraic Programming Style for Numerical Software and its Optimization;[Submitted on 1 Mar 1999];T. B. Dinesh;M. Haveraaen;J. Heering;;;The abstract mathematical theory of partial differential equations (PDEs) isformulated in terms of manifolds, scalar fields, tensors, and the like, butthese algebraic structures are hardly recognizable in actual PDE solvers. Thegeneral aim of the Sophus programming style is to bridge the gap between theoryand practice in the domain of PDE solvers. Its main ingredients are a libraryof abstract datatypes corresponding to the algebraic structures used in themathematical theory and an algebraic expression style similar to the expressionstyle used in the mathematical theory. Because of its emphasis on abstractdatatypes, Sophus is most naturally combined with object-oriented languages orother languages supporting abstract datatypes. The resulting source codepatterns are beyond the scope of current compiler optimizations, but aresufficiently specific for a dedicated source-to-source optimizer. The limited,domain-specific, character of Sophus is the key to success here. This kind ofoptimization has been tested on computationally intensive Sophus style codewith promising results. The general approach may be useful for other styles andin other application domains as well.;https://arxiv.org/abs/cs/9903002
arXiv:cs/9906029;[cs.SE];Computer Science > Software Engineering;Events in Property Patterns;[Submitted on 28 Jun 1999 (;M.Chechik;D.Paun;;;;A pattern-based approach to the presentation, codification and reuse ofproperty specifications for finite-state verification was proposed by Dwyer andhis collegues. The patterns enable non-experts to read and write formalspecifications for realistic systems and facilitate easy conversion ofspecifications between formalisms, such as LTL, CTL, QRE. In this paper, weextend the pattern system with events - changes of values of variables in thecontext of LTL.;https://arxiv.org/abs/cs/9906029
arXiv:cs/9906019;[cs.CL];Computer Science > Computation and Language;Resolving Part-of-Speech Ambiguity in the Greek Language Using Learning Techniques;[Submitted on 22 Jun 1999 (;G. Petasis;G. Paliouras;V. Karkaletsis;C. D. Spyropoulos;I. Androutsopoulos;"This article investigates the use of Transformation-Based Error-Drivenlearning for resolving part-of-speech ambiguity in the Greek language. The aimis not only to study the performance, but also to examine its dependence ondifferent thematic domains. Results are presented here for two different testcases: a corpus on ""management succession events"" and a general-theme corpus.The two experiments show that the performance of this method does not depend onthe thematic domain of the corpus, and its accuracy for the Greek language isaround 95%.";https://arxiv.org/abs/cs/9906019
arXiv:cs/9909019;[cs.LO];Computer Science > Logic in Computer Science;Knowledge in Multi-Agent Systems: Initial Configurations and Broadcast;[Submitted on 30 Sep 1999];A. R. Lomuscio;R. van der Meyden;M. D. Ryan;;;The semantic framework for the modal logic of knowledge due to Halpern andMoses provides a way to ascribe knowledge to agents in distributed andmulti-agent systems. In this paper we study two special cases of thisframework: full systems and hypercubes. Both model static situations in whichno agent has any information about another agent's state. Full systems andhypercubes are an appropriate model for the initial configurations of manysystems of interest. We establish a correspondence between full systems andhypercube systems and certain classes of Kripke frames. We show that theseclasses of systems correspond to the same logic. Moreover, this logic is alsothe same as that generated by the larger class of weakly directed frames. Weprovide a sound and complete axiomatization, S5WDn, of this logic. Finally, weshow that under certain natural assumptions, in a model where knowledge evolvesover time, S5WDn characterizes the properties of knowledge not just at theinitial configuration, but also at all later configurations. In particular,this holds for homogeneous broadcast systems, which capture settings in whichagents are initially ignorant of each others local states, operatesynchronously, have perfect recall and can communicate only by broadcasting.;https://arxiv.org/abs/cs/9909019
arXiv:cs/9910016;[cs.AI];Computer Science > Artificial Intelligence;Probabilistic Agent Programs;[Submitted on 21 Oct 1999];Juergen Dix;Mirco Nanni;VS Subrahmanian;;;Agents are small programs that autonomously take actions based on changes intheir environment or ``state.'' Over the last few years, there have been anincreasing number of efforts to build agents that can interact and/orcollaborate with other agents. In one of these efforts, Eiter, Subrahmanian amdPick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on topof legacy code. However, their framework assumes that agent states arecompletely determined, and there is no uncertainty in an agent's state. Thus,their framework allows an agent developer to specify how his agents will reactwhen the agent is 100% sure about what is true/false in the world state. Inthis paper, we propose the concept of a \emph{probabilistic agent program} andshow how, given an arbitrary program written in any imperative language, we maybuild a declarative ``probabilistic'' agent program on top of it which supportsdecision making in the presence of uncertainty. We provide two alternativesemantics for probabilistic agent programs. We show that the second semantics,though more epistemically appealing, is more complex to compute. We providesound and complete algorithms to compute the semantics of \emph{positive} agentprograms.;https://arxiv.org/abs/cs/9910016
arXiv:cs/9910015;[cs.IR];Computer Science > Information Retrieval;PIPE: Personalizing Recommendations via Partial Evaluation;[Submitted on 18 Oct 1999 (;Naren Ramakrishnan;;;;;It is shown that personalization of web content can be advantageously viewedas a form of partial evaluation --- a technique well known in the programminglanguages community. The basic idea is to model a recommendation space as aprogram, then partially evaluate this program with respect to user preferences(and features) to obtain specialized content. This technique supports bothcontent-based and collaborative approaches, and is applicable to a range ofapplications that require automatic information integration from multiple websources. The effectiveness of this methodology is illustrated by two exampleapplications --- (i) personalizing content for visitors to the BlacksburgElectronic Village (;https://arxiv.org/abs/cs/9910015
arXiv:cs/9911012;[cs.AI];Computer Science > Artificial Intelligence;Cox's Theorem Revisited;[Submitted on 27 Nov 1999 (;Joseph Y. Halpern;;;;;The assumptions needed to prove Cox's Theorem are discussed and examined.Various sets of assumptions under which a Cox-style theorem can be proved areprovided, although all are rather strong and, arguably, not natural.;https://arxiv.org/abs/cs/9911012
arXiv:cs/0001002;[cs.CL];Computer Science > Computation and Language;Minimum Description Length and Compositionality;[Submitted on 4 Jan 2000];Wlodek Zadrozny;;;;;We present a non-vacuous definition of compositionality. It is based on theidea of combining the minimum description length principle with the originaldefinition of compositionality (that is, that the meaning of the whole is afunction of the meaning of the parts).;https://arxiv.org/abs/cs/0001002
arXiv:cs/0001015;[cs.AI];Computer Science > Artificial Intelligence;Multi-Agent Only Knowing;[Submitted on 19 Jan 2000];Joseph Y. Halpern;Gerhard Lakemeyer;;;;Levesque introduced a notion of ``only knowing'', with the goal of capturingcertain types of nonmonotonic reasoning. Levesque's logic dealt with only thecase of a single agent. Recently, both Halpern and Lakemeyer independentlyattempted to extend Levesque's logic to the multi-agent case. Although thereare a number of similarities in their approaches, there are some significantdifferences. In this paper, we reexamine the notion of only knowing, going backto first principles. In the process, we simplify Levesque's completeness proof,and point out some problems with the earlier definitions. This leads us toreconsider what the properties of only knowing ought to be. We provide an axiomsystem that captures our desiderata, and show that it has a semantics thatcorresponds to it. The axiom system has an added feature of interest: itincludes a modal operator for satisfiability, and thus provides a completeaxiomatization for satisfiability in the logic K45.;https://arxiv.org/abs/cs/0001015
arXiv:cs/0003082;[cs.LO];Computer Science > Logic in Computer Science;Representation results for defeasible logic;[Submitted on 30 Mar 2000];G. Antoniou;D. Billington;G. Governatori;M.J. Maher;;The importance of transformations and normal forms in logic programming, andgenerally in computer science, is well documented. This paper investigatestransformations and normal forms in the context of Defeasible Logic, a simplebut efficient formalism for nonmonotonic reasoning based on rules andpriorities. The transformations described in this paper have two main benefits:on one hand they can be used as a theoretical tool that leads to a deeperunderstanding of the formalism, and on the other hand they have been used inthe development of an efficient implementation of defeasible logic.;https://arxiv.org/abs/cs/0003082
arXiv:cs/0003067;[cs.LO];Computer Science > Logic in Computer Science;Detecting Unsolvable Queries for Definite Logic Programs;[Submitted on 17 Mar 2000];Maurice Bruynooghe;Henk Vandecasteele;D. Andre de Waal;Marc Denecker;;In solving a query, the SLD proof procedure for definite programs sometimessearches an infinite space for a non existing solution. For example, querying aplanner for an unreachable goal state. Such programs motivate the developmentof methods to prove the absence of a solution. Considering the definite programand the query ``<- Q'' as clauses of a first order theory, one can apply modelgenerators which search for a finite interpretation in which the programclauses as well as the clause ``false <- Q'' are true. This paper develops anew approach which exploits the fact that all clauses are definite. It is basedon a goal directed abductive search in the space of finite pre-interpretationsfor a pre-interpretation such that ``Q'' is false in the least model of theprogram based on it. Several methods for efficiently searching the space ofpre-interpretations are presented. Experimental results confirm that ourapproach find solutions with less search than with the use of a first ordermodel generator.;https://arxiv.org/abs/cs/0003067
arXiv:cs/0003057;[cs.LO];Computer Science > Logic in Computer Science;XNMR: A tool for knowledge bases exploration;[Submitted on 13 Mar 2000];L. Castro;D. Warren;;;;XNMR is a system designed to explore the results of combining thewell-founded semantics system XSB with the stable-models evaluator SMODELS. Itsmain goal is to work as a tool for fast and interactive exploration ofknowledge bases.;https://arxiv.org/abs/cs/0003057
arXiv:cs/0003050;[cs.LO];Computer Science > Logic in Computer Science;A tableau methodology for deontic conditional logics;[Submitted on 10 Mar 2000];Alberto Artosi;Guido Governatori;;;;In this paper we present a theorem proving methodology for a restricted butsignificant fragment of the conditional language made up of (booleancombinations of) conditional statements with unnested antecedents. The methodis based on the possible world semantics for conditional logics. The KEM labelformalism, designed to account for the semantics of normal modal logics, iseasily adapted to the semantics of conditional logics by simply indexing labelswith formulas. The inference rules are provided by the propositional system KE+- a tableau-like analytic proof system devised to be used both as a refutationand a direct method of proof - enlarged with suitable elimination rules for theconditional connective. The theorem proving methodology we are going to presentcan be viewed as a first step towards developing an appropriate algorithmicframework for several conditional logics for (defeasible) conditionalobligation.;https://arxiv.org/abs/cs/0003050
arXiv:cs/0003056;[cs.LO];Computer Science > Logic in Computer Science;A note on the Declarative reading(s) of Logic Programming;[Submitted on 13 Mar 2000];Marc Denecker;;;;;This paper analyses the declarative readings of logic programming. Logicprogramming - and negation as failure - has no unique declarative reading. Onecommon view is that logic programming is a logic for default reasoning, asub-formalism of default logic or autoepistemic logic. In this view, negationas failure is a modal operator. In an alternative view, a logic program isinterpreted as a definition. In this view, negation as failure is classicalobjective negation. From a commonsense point of view, there is definitely adifference between these views. Surprisingly though, both types of declarativereadings lead to grosso modo the same model semantics. This note investigatesthe causes for this.;https://arxiv.org/abs/cs/0003056
arXiv:cs/0003019;[cs.LO];Computer Science > Logic in Computer Science;Extending Classical Logic with Inductive Definitions;[Submitted on 7 Mar 2000];Marc Denecker;;;;;The goal of this paper is to extend classical logic with a generalized notionof inductive definition supporting positive and negative induction, toinvestigate the properties of this logic, its relationships to other logics inthe area of non-monotonic reasoning, logic programming and deductive databases,and to show its application for knowledge representation by giving a typologyof definitional knowledge.;https://arxiv.org/abs/cs/0003019
arXiv:cs/0003080;[cs.AI];Computer Science > Artificial Intelligence;Some Remarks on Boolean Constraint Propagation;[Submitted on 28 Mar 2000];Krzysztof R. Apt;;;;;We study here the well-known propagation rules for Boolean constraints. Firstwe propose a simple notion of completeness for sets of such rules and establisha completeness result. Then we show an equivalence in an appropriate sensebetween Boolean constraint propagation and unit propagation, a form ofresolution for propositional logic.;https://arxiv.org/abs/cs/0003080
arXiv:cs/0003077;[cs.AI];Computer Science > Artificial Intelligence;DATALOG with constraints - an answer-set programming system;[Submitted on 24 Mar 2000];Deborah East;Miroslaw Truszczynski;;;;Answer-set programming (ASP) has emerged recently as a viable programmingparadigm well attuned to search problems in AI, constraint satisfaction andcombinatorics. Propositional logic is, arguably, the simplest ASP system withan intuitive semantics supporting direct modeling of problem constraints.However, for some applications, especially those requiring that transitiveclosure be computed, it requires additional variables and results in largetheories. Consequently, it may not be a practical computational tool for suchproblems. On the other hand, ASP systems based on nonmonotonic logics, such asstable logic programming, can handle transitive closure computation efficientlyand, in general, yield very concise theories as problem representations. Theirsemantics is, however, more complex. Searching for the middle ground, in thispaper we introduce a new nonmonotonic logic, DATALOG with constraints or DC.Informally, DC theories consist of propositional clauses (constraints) and ofHorn rules. The semantics is a simple and natural extension of the semantics ofthe propositional logic. However, thanks to the presence of Horn rules in thesystem, modeling of transitive closure becomes straightforward. We describe thesyntax and semantics of DC, and study its properties. We discuss animplementation of DC and present results of experimental study of theeffectiveness of DC, comparing it with CSAT, a satisfiability checker andSMODELS implementation of stable logic programming. Our results show that DC iscompetitive with the other two approaches, in case of many search problems,often yielding much more efficient solutions.;https://arxiv.org/abs/cs/0003077
arXiv:cs/0003076;[cs.AI];Computer Science > Artificial Intelligence;Constraint Programming viewed as Rule-based Programming;[Submitted on 24 Mar 2000 (;Krzysztof R. Apt;Eric Monfroy;;;;We study here a natural situation when constraint programming can be entirelyreduced to rule-based programming. To this end we explain first how one cancompute on constraint satisfaction problems using rules represented by simplefirst-order formulas. Then we consider constraint satisfaction problems thatare based on predefined, explicitly given constraints. To solve them we firstderive rules from these explicitly given constraints and limit the computationprocess to a repeated application of these rules, combined with labeling.Weconsider here two types of rules. The first type, that we call equality rules,leads to a new notion of local consistency, called {\em rule consistency} thatturns out to be weaker than arc consistency for constraints of arbitrary arity(called hyper-arc consistency in \cite{MS98b}). For Boolean constraints ruleconsistency coincides with the closure under the well-known propagation rulesfor Boolean constraints. The second type of rules, that we call membershiprules, yields a rule-based characterization of arc consistency. To showfeasibility of this rule-based approach to constraint programming we show howboth types of rules can be automatically generated, as {\tt CHR} rules of\cite{fruhwirth-constraint-95}. This yields an implementation of this approachto programming by means of constraint logic programming. We illustrate theusefulness of this approach to constraint programming by discussing variousexamples, including Boolean constraints, two typical examples of many valuedlogics, constraints dealing with Waltz's language for describing polyhedralscenes, and Allen's qualitative approach to temporal logic.;https://arxiv.org/abs/cs/0003076
arXiv:cs/0003073;[cs.AI];Computer Science > Artificial Intelligence;Proceedings of the 8th International Workshop on Non-Monotonic Reasoning, NMR'2000;[Submitted on 22 Mar 2000];Chitta Baral;Miroslaw Truszczynski;;;;The papers gathered in this collection were presented at the 8thInternational Workshop on Nonmonotonic Reasoning, NMR2000. The series wasstarted by John McCarthy in 1978. The first international NMR workshop was heldat Mohonk Mountain House, New Paltz, New York in June, 1984, and was organizedby Ray Reiter and Bonnie Webber.;https://arxiv.org/abs/cs/0003073
arXiv:cs/0003061;[cs.AI];Computer Science > Artificial Intelligence;dcs: An Implementation of DATALOG with Constraints;[Submitted on 14 Mar 2000];Deborah East;Miroslaw Truszczynski;;;;Answer-set programming (ASP) has emerged recently as a viable programmingparadigm. We describe here an ASP system, DATALOG with constraints or DC, basedon non-monotonic logic. Informally, DC theories consist of propositionalclauses (constraints) and of Horn rules. The semantics is a simple and naturalextension of the semantics of the propositional logic. However, thanks to thepresence of Horn rules in the system, modeling of transitive closure becomesstraightforward. We describe the syntax, use and implementation of DC andprovide experimental results.;https://arxiv.org/abs/cs/0003061
arXiv:cs/0003059;[cs.AI];Computer Science > Artificial Intelligence;SATEN: An Object-Oriented Web-Based Revision and Extraction Engine;[Submitted on 14 Mar 2000];Mary-Anne Williams;Aidan Sims;;;;SATEN is an object-oriented web-based extraction and belief revision engine.It runs on any computer via a Java 1.1 enabled browser such as Netscape 4.SATEN performs belief revision based on the AGM approach. The extraction andbelief revision reasoning engines operate on a user specified ranking ofinformation. One of the features of SATEN is that it can be used to integratemutually inconsistent commensuate rankings into a consistent ranking.;https://arxiv.org/abs/cs/0003059
arXiv:cs/0003052;[cs.AI];Computer Science > Artificial Intelligence;A Consistency-Based Model for Belief Change: Preliminary Report;[Submitted on 11 Mar 2000 (;James Delgrande;Torsten Schaub;;;;"We present a general, consistency-based framework for belief change.Informally, in revising K by A, we begin with A and incorporate as much of K asconsistently possible. Formally, a knowledge base K and sentence A areexpressed, via renaming propositions in K, in separate languages. Using amaximization process, we assume the languages are the same insofar asconsistently possible. Lastly, we express the resultant knowledge base in asingle language. There may be more than one way in which A can be so extendedby K: in choice revision, one such ``extension'' represents the revised state;alternately revision consists of the intersection of all such extensions.";https://arxiv.org/abs/cs/0003052
arXiv:cs/0003051;[cs.AI];Computer Science > Artificial Intelligence;Local Diagnosis;[Submitted on 10 Mar 2000];Renata Wassermann;;;;;In an earlier work, we have presented operations of belief change which onlyaffect the relevant part of a belief base. In this paper, we propose theapplication of the same strategy to the problem of model-based diangosis. Wefirst isolate the subset of the system description which is relevant for agiven observation and then solve the diagnosis problem for this subset.;https://arxiv.org/abs/cs/0003051
arXiv:cs/0003049;[cs.AI];Computer Science > Artificial Intelligence;Planning with Incomplete Information;[Submitted on 9 Mar 2000];Antonis Kakas;Rob Miller;Francesca Toni;;;Planning is a natural domain of application for frameworks of reasoning aboutactions and change. In this paper we study how one such framework, the LanguageE, can form the basis for planning under (possibly) incomplete information. Wedefine two types of plans: weak and safe plans, and propose a planner, calledthe E-Planner, which is often able to extend an initial weak plan into a safeplan even though the (explicit) information available is incomplete, e.g. forcases where the initial state is not completely known. The E-Planner is basedupon a reformulation of the Language E in argumentation terms and a naturalproof theory resulting from the reformulation. It uses an extension of thisproof theory by means of abduction for the generation of plans and adoptsargumentation-based techniques for extending weak plans into safe plans. Weprovide representative examples illustrating the behaviour of the E-Planner, inparticular for cases where the status of fluents is incompletely known.;https://arxiv.org/abs/cs/0003049
arXiv:cs/0003048;[cs.AI];Computer Science > Artificial Intelligence;PAL: Pertinence Action Language;[Submitted on 9 Mar 2000];Pedro Cabalar;Manuel Cabarcos;Ramon P. Otero;;;The current document contains a brief description of a system for Reasoningabout Actions and Change called PAL (Pertinence Action Language) which makesuse of several reasoning properties extracted from a Temporal Expert Systemstool called Medtool.;https://arxiv.org/abs/cs/0003048
arXiv:cs/0003047;[cs.AI];Computer Science > Artificial Intelligence;BDD-based reasoning in the fluent calculus - first results;[Submitted on 9 Mar 2000];Steffen Hoelldobler;Hans-Peter Stoerr;;;;The paper reports on first preliminary results and insights gained in aproject aiming at implementing the fluent calculus using methods and techniquesbased on binary decision diagrams. After reporting on an initial experimentshowing promising results we discuss our findings concerning various techniquesand heuristics used to speed up the reasoning process.;https://arxiv.org/abs/cs/0003047
arXiv:cs/0003046;[cs.AI];Computer Science > Artificial Intelligence;Linear Tabulated Resolution Based on Prolog Control Strategy;[Submitted on 9 Mar 2000];Yi-Dong Shen;Li-Yan Yuan;Jia-Huai You;Neng-Fa Zhou;;Infinite loops and redundant computations are long recognized open problemsin Prolog. Two ways have been explored to resolve these problems: loop checkingand tabling. Loop checking can cut infinite loops, but it cannot be both soundand complete even for function-free logic programs. Tabling seems to be aneffective way to resolve infinite loops and redundant computations. However,existing tabulated resolutions, such as OLDT-resolution, SLG- resolution, andTabulated SLS-resolution, are non-linear because they rely on thesolution-lookup mode in formulating tabling. The principal disadvantage ofnon-linear resolutions is that they cannot be implemented using a simplestack-based memory structure like that in Prolog. Moreover, some strictlysequential operators such as cuts may not be handled as easily as in Prolog.;https://arxiv.org/abs/cs/0003046
arXiv:cs/0003044;[cs.AI];Computer Science > Artificial Intelligence;On the tractable counting of theory models and its application to belief revision and truth maintenance;[Submitted on 9 Mar 2000];Adnan Darwiche;;;;;We introduced decomposable negation normal form (DNNF) recently as atractable form of propositional theories, and provided a number of powerfullogical operations that can be performed on it in polynomial time. We alsopresented an algorithm for compiling any conjunctive normal form (CNF) intoDNNF and provided a structure-based guarantee on its space and time complexity.We present in this paper a linear-time algorithm for converting an orderedbinary decision diagram (OBDD) representation of a propositional theory into anequivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify asubclass of DNNF which we call deterministic DNNF, d-DNNF, and show that theprevious complexity guarantees on compiling DNNF continue to hold for thisstricter subclass, which has stronger properties. In particular, we present anew operation on d-DNNF which allows us to count its models under theassertion, retraction and flipping of every literal by traversing the d-DNNFtwice. That is, after such traversal, we can test in constant-time: theentailment of any literal by the d-DNNF, and the consistency of the d-DNNFunder the retraction or flipping of any literal. We demonstrate thesignificance of these new operations by showing how they allow us to implementlinear-time, complete truth maintenance systems and linear-time, completebelief revision systems for two important classes of propositional theories.;https://arxiv.org/abs/cs/0003044
arXiv:cs/0003042;[cs.AI];Computer Science > Artificial Intelligence;Fages' Theorem and Answer Set Programming;[Submitted on 9 Mar 2000];Yuliya Babovich;Esra Erdem;Vladimir Lifschitz;;;We generalize a theorem by Francois Fages that describes the relationshipbetween the completion semantics and the answer set semantics for logicprograms with negation as failure. The study of this relationship is importantin connection with the emergence of answer set programming. Whenever the twosemantics are equivalent, answer sets can be computed by a satisfiabilitysolver, and the use of answer set solvers such as smodels and dlv isunnecessary. A logic programming representation of the blocks world due toIlkka Niemelae is discussed as an example.;https://arxiv.org/abs/cs/0003042
arXiv:cs/0003041;[cs.AI];Computer Science > Artificial Intelligence;Coherence, Belief Expansion and Bayesian Networks;[Submitted on 8 Mar 2000];Luc Bovens;Stephan Hartmann;;;;We construct a probabilistic coherence measure for information sets whichdetermines a partial coherence ordering. This measure is applied inconstructing a criterion for expanding our beliefs in the face of newinformation. A number of idealizations are being made which can be relaxed byan appeal to Bayesian Networks.;https://arxiv.org/abs/cs/0003041
