id;category_code;category;title;date;author_1;author_2;author_3;author_4;author_5;summary;link
arXiv:cs/9309101;[cs.AI];Computer Science > Artificial Intelligence;An Empirical Analysis of Search in GSAT;[Submitted on 1 Sep 1993];I. P. Gent;T. Walsh;;;;We describe an extensive study of search in GSAT, an approximation procedurefor propositional satisfiability. GSAT performs greedy hill-climbing on thenumber of satisfied clauses in a truth assignment. Our experiments provide amore complete picture of GSAT's search than previous accounts. We describe indetail the two phases of search: rapid hill-climbing followed by a long plateausearch. We demonstrate that when applied to randomly generated 3SAT problems,there is a very simple scaling with problem size for both the mean number ofsatisfied clauses and the mean branching rate. Our results allow us to makedetailed numerical conjectures about the length of the hill-climbing phase, theaverage gradient of this phase, and to conjecture that both the average scoreand average branching rate decay exponentially during plateau search. We end byshowing how these results can be used to direct future theoretical analysis.This work provides a case study of how computer experiments can be used toimprove understanding of the theoretical properties of algorithms.;https://arxiv.org/abs/cs/9309101
arXiv:cs/9308102;[cs.AI];Computer Science > Artificial Intelligence;A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems;[Submitted on 1 Aug 1993];M. P. Wellman;;;;;Market price systems constitute a well-understood class of mechanisms thatunder certain conditions provide effective decentralization of decision makingwith minimal communication overhead. In a market-oriented programming approachto distributed problem solving, we derive the activities and resourceallocations for a set of computational agents by computing the competitiveequilibrium of an artificial economy. WALRAS provides basic constructs fordefining computational market structures, and protocols for deriving theircorresponding price equilibria. In a particular realization of this approachfor a form of multicommodity flow problem, we see that careful construction ofthe decision process according to economic principles can lead to efficientdistributed resource allocation, and that the behavior of the system can bemeaningfully analyzed in economic terms.;https://arxiv.org/abs/cs/9308102
arXiv:cs/9311102;[cs.AI];Computer Science > Artificial Intelligence;Software Agents: Completing Patterns and Constructing User Interfaces;[Submitted on 1 Nov 1993];J. C. Schlimmer;L. A. Hermens;;;;To support the goal of allowing users to record and retrieve information,this paper describes an interactive note-taking system for pen-based computerswith two distinctive features. First, it actively predicts what the user isgoing to write. Second, it automatically constructs a custom, button-box userinterface on request. The system is an example of a learning-apprenticesoftware- agent. A machine learning component characterizes the syntax andsemantics of the user's information. A performance system uses this learnedinformation to generate completion strings and construct a user interface.Description of Online Appendix: People like to record information. Doing thison paper is initially efficient, but lacks flexibility. Recording informationon a computer is less efficient but more powerful. In our new note takingsoftwre, the user records information directly on a computer. Behind theinterface, an agent acts for the user. To help, it provides defaults andconstructs a custom user interface. The demonstration is a QuickTime movie ofthe note taking agent in action. The file is a binhexed self-extractingarchive. Macintosh utilities for binhex are available from;https://arxiv.org/abs/cs/9311102
arXiv:cs/9312101;[cs.AI];Computer Science > Artificial Intelligence;Decidable Reasoning in Terminological Knowledge Representation Systems;[Submitted on 1 Dec 1993];M. Buchheit;F. M. Donini;A. Schaerf;;;Terminological knowledge representation systems (TKRSs) are tools fordesigning and using knowledge bases that make use of terminological languages(or concept languages). We analyze from a theoretical point of view a TKRSwhose capabilities go beyond the ones of presently available TKRSs. The newfeatures studied, often required in practical applications, can be summarizedin three main points. First, we consider a highly expressive terminologicallanguage, called ALCNR, including general complements of concepts, numberrestrictions and role conjunction. Second, we allow to express inclusionstatements between general concepts, and terminological cycles as a particularcase. Third, we prove the decidability of a number of desirable TKRS-deductionservices (like satisfiability, subsumption and instance checking) through asound, complete and terminating calculus for reasoning in ALCNR-knowledgebases. Our calculus extends the general technique of constraint systems. As abyproduct of the proof, we get also the result that inclusion statements inALCNR can be simulated by terminological cycles, if descriptive semantics isadopted.;https://arxiv.org/abs/cs/9312101
arXiv:cs/9311101;[cs.AI];Computer Science > Artificial Intelligence;The Difficulties of Learning Logic Programs with Cut;[Submitted on 1 Nov 1993];F. Bergadano;D. Gunetti;U. Trinchero;;;As real logic programmers normally use cut (!), an effective learningprocedure for logic programs should be able to deal with it. Because the cutpredicate has only a procedural meaning, clauses containing cut cannot belearned using an extensional evaluation method, as is done in most learningsystems. On the other hand, searching a space of possible programs (instead ofa space of independent clauses) is unfeasible. An alternative solution is togenerate first a candidate base program which covers the positive examples, andthen make it consistent by inserting cut where appropriate. The problem oflearning programs with cut has not been investigated before and this seems tobe a natural and reasonable approach. We generalize this scheme and investigatethe difficulties that arise. Some of the major shortcomings are actuallycaused, in general, by the need for intensional evaluation. As a conclusion,the analysis of this paper suggests, on precise and technical grounds, thatlearning cut is difficult, and current induction techniques should probably berestricted to purely declarative logic languages.;https://arxiv.org/abs/cs/9311101
arXiv:cs/9403101;[cs.AI];Computer Science > Artificial Intelligence;Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction;[Submitted on 1 Mar 1994];P. M. Murphy;M. J. Pazzani;;;;We report on a series of experiments in which all decision trees consistentwith the training data are constructed. These experiments were run to gain anunderstanding of the properties of the set of consistent decision trees and thefactors that affect the accuracy of individual trees. In particular, weinvestigated the relationship between the size of a decision tree consistentwith some training data and the accuracy of the tree on test data. Theexperiments were performed on a massively parallel Maspar computer. The resultsof the experiments on several artificial and two real world problems indicatethat, for many of the problems investigated, smaller consistent decision treesare on average less accurate than the average accuracy of slightly largertrees.;https://arxiv.org/abs/cs/9403101
arXiv:cs/9402103;[cs.AI];Computer Science > Artificial Intelligence;Bias-Driven Revision of Logical Domain Theories;[Submitted on 1 Feb 1994];M. Koppel;R. Feldman;A. M. Segre;;;The theory revision problem is the problem of how best to go about revising adeficient domain theory using information contained in examples that exposeinaccuracies. In this paper we present our approach to the theory revisionproblem for propositional domain theories. The approach described here, calledPTR, uses probabilities associated with domain theory elements to numericallytrack the ``flow'' of proof through the theory. This allows us to measure theprecise role of a clause or literal in allowing or preventing a (desired orundesired) derivation for a given example. This information is used toefficiently locate and repair flawed elements of the theory. PTR is proved toconverge to a theory which correctly classifies all examples, and shownexperimentally to be fast and accurate even for deep theories.;https://arxiv.org/abs/cs/9402103
arXiv:cs/9308101;[cs.AI];Computer Science > Artificial Intelligence;Dynamic Backtracking;[Submitted on 1 Aug 1993];M. L. Ginsberg;;;;;Because of their occasional need to return to shallow points in a searchtree, existing backtracking methods can sometimes erase meaningful progresstoward solving a search problem. In this paper, we present a method by whichbacktrack points can be moved deeper in the search space, thereby avoiding thisdifficulty. The technique developed is a variant of dependency-directedbacktracking that uses only polynomial space while still providing usefulcontrol information and retaining the completeness guarantees provided byearlier approaches.;https://arxiv.org/abs/cs/9308101
arXiv:cs/9402102;[cs.AI];Computer Science > Artificial Intelligence;Substructure Discovery Using Minimum Description Length and Background Knowledge;[Submitted on 1 Feb 1994];D. J. Cook;L. B. Holder;;;;The ability to identify interesting and repetitive substructures is anessential component to discovering knowledge in structural data. We describe anew version of our SUBDUE substructure discovery system based on the minimumdescription length principle. The SUBDUE system discovers substructures thatcompress the original data and represent structural concepts in the data. Byreplacing previously-discovered substructures in the data, multiple passes ofSUBDUE produce a hierarchical description of the structural regularities in thedata. SUBDUE uses a computationally-bounded inexact graph match that identifiessimilar, but not identical, instances of a substructure and finds anapproximate measure of closeness of two substructures when under computationalconstraints. In addition to the minimum description length principle, otherbackground knowledge can be used by SUBDUE to guide the search towards moreappropriate substructures. Experiments in a variety of domains demonstrateSUBDUE's ability to find substructures capable of compressing the original dataand to discover structural concepts important to the domain. Description ofOnline Appendix: This is a compressed tar file containing the SUBDUE discoverysystem, written in C. The program accepts as input databases represented ingraph form, and will output discovered substructures with their correspondingvalue.;https://arxiv.org/abs/cs/9402102
arXiv:cs/9406102;[cs.AI];Computer Science > Artificial Intelligence;Applying GSAT to Non-Clausal Formulas;[Submitted on 1 Jun 1994];R. Sebastiani;;;;;In this paper we describe how to modify GSAT so that it can be applied tonon-clausal formulas. The idea is to use a particular ``score'' function whichgives the number of clauses of the CNF conversion of a formula which are falseunder a given truth assignment. Its value is computed in linear time, withoutconstructing the CNF conversion itself. The proposed methodology applies tomost of the variants of GSAT proposed so far.;https://arxiv.org/abs/cs/9406102
arXiv:cs/9402101;[cs.AI];Computer Science > Artificial Intelligence;Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models;[Submitted on 1 Feb 1994];C. X. Ling;;;;;Learning the past tense of English verbs - a seemingly minor aspect oflanguage acquisition - has generated heated debates since 1986, and has becomea landmark task for testing the adequacy of cognitive modeling. Severalartificial neural networks (ANNs) have been implemented, and a challenge forbetter symbolic models has been posed. In this paper, we present ageneral-purpose Symbolic Pattern Associator (SPA) based upon the decision-treelearning algorithm ID3. We conduct extensive head-to-head comparisons on thegeneralization ability between ANN models and the SPA under differentrepresentations. We conclude that the SPA generalizes the past tense of unseenverbs better than ANN models by a wide margin, and we offer insights as to whythis should be the case. We also discuss a new default strategy fordecision-tree learning algorithms.;https://arxiv.org/abs/cs/9402101
arXiv:cs/9406101;[cs.AI];Computer Science > Artificial Intelligence;A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic;[Submitted on 1 Jun 1994];A. Borgida;P. F. Patel-Schneider;;;;This paper analyzes the correctness of the subsumption algorithm used inCLASSIC, a description logic-based knowledge representation system that isbeing used in practical applications. In order to deal efficiently withindividuals in CLASSIC descriptions, the developers have had to use analgorithm that is incomplete with respect to the standard, model-theoreticsemantics for description logics. We provide a variant semantics fordescriptions with respect to which the current implementation is complete, andwhich can be independently motivated. The soundness and completeness of thepolynomial-time subsumption algorithm is established using description graphs,which are an abstracted version of the implementation structures used inCLASSIC, and are of independent interest.;https://arxiv.org/abs/cs/9406101
arXiv:cs/9401101;[cs.AI];Computer Science > Artificial Intelligence;Teleo-Reactive Programs for Agent Control;[Submitted on 1 Jan 1994];N. Nilsson;;;;;"A formalism is presented for computing and organizing actions for autonomousagents in dynamic environments. We introduce the notion of teleo-reactive (T-R)programs whose execution entails the construction of circuitry for thecontinuous computation of the parameters and conditions on which agent actionis based. In addition to continuous feedback, T-R programs support parameterbinding and recursion. A primary difference between T-R programs and many othercircuit-based systems is that the circuitry of T-R programs is more compact; itis constructed at run time and thus does not have to anticipate all thecontingencies that might arise over all possible runs. In addition, T-Rprograms are intuitive and easy to write and are written in a form that iscompatible with automatic planning and learning methods. We briefly describesome experimental applications of T-R programs in the control of simulated andactual mobile robots.";https://arxiv.org/abs/cs/9401101
arXiv:cs/9408103;[cs.AI];Computer Science > Artificial Intelligence;A System for Induction of Oblique Decision Trees;[Submitted on 1 Aug 1994];S. K. Murthy;S. Kasif;S. Salzberg;;;This article describes a new system for induction of oblique decision trees.This system, OC1, combines deterministic hill-climbing with two forms ofrandomization to find a good oblique split (in the form of a hyperplane) ateach node of a decision tree. Oblique decision tree methods are tunedespecially for domains in which the attributes are numeric, although they canbe adapted to symbolic or mixed symbolic/numeric attributes. We presentextensive empirical studies, using both real and artificial data, that analyzeOC1's ability to construct oblique trees that are smaller and more accuratethan their axis-parallel counterparts. We also examine the benefits ofrandomization for the construction of oblique decision trees.;https://arxiv.org/abs/cs/9408103
arXiv:cs/9409101;[cs.AI];Computer Science > Artificial Intelligence;On Planning while Learning;[Submitted on 1 Sep 1994];S. Safra;M. Tennenholtz;;;;This paper introduces a framework for Planning while Learning where an agentis given a goal to achieve in an environment whose behavior is only partiallyknown to the agent. We discuss the tractability of various plan-designprocesses. We show that for a large natural class of Planning while Learningsystems, a plan can be presented and verified in a reasonable time. However,coming up algorithmically with a plan, even for simple classes of systems isapparently intractable. We emphasize the role of off-line plan-designprocesses, and show that, in most natural cases, the verification (projection)part can be carried out in an efficient algorithmic manner.;https://arxiv.org/abs/cs/9409101
arXiv:cs/9408102;[cs.AI];Computer Science > Artificial Intelligence;Pattern Matching and Discourse Processing in Information Extraction from Japanese Text;[Submitted on 1 Aug 1994];T. Kitani;Y. Eriguchi;M. Hara;;;"Information extraction is the task of automatically picking up information ofinterest from an unconstrained text. Information of interest is usuallyextracted in two steps. First, sentence level processing locates relevantpieces of information scattered throughout the text; second, discourseprocessing merges coreferential information to generate the output. In thefirst step, pieces of information are locally identified without recognizingany relationships among them. A key word search or simple pattern search canachieve this purpose. The second step requires deeper knowledge in order tounderstand relationships among separately identified pieces of information.Previous information extraction systems focused on the first step, partlybecause they were not required to link up each piece of information with otherpieces. To link the extracted pieces of information and map them onto astructured output format, complex discourse processing is essential. This paperreports on a Japanese information extraction system that merges informationusing a pattern matcher and discourse processor. Evaluation results show a highlevel of system performance which approaches human performance.";https://arxiv.org/abs/cs/9408102
arXiv:cs/9408101;[cs.AI];Computer Science > Artificial Intelligence;Random Worlds and Maximum Entropy;[Submitted on 1 Aug 1994];A. J. Grove;J. Y. Halpern;D. Koller;;;Given a knowledge base KB containing first-order and statistical facts, weconsider a principled method, called the random-worlds method, for computing adegree of belief that some formula Phi holds given KB. If we are reasoningabout a world or system consisting of N individuals, then we can consider allpossible worlds, or first-order models, with domain {1,...,N} that satisfy KB,and compute the fraction of them in which Phi is true. We define the degree ofbelief to be the asymptotic value of this fraction as N grows large. We showthat when the vocabulary underlying Phi and KB uses constants and unarypredicates only, we can naturally associate an entropy with each world. As Ngrows larger, there are many more worlds with higher entropy. Therefore, we canuse a maximum-entropy computation to compute the degree of belief. This resultis in a similar spirit to previous work in physics and artificial intelligence,but is far more general. Of equal interest to the result itself are thelimitations on its scope. Most importantly, the restriction to unary predicatesseems necessary. Although the random-worlds method makes sense in general, theconnection to maximum entropy seems to disappear in the non-unary case. Theseobservations suggest unexpected limitations to the applicability ofmaximum-entropy methods.;https://arxiv.org/abs/cs/9408101
arXiv:cs/9412103;[cs.AI];Computer Science > Artificial Intelligence;Total-Order and Partial-Order Planning: A Comparative Analysis;[Submitted on 1 Dec 1994];S. Minton;J. Bresina;M. Drummond;;;For many years, the intuitions underlying partial-order planning were largelytaken for granted. Only in the past few years has there been renewed interestin the fundamental principles underlying this paradigm. In this paper, wepresent a rigorous comparative analysis of partial-order and total-orderplanning by focusing on two specific planners that can be directly compared. Weshow that there are some subtle assumptions that underly the wide-spreadintuitions regarding the supposed efficiency of partial-order planning. Forinstance, the superiority of partial-order planning can depend critically uponthe search strategy and the structure of the search space. Understanding theunderlying assumptions is crucial for constructing efficient planners.;https://arxiv.org/abs/cs/9412103
arXiv:cs/9412102;[cs.AI];Computer Science > Artificial Intelligence;Operations for Learning with Graphical Models;[Submitted on 1 Dec 1994];W. L. Buntine;;;;;This paper is a multidisciplinary review of empirical, statistical learningfrom a graphical model perspective. Well-known examples of graphical modelsinclude Bayesian networks, directed graphs representing a Markov chain, andundirected networks representing a Markov field. These graphical models areextended to model data analysis and empirical learning using the notation ofplates. Graphical operations for simplifying and manipulating a problem areprovided including decomposition, differentiation, and the manipulation ofprobability models from the exponential family. Two standard algorithm schemasfor learning are reviewed in a graphical framework: Gibbs sampling and theexpectation maximization algorithm. Using these operations and schemas, somepopular algorithms can be synthesized from their graphical specification. Thisincludes versions of linear regression, techniques for feed-forward networks,and learning Gaussian and discrete Bayesian networks from data. The paperconcludes by sketching some implications for data analysis and summarizing howsome popular algorithms fall within the framework presented. The main originalcontributions here are the decomposition techniques and the demonstration thatgraphical models provide a framework for understanding and developing complexlearning algorithms.;https://arxiv.org/abs/cs/9412102
arXiv:cs/9412101;[cs.AI];Computer Science > Artificial Intelligence;Wrap-Up: a Trainable Discourse Module for Information Extraction;[Submitted on 1 Dec 1994];S. Soderland;Lehnert. W;;;;The vast amounts of on-line text now available have led to renewed interestin information extraction (IE) systems that analyze unrestricted text,producing a structured representation of selected information from the text.This paper presents a novel approach that uses machine learning to acquireknowledge for some of the higher level IE processing. Wrap-Up is a trainable IEdiscourse component that makes intersentential inferences and identifieslogical relations among information extracted from the text. Previouscorpus-based approaches were limited to lower level processing such aspart-of-speech tagging, lexical disambiguation, and dictionary construction.Wrap-Up is fully trainable, and not only automatically decides what classifiersare needed, but even derives the feature set for each classifier automatically.Performance equals that of a partially trainable discourse module requiringmanual customization for each domain.;https://arxiv.org/abs/cs/9412101
arXiv:cs/9501103;[cs.AI];Computer Science > Artificial Intelligence;Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning;[Submitted on 1 Jan 1995];P. Cichosz;;;;;Temporal difference (TD) methods constitute a class of methods for learningpredictions in multi-step prediction problems, parameterized by a recencyfactor lambda. Currently the most important application of these methods is totemporal credit assignment in reinforcement learning. Well known reinforcementlearning algorithms, such as AHC or Q-learning, may be viewed as instances ofTD learning. This paper examines the issues of the efficient and generalimplementation of TD(lambda) for arbitrary lambda, for use with reinforcementlearning algorithms optimizing the discounted sum of rewards. The traditionalapproach, based on eligibility traces, is argued to suffer from bothinefficiency and lack of generality. The TTD (Truncated Temporal Differences)procedure is proposed as an alternative, that indeed only approximatesTD(lambda), but requires very little computation per action and can be usedwith arbitrary function representation methods. The idea from which it isderived is fairly simple and not new, but probably unexplored so far.Encouraging experimental results are presented, suggesting that using lambda&gt 0 with the TTD procedure allows one to obtain a significant learningspeedup at essentially the same cost as usual TD(0) learning.;https://arxiv.org/abs/cs/9501103
arXiv:cs/9501102;[cs.AI];Computer Science > Artificial Intelligence;A Domain-Independent Algorithm for Plan Adaptation;[Submitted on 1 Jan 1995];S. Hanks;D. S. Weld;;;;The paradigms of transformational planning, case-based planning, and plandebugging all involve a process known as plan adaptation - modifying orrepairing an old plan so it solves a new problem. In this paper we provide adomain-independent algorithm for plan adaptation, demonstrate that it is sound,complete, and systematic, and compare it to other adaptation algorithms in theliterature. Our approach is based on a view of planning as searching a graph ofpartial plans. Generative planning starts at the graph's root and moves fromnode to node using plan-refinement operators. In planning by adaptation, alibrary plan - an arbitrary node in the plan graph - is the starting point forthe search, and the plan-adaptation algorithm can apply both the samerefinement operators available to a generative planner and can also retractconstraints and steps from the plan. Our algorithm's completeness ensures thatthe adaptation algorithm will eventually search the entire graph and itssystematicity ensures that it will do so without redundantly searching anyparts of the graph.;https://arxiv.org/abs/cs/9501102
arXiv:cs/9501101;[cs.AI];Computer Science > Artificial Intelligence;Solving Multiclass Learning Problems via Error-Correcting Output Codes;[Submitted on 1 Jan 1995];T. G. Dietterich;G. Bakiri;;;;Multiclass learning problems involve finding a definition for an unknownfunction f(x) whose range is a discrete set containing k &gt 2 values (i.e., k``classes''). The definition is acquired by studying collections of trainingexamples of the form [x_i, f (x_i)]. Existing approaches to multiclass learningproblems include direct application of multiclass algorithms such as thedecision-tree algorithms C4.5 and CART, application of binary concept learningalgorithms to learn individual binary functions for each of the k classes, andapplication of binary concept learning algorithms with distributed outputrepresentations. This paper compares these three approaches to a new techniquein which error-correcting codes are employed as a distributed outputrepresentation. We show that these output representations improve thegeneralization performance of both C4.5 and backpropagation on a wide range ofmulticlass learning tasks. We also demonstrate that this approach is robustwith respect to changes in the size of the training sample, the assignment ofdistributed representations to particular classes, and the application ofoverfitting avoidance techniques such as decision-tree pruning. Finally, weshow that---like the other methods---the error-correcting code technique canprovide reliable class probability estimates. Taken together, these resultsdemonstrate that error-correcting output codes provide a general-purpose methodfor improving the performance of inductive learning programs on multiclassproblems.;https://arxiv.org/abs/cs/9501101
arXiv:cs/9505105;[cs.AI];Computer Science > Artificial Intelligence;Pac-learning Recursive Logic Programs: Negative Results;[Submitted on 1 May 1995];W. W. Cohen;;;;;"In a companion paper it was shown that the class of constant-depthdeterminate k-ary recursive clauses is efficiently learnable. In this paper wepresent negative results showing that any natural generalization of this classis hard to learn in Valiant's model of pac-learnability. In particular, we showthat the following program classes are cryptographically hard to learn:programs with an unbounded number of constant-depth linear recursive clauses;programs with one constant-depth determinate clause containing an unboundednumber of recursive calls; and programs with one linear recursive clause ofconstant locality. These results immediately imply the non-learnability of anymore general class of programs. We also show that learning a constant-depthdeterminate program with either two linear recursive clauses or one linearrecursive clause and one non-recursive clause is as hard as learning booleanDNF. Together with positive results from the companion paper, these negativeresults establish a boundary of efficient learnability for recursivefunction-free clauses.";https://arxiv.org/abs/cs/9505105
arXiv:cs/9505104;[cs.AI];Computer Science > Artificial Intelligence;Pac-Learning Recursive Logic Programs: Efficient Algorithms;[Submitted on 1 May 1995];W. W. Cohen;;;;;We present algorithms that learn certain classes of function-free recursivelogic programs in polynomial time from equivalence queries. In particular, weshow that a single k-ary recursive constant-depth determinate clause islearnable. Two-clause programs consisting of one learnable recursive clause andone constant-depth determinate non-recursive clause are also learnable, if anadditional ``basecase'' oracle is assumed. These results immediately imply thepac-learnability of these classes. Although these classes of learnablerecursive programs are very constrained, it is shown in a companion paper thatthey are maximally general, in that generalizing either class in any naturalway leads to a computationally difficult learning problem. Thus, taken togetherwith its companion paper, this paper establishes a boundary of efficientlearnability for recursive logic programs.;https://arxiv.org/abs/cs/9505104
arXiv:cs/9505103;[cs.AI];Computer Science > Artificial Intelligence;Provably Bounded-Optimal Agents;[Submitted on 1 May 1995];S. J. Russell;D. Subramanian;;;;Since its inception, artificial intelligence has relied upon a theoreticalfoundation centered around perfect rationality as the desired property ofintelligent systems. We argue, as others have done, that this foundation isinadequate because it imposes fundamentally unsatisfiable requirements. As aresult, there has arisen a wide gap between theory and practice in AI,hindering progress in the field. We propose instead a property called boundedoptimality. Roughly speaking, an agent is bounded-optimal if its program is asolution to the constrained optimization problem presented by its architectureand the task environment. We show how to construct agents with this propertyfor a simple class of machine architectures in a broad class of real-timeenvironments. We illustrate these results using a simple model of an automatedmail sorting facility. We also define a weaker property, asymptotic boundedoptimality (ABO), that generalizes the notion of optimality in classicalcomplexity theory. We then construct universal ABO programs, i.e., programsthat are ABO no matter what real-time constraints are applied. Universal ABOprograms can be used as building blocks for more complex systems. We concludewith a discussion of the prospects for bounded optimality as a theoreticalbasis for AI, and relate it to similar trends in philosophy, economics, andgame theory.;https://arxiv.org/abs/cs/9505103
arXiv:cs/9507101;[cs.AI];Computer Science > Artificial Intelligence;Building and Refining Abstract Planning Cases by Change of Representation Language;[Submitted on 1 Jul 1995];R. Bergmann;W. Wilke;;;;ion is one of the most promising approaches to improve the performance ofproblem solvers. In several domains abstraction by dropping sentences of adomain description -- as used in most hierarchical planners -- has provenuseful. In this paper we present examples which illustrate significantdrawbacks of abstraction by dropping sentences. To overcome these drawbacks, wepropose a more general view of abstraction involving the change ofrepresentation language. We have developed a new abstraction methodology and arelated sound and complete learning algorithm that allows the complete changeof representation language of planning cases from concrete to abstract.However, to achieve a powerful change of the representation language, theabstract language itself as well as rules which describe admissible ways ofabstracting states must be provided in the domain model. This new abstractionapproach is the core of Paris (Plan Abstraction and Refinement in an IntegratedSystem), a system in which abstract planning cases are automatically learnedfrom given concrete cases. An empirical study in the domain of process planningin mechanical engineering shows significant advantages of the proposedreasoning from abstract cases over classical hierarchical planning.;https://arxiv.org/abs/cs/9507101
arXiv:cs/9506102;[cs.AI];Computer Science > Artificial Intelligence;Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs;[Submitted on 1 Jun 1995];R. J. Mooney;M. E. Califf;;;;This paper presents a method for inducing logic programs from examples thatlearns a new class of concepts called first-order decision lists, defined asordered lists of clauses each ending in a cut. The method, called FOIDL, isbased on FOIL (Quinlan, 1990) but employs intensional background knowledge andavoids the need for explicit negative examples. It is particularly useful forproblems that involve rules with specific exceptions, such as learning thepast-tense of English verbs, a task widely studied in the context of thesymbolic/connectionist debate. FOIDL is able to learn concise, accurateprograms for this problem from significantly fewer examples than previousmethods (both connectionist and symbolic).;https://arxiv.org/abs/cs/9506102
arXiv:cs/9510102;[cs.AI];Computer Science > Artificial Intelligence;Improving Connectionist Energy Minimization;[Submitted on 1 Oct 1995];G. Pinkas;R. Dechter;;;;Symmetric networks designed for energy minimization such as Boltzman machinesand Hopfield nets are frequently investigated for use in optimization,constraint satisfaction and approximation of NP-hard problems. Nevertheless,finding a global solution (i.e., a global minimum for the energy function) isnot guaranteed and even a local solution may take an exponential number ofsteps. We propose an improvement to the standard local activation function usedfor such networks. The improved algorithm guarantees that a global minimum isfound in linear time for tree-like subnetworks. The algorithm, called activate,is uniform and does not assume that the network is tree-like. It can identifytree-like subnetworks even in cyclic topologies (arbitrary networks) and avoidlocal minima along these trees. For acyclic networks, the algorithm isguaranteed to converge to a global minimum from any initial state of the system(self-stabilization) and remains correct under various types of schedulers. Onthe negative side, we show that in the presence of cycles, no uniform algorithmexists that guarantees optimality even under a sequential asynchronousscheduler. An asynchronous scheduler can activate only one unit at a time whilea synchronous scheduler can activate any number of units in a single time step.In addition, no uniform algorithm exists to optimize even acyclic networks whenthe scheduler is synchronous. Finally, we show how the algorithm can beimproved using the cycle-cutset scheme. The general algorithm, calledactivate-with-cutset, improves over activate and has some performanceguarantees that are related to the size of the network's cycle-cutset.;https://arxiv.org/abs/cs/9510102
arXiv:cs/9510101;[cs.AI];Computer Science > Artificial Intelligence;Diffusion of Context and Credit Information in Markovian Models;[Submitted on 1 Oct 1995];Y. Bengio;P. Frasconi;;;;This paper studies the problem of ergodicity of transition probabilitymatrices in Markovian models, such as hidden Markov models (HMMs), and how itmakes very difficult the task of learning to represent long-term context forsequential data. This phenomenon hurts the forward propagation of long-termcontext information, as well as learning a hidden state representation torepresent long-term context, which depends on propagating credit informationbackwards in time. Using results from Markov chain theory, we show that thisproblem of diffusion of context and credit is reduced when the transitionprobabilities approach 0 or 1, i.e., the transition probability matrices aresparse and the model essentially deterministic. The results found in this paperapply to learning approaches based on continuous optimization, such as gradientdescent and the Baum-Welch algorithm.;https://arxiv.org/abs/cs/9510101
arXiv:cs/9510103;[cs.AI];Computer Science > Artificial Intelligence;Learning Membership Functions in a Function-Based Object Recognition System;[Submitted on 1 Oct 1995];K. Woods;D. Cook;L. Hall;K. Bowyer;L. Stark;Functionality-based recognition systems recognize objects at the categorylevel by reasoning about how well the objects support the expected function.Such systems naturally associate a ``measure of goodness'' or ``membershipvalue'' with a recognized object. This measure of goodness is the result ofcombining individual measures, or membership values, from potentially manyprimitive evaluations of different properties of the object's shape. Amembership function is used to compute the membership value when evaluating aprimitive of a particular physical property of an object. In previous versionsof a recognition system known as Gruff, the membership function for each of theprimitive evaluations was hand-crafted by the system designer. In this paper,we provide a learning component for the Gruff system, called Omlet, thatautomatically learns membership functions given a set of example objectslabeled with their desired category measure. The learning algorithm isgenerally applicable to any problem in which low-level membership values arecombined through an and-or tree structure to give a final overall membershipvalue.;https://arxiv.org/abs/cs/9510103
arXiv:cs/9601101;[cs.AI];Computer Science > Artificial Intelligence;The Design and Experimental Analysis of Algorithms for Temporal Reasoning;[Submitted on 1 Jan 1996];P. vanBeek;D. W. Manchak;;;;Many applications -- from planning and scheduling to problems in molecularbiology -- rely heavily on a temporal reasoning component. In this paper, wediscuss the design and empirical analysis of algorithms for a temporalreasoning system based on Allen's influential interval-based framework forrepresenting temporal information. At the core of the system are algorithms fordetermining whether the temporal information is consistent, and, if so, findingone or more scenarios that are consistent with the temporal information. Twoimportant algorithms for these tasks are a path consistency algorithm and abacktracking algorithm. For the path consistency algorithm, we developtechniques that can result in up to a ten-fold speedup over an already highlyoptimized implementation. For the backtracking algorithm, we develop variableand value ordering heuristics that are shown empirically to dramaticallyimprove the performance of the algorithm. As well, we show that a previouslysuggested reformulation of the backtracking search problem can reduce the timeand space requirements of the backtracking search. Taken together, thetechniques we develop allow a temporal reasoning component to solve problemsthat are of practical size.;https://arxiv.org/abs/cs/9601101
arXiv:cs/9512107;[cs.AI];Computer Science > Artificial Intelligence;Rule-based Machine Learning Methods for Functional Prediction;[Submitted on 1 Dec 1995];S. M. Weiss;N. Indurkhya;;;;We describe a machine learning method for predicting the value of areal-valued function, given the values of multiple input variables. The methodinduces solutions from samples in the form of ordered disjunctive normal form(DNF) decision rules. A central objective of the method and representation isthe induction of compact, easily interpretable solutions. This rule-baseddecision model can be extended to search efficiently for similar cases prior toapproximating function values. Experimental results on real-world datademonstrate that the new techniques are competitive with existing machinelearning and statistical methods and can sometimes yield superior regressionperformance.;https://arxiv.org/abs/cs/9512107
arXiv:cs/9602102;[cs.AI];Computer Science > Artificial Intelligence;Logarithmic-Time Updates and Queries in Probabilistic Networks;[Submitted on 1 Feb 1996];A. L. Delcher;A. J. Grove;S. Kasif;J. Pearl;;Traditional databases commonly support efficient query and update proceduresthat operate in time which is sublinear in the size of the database. Our goalin this paper is to take a first step toward dynamic reasoning in probabilisticdatabases with comparable efficiency. We propose a dynamic data structure thatsupports efficient algorithms for updating and querying singly connectedBayesian networks. In the conventional algorithm, new evidence is absorbed inO(1) time and queries are processed in time O(N), where N is the size of thenetwork. We propose an algorithm which, after a preprocessing phase, allows usto answer queries in time O(log N) at the expense of O(log N) time per evidenceabsorption. The usefulness of sub-linear processing time manifests itself inapplications requiring (near) real-time response over large probabilisticdatabases. We briefly discuss a potential application of dynamic probabilisticreasoning in computational biology.;https://arxiv.org/abs/cs/9602102
arXiv:cs/9602101;[cs.AI];Computer Science > Artificial Intelligence;Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences;[Submitted on 1 Feb 1996];G. Brewka;;;;;The paper describes an extension of well-founded semantics for logic programswith two types of negation. In this extension information about preferencesbetween rules can be expressed in the logical language and derived dynamically.This is achieved by using a reserved predicate symbol and a naming technique.Conflicts among rules are resolved whenever possible on the basis of derivedpreference information. The well-founded conclusions of prioritized logicprograms can be computed in polynomial time. A legal reasoning exampleillustrates the usefulness of the approach.;https://arxiv.org/abs/cs/9602101
arXiv:cs/9603104;[cs.AI];Computer Science > Artificial Intelligence;Active Learning with Statistical Models;[Submitted on 1 Mar 1996];D. A. Cohn;Z. Ghahramani;M. I. Jordan;;;For many types of machine learning algorithms, one can compute thestatistically `optimal' way to select training data. In this paper, we reviewhow optimal data selection techniques have been used with feedforward neuralnetworks. We then show how the same principles may be used to select data fortwo alternative, statistically-based learning architectures: mixtures ofGaussians and locally weighted regression. While the techniques for neuralnetworks are computationally expensive and approximate, the techniques formixtures of Gaussians and locally weighted regression are both efficient andaccurate. Empirically, we observe that the optimality criterion sharplydecreases the number of training examples the learner needs in order to achievegood performance.;https://arxiv.org/abs/cs/9603104
arXiv:cs/9605106;[cs.AI];Computer Science > Artificial Intelligence;2Planning for Contingencies: A Decision-based Approach;[Submitted on 1 May 1996];L. Pryor;G. Collins;;;;A fundamental assumption made by classical AI planners is that there is nouncertainty in the world: the planner has full knowledge of the conditionsunder which the plan will be executed and the outcome of every action is fullypredictable. These planners cannot therefore construct contingency plans, i.e.,plans in which different actions are performed in different circumstances. Inthis paper we discuss some issues that arise in the representation andconstruction of contingency plans and describe Cassandra, a partial-ordercontingency planner. Cassandra uses explicit decision-steps that enable theagent executing the plan to decide which plan branch to follow. Thedecision-steps in a plan result in subgoals to acquire knowledge, which areplanned for in the same way as any other subgoals. Cassandra thus distinguishesthe process of gathering information from the process of making decisions. Theexplicit representation of decisions in Cassandra allows a coherent approach tothe problems of contingent planning, and provides a solid base for extensionssuch as the use of different decision-making procedures.;https://arxiv.org/abs/cs/9605106
arXiv:cs/9605105;[cs.AI];Computer Science > Artificial Intelligence;A Formal Framework for Speedup Learning from Problems and Solutions;[Submitted on 1 May 1996];P. Tadepalli;B. K. Natarajan;;;;Speedup learning seeks to improve the computational efficiency of problemsolving with experience. In this paper, we develop a formal framework forlearning efficient problem solving from random problems and their solutions. Weapply this framework to two different representations of learned knowledge,namely control rules and macro-operators, and prove theorems that identifysufficient conditions for learning in each representation. Our proofs areconstructive in that they are accompanied with learning algorithms. Ourframework captures both empirical and explanation-based speedup learning in aunified fashion. We illustrate our framework with implementations in twodomains: symbolic integration and Eight Puzzle. This work integrates manystrands of experimental and theoretical work in machine learning, includingempirical learning of control rules, macro-operator learning, Explanation-BasedLearning (EBL), and Probably Approximately Correct (PAC) Learning.;https://arxiv.org/abs/cs/9605105
arXiv:cs/9605104;[cs.AI];Computer Science > Artificial Intelligence;Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study;[Submitted on 1 May 1996];J. Gratch;S. Chien;;;;Although most scheduling problems are NP-hard, domain specific techniquesperform well in practice but are quite expensive to construct. In adaptiveproblem-solving solving, domain specific knowledge is acquired automaticallyfor a general problem solver with a flexible control architecture. In thisapproach, a learning system explores a space of possible heuristic methods forone well-suited to the eccentricities of the given domain and problemdistribution. In this article, we discuss an application of the approach toscheduling satellite communications. Using problem distributions based onactual mission requirements, our approach identifies strategies that not onlydecrease the amount of CPU time required to produce schedules, but alsoincrease the percentage of problems that are solvable within computationalresource limitations.;https://arxiv.org/abs/cs/9605104
arXiv:cs/9606102;[cs.AI];Computer Science > Artificial Intelligence;On Partially Controlled Multi-Agent Systems;[Submitted on 1 Jun 1996];R. I. Brafman;M. Tennenholtz;;;;Motivated by the control theoretic distinction between controllable anduncontrollable events, we distinguish between two types of agents within amulti-agent system: controllable agents, which are directly controlled by thesystem's designer, and uncontrollable agents, which are not under thedesigner's direct control. We refer to such systems as partially controlledmulti-agent systems, and we investigate how one might influence the behavior ofthe uncontrolled agents through appropriate design of the controlled agents. Inparticular, we wish to understand which problems are naturally described inthese terms, what methods can be applied to influence the uncontrollableagents, the effectiveness of such methods, and whether similar methods workacross different domains. Using a game-theoretic framework, this paper studiesthe design of partially controlled multi-agent systems in two contexts: in onecontext, the uncontrollable agents are expected utility maximizers, while inthe other they are reinforcement learners. We suggest different techniques forcontrolling agents' behavior in each domain, assess their success, and examinetheir relationship.;https://arxiv.org/abs/cs/9606102
arXiv:cs/9606101;[cs.AI];Computer Science > Artificial Intelligence;A Principled Approach Towards Symbolic Geometric Constraint Satisfaction;[Submitted on 1 Jun 1996];S. Bhansali;G. A. Kramer;T. J. Hoar;;;An important problem in geometric reasoning is to find the configuration of acollection of geometric bodies so as to satisfy a set of given constraints.Recently, it has been suggested that this problem can be solved efficiently bysymbolically reasoning about geometry. This approach, called degrees of freedomanalysis, employs a set of specialized routines called plan fragments thatspecify how to change the configuration of a set of bodies to satisfy a newconstraint while preserving existing constraints. A potential drawback, whichlimits the scalability of this approach, is concerned with the difficulty ofwriting plan fragments. In this paper we address this limitation by showing howthese plan fragments can be automatically synthesized using first principlesabout geometric bodies, actions, and topology.;https://arxiv.org/abs/cs/9606101
arXiv:cs/9605103;[cs.AI];Computer Science > Artificial Intelligence;Reinforcement Learning: A Survey;[Submitted on 1 May 1996];L. P. Kaelbling;M. L. Littman;A. W. Moore;;;This paper surveys the field of reinforcement learning from acomputer-science perspective. It is written to be accessible to researchersfamiliar with machine learning. Both the historical basis of the field and abroad selection of current work are summarized. Reinforcement learning is theproblem faced by an agent that learns behavior through trial-and-errorinteractions with a dynamic environment. The work described here has aresemblance to work in psychology, but differs considerably in the details andin the use of the word ``reinforcement.'' The paper discusses central issues ofreinforcement learning, including trading off exploration and exploitation,establishing the foundations of the field via Markov decision theory, learningfrom delayed reinforcement, constructing empirical models to acceleratelearning, making use of generalization and hierarchy, and coping with hiddenstate. It concludes with a survey of some implemented systems and an assessmentof the practical utility of current methods for reinforcement learning.;https://arxiv.org/abs/cs/9605103
arXiv:cs/9605102;[cs.AI];Computer Science > Artificial Intelligence;Least Generalizations and Greatest Specializations of Sets of Clauses;[Submitted on 1 May 1996];S. H. Nienhuys-Cheng;R. deWolf;;;;The main operations in Inductive Logic Programming (ILP) are generalizationand specialization, which only make sense in a generality order. In ILP, thethree most important generality orders are subsumption, implication andimplication relative to background knowledge. The two languages used most oftenare languages of clauses and languages of only Horn clauses. This gives a totalof six different ordered languages. In this paper, we give a systematictreatment of the existence or non-existence of least generalizations andgreatest specializations of finite sets of clauses in each of these six orderedsets. We survey results already obtained by others and also contribute someanswers of our own. Our main new results are, firstly, the existence of acomputable least generalization under implication of every finite set ofclauses containing at least one non-tautologous function-free clause (amongother, not necessarily function-free clauses). Secondly, we show that such aleast generalization need not exist under relative implication, not even ifboth the set that is to be generalized and the background knowledge arefunction-free. Thirdly, we give a complete discussion of existence andnon-existence of greatest specializations in each of the six ordered languages.;https://arxiv.org/abs/cs/9605102
arXiv:cs/9605101;[cs.AI];Computer Science > Artificial Intelligence;Further Experimental Evidence against the Utility of Occam's Razor;[Submitted on 1 May 1996];G. I. Webb;;;;;This paper presents new experimental evidence against the utility of Occam'srazor. A~systematic procedure is presented for post-processing decision treesproduced by C4.5. This procedure was derived by rejecting Occam's razor andinstead attending to the assumption that similar objects are likely to belongto the same class. It increases a decision tree's complexity without alteringthe performance of that tree on the training data from which it is inferred.The resulting more complex decision trees are demonstrated to have, on average,for a variety of common learning tasks, higher predictive accuracy than theless complex original decision trees. This result raises considerable doubtabout the utility of Occam's razor as it is commonly applied in modern machinelearning.;https://arxiv.org/abs/cs/9605101
arXiv:cs/9604102;[cs.AI];Computer Science > Artificial Intelligence;Practical Methods for Proving Termination of General Logic Programs;[Submitted on 1 Apr 1996];E. Marchiori;;;;;Termination of logic programs with negated body atoms (here called generallogic programs) is an important topic. One reason is that many computationalmechanisms used to process negated atoms, like Clark's negation as failure andChan's constructive negation, are based on termination conditions. This paperintroduces a methodology for proving termination of general logic programsw.r.t. the Prolog selection rule. The idea is to distinguish parts of theprogram depending on whether or not their termination depends on the selectionrule. To this end, the notions of low-, weakly up-, and up-acceptable programare introduced. We use these notions to develop a methodology for provingtermination of general logic programs, and show how interesting problems innon-monotonic reasoning can be formalized and implemented by means ofterminating general logic programs.;https://arxiv.org/abs/cs/9604102
arXiv:cs/9604103;[cs.AI];Computer Science > Artificial Intelligence;Iterative Optimization and Simplification of Hierarchical Clusterings;[Submitted on 1 Apr 1996];D. Fisher;;;;;Clustering is often used for discovering structure in data. Clusteringsystems differ in the objective function used to evaluate clustering qualityand the control strategy used to search the space of clusterings. Ideally, thesearch strategy should consistently construct clusterings of high quality, butbe computationally inexpensive as well. In general, we cannot have it bothways, but we can partition the search so that a system inexpensively constructsa `tentative' clustering for initial examination, followed by iterativeoptimization, which continues to search in background for improved clusterings.Given this motivation, we evaluate an inexpensive strategy for creating initialclusterings, coupled with several control strategies for iterativeoptimization, each of which repeatedly modifies an initial clustering in searchof a better one. One of these methods appears novel as an iterativeoptimization strategy in clustering contexts. Once a clustering has beenconstructed it is judged by analysts -- often according to task-specificcriteria. Several authors have abstracted these criteria and posited a genericperformance task akin to pattern completion, where the error rate overcompleted patterns is used to `externally' judge clustering utility. Given thisperformance task, we adapt resampling-based pruning strategies used bysupervised learning systems to the task of simplifying hierarchicalclusterings, thus promising to ease post-clustering analysis. Finally, wepropose a number of objective functions, based on attribute-selection measuresfor decision-tree induction, that might perform well on the error rate andsimplicity dimensions.;https://arxiv.org/abs/cs/9604103
arXiv:cs/9604101;[cs.AI];Computer Science > Artificial Intelligence;A Divergence Critic for Inductive Proof;[Submitted on 1 Apr 1996];T. Walsh;;;;;Inductive theorem provers often diverge. This paper describes a simplecritic, a computer program which monitors the construction of inductive proofsattempting to identify diverging proof attempts. Divergence is recognized bymeans of a ``difference matching'' procedure. The critic then proposes lemmasand generalizations which ``ripple'' these differences away so that the proofcan go through without divergence. The critic enables the theorem prover Spiketo prove many theorems completely automatically from the definitions alone.;https://arxiv.org/abs/cs/9604101
arXiv:cs/9603103;[cs.AI];Computer Science > Artificial Intelligence;Improved Use of Continuous Attributes in C4.5;[Submitted on 1 Mar 1996];J. R. Quinlan;;;;;A reported weakness of C4.5 in domains with continuous attributes isaddressed by modifying the formation and evaluation of tests on continuousattributes. An MDL-inspired penalty is applied to such tests, eliminating someof them from consideration and altering the relative desirability of all tests.Empirical trials show that the modifications lead to smaller decision treeswith higher predictive accuracies. Results also confirm that a new version ofC4.5 incorporating these changes is superior to recent approaches that useglobal discretization and that construct small trees with multi-intervalsplits.;https://arxiv.org/abs/cs/9603103
arXiv:cs/9603102;[cs.AI];Computer Science > Artificial Intelligence;Mean Field Theory for Sigmoid Belief Networks;[Submitted on 1 Mar 1996];L. K. Saul;T. Jaakkola;M. I. Jordan;;;"We develop a mean field theory for sigmoid belief networks based on ideasfrom statistical mechanics. Our mean field theory provides a tractableapproximation to the true probability distribution in these networks; it alsoyields a lower bound on the likelihood of evidence. We demonstrate the utilityof this framework on a benchmark problem in statistical patternrecognition---the classification of handwritten digits.";https://arxiv.org/abs/cs/9603102
arXiv:cs/9512106;[cs.AI];Computer Science > Artificial Intelligence;Statistical Feature Combination for the Evaluation of Game Positions;[Submitted on 1 Dec 1995];M. Buro;;;;;This article describes an application of three well-known statistical methodsin the field of game-tree search: using a large number of classified Othellopositions, feature weights for evaluation functions with agame-phase-independent meaning are estimated by means of logistic regression,Fisher's linear discriminant, and the quadratic discriminant function fornormally distributed features. Thereafter, the playing strengths are comparedby means of tournaments between the resulting versions of a world-class Othelloprogram. In this application, logistic regression - which is used here for thefirst time in the context of game playing - leads to better results than theother approaches.;https://arxiv.org/abs/cs/9512106
arXiv:cs/9512105;[cs.AI];Computer Science > Artificial Intelligence;Translating between Horn Representations and their Characteristic Models;[Submitted on 1 Dec 1995];R. Khardon;;;;;Characteristic models are an alternative, model based, representation forHorn expressions. It has been shown that these two representations areincomparable and each has its advantages over the other. It is thereforenatural to ask what is the cost of translating, back and forth, between theserepresentations. Interestingly, the same translation questions arise indatabase theory, where it has applications to the design of relationaldatabases. This paper studies the computational complexity of these problems.Our main result is that the two translation problems are equivalent underpolynomial reductions, and that they are equivalent to the correspondingdecision problem. Namely, translating is equivalent to deciding whether a givenset of models is the set of characteristic models for a given Horn expression.We also relate these problems to the hypergraph transversal problem, a wellknown problem which is related to other applications in AI and for which nopolynomial time algorithm is known. It is shown that in general our translationproblems are at least as hard as the hypergraph transversal problem, and in aspecial case they are equivalent to it.;https://arxiv.org/abs/cs/9512105
arXiv:cs/9603101;[cs.AI];Computer Science > Artificial Intelligence;Quantum Computing and Phase Transitions in Combinatorial Search;[Submitted on 1 Mar 1996];T. Hogg;;;;;We introduce an algorithm for combinatorial search on quantum computers thatis capable of significantly concentrating amplitude into solutions for some NPsearch problems, on average. This is done by exploiting the same aspects ofproblem structure as used by classical backtrack methods to avoid unproductivesearch choices. This quantum algorithm is much more likely to find solutionsthan the simple direct use of quantum parallelism. Furthermore, empiricalevaluation on small problems shows this quantum algorithm displays the samephase transition behavior, and at the same location, as seen in many previouslystudied classical search methods. Specifically, difficult problem instances areconcentrated near the abrupt change from underconstrained to overconstrainedproblems.;https://arxiv.org/abs/cs/9603101
arXiv:cs/9512104;[cs.AI];Computer Science > Artificial Intelligence;Decision-Theoretic Foundations for Causal Reasoning;[Submitted on 1 Dec 1995];D. Heckerman;R. Shachter;;;;We present a definition of cause and effect in terms of decision-theoreticprimitives and thereby provide a principled foundation for causal reasoning.Our definition departs from the traditional view of causation in that causalassertions may vary with the set of decisions available. We argue that thisapproach provides added clarity to the notion of cause. Also in this paper, weexamine the encoding of causal relationships in directed acyclic graphs. Wedescribe a special class of influence diagrams, those in canonical form, andshow its relationship to Pearl's representation of cause and effect. Finally,we show how canonical form facilitates counterfactual reasoning.;https://arxiv.org/abs/cs/9512104
arXiv:cs/9512103;[cs.AI];Computer Science > Artificial Intelligence;Generalization of Clauses under Implication;[Submitted on 1 Dec 1995];P. Idestam-Almquist;;;;;In the area of inductive learning, generalization is a main operation, andthe usual definition of induction is based on logical implication. Recentlythere has been a rising interest in clausal representation of knowledge inmachine learning. Almost all inductive learning systems that performgeneralization of clauses use the relation theta-subsumption instead ofimplication. The main reason is that there is a well-known and simple techniqueto compute least general generalizations under theta-subsumption, but not underimplication. However generalization under theta-subsumption is inappropriatefor learning recursive clauses, which is a crucial problem since recursion isthe basic program structure of logic programs. We note that implication betweenclauses is undecidable, and we therefore introduce a stronger form ofimplication, called T-implication, which is decidable between clauses. We showthat for every finite set of clauses there exists a least generalgeneralization under T-implication. We describe a technique to reducegeneralizations under implication of a clause to generalizations undertheta-subsumption of what we call an expansion of the original clause. Moreoverwe show that for every non-tautological clause there exists a T-completeexpansion, which means that every generalization under T-implication of theclause is reduced to a generalization under theta-subsumption of the expansion.;https://arxiv.org/abs/cs/9512103
arXiv:cs/9512102;[cs.AI];Computer Science > Artificial Intelligence;Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach;[Submitted on 1 Dec 1995];A. Broggi;S. Berte;;;;"The main aim of this work is the development of a vision-based road detectionsystem fast enough to cope with the difficult real-time constraints imposed bymoving vehicle applications. The hardware platform, a special-purpose massivelyparallel system, has been chosen to minimize system production and operationalcosts. This paper presents a novel approach to expectation-driven low-levelimage segmentation, which can be mapped naturally onto mesh-connected massivelyparallel SIMD architectures capable of handling hierarchical data structures.The input image is assumed to contain a distorted version of a given template;a multiresolution stretching process is used to reshape the original templatein accordance with the acquired image content, minimizing a potential function.The distorted template is the process output.";https://arxiv.org/abs/cs/9512102
arXiv:cs/9512101;[cs.AI];Computer Science > Artificial Intelligence;OPUS: An Efficient Admissible Algorithm for Unordered Search;[Submitted on 1 Dec 1995];G. I. Webb;;;;;OPUS is a branch and bound search algorithm that enables efficient admissiblesearch through spaces for which the order of search operator application is notsignificant. The algorithm's search efficiency is demonstrated with respect tovery large machine learning search spaces. The use of admissible search is ofpotential value to the machine learning community as it means that the exactlearning biases to be employed for complex learning tasks can be preciselyspecified and manipulated. OPUS also has potential for application in otherareas of artificial intelligence, notably, truth maintenance.;https://arxiv.org/abs/cs/9512101
arXiv:cs/9511101;[cs.AI];Computer Science > Artificial Intelligence;Flexibly Instructable Agents;[Submitted on 1 Nov 1995];S. B. Huffman;J. E. Laird;;;;"This paper presents an approach to learning from situated, interactivetutorial instruction within an ongoing agent. Tutorial instruction is aflexible (and thus powerful) paradigm for teaching tasks because it allows aninstructor to communicate whatever types of knowledge an agent might need inwhatever situations might arise. To support this flexibility, however, theagent must be able to learn multiple kinds of knowledge from a broad range ofinstructional interactions. Our approach, called situated explanation, achievessuch learning through a combination of analytic and inductive techniques. Itcombines a form of explanation-based learning that is situated for eachinstruction with a full suite of contextually guided responses to incompleteexplanations. The approach is implemented in an agent called Instructo-Soarthat learns hierarchies of new tasks and other domain knowledge frominteractive natural language instructions. Instructo-Soar meets three keyrequirements of flexible instructability that distinguish it from previoussystems: (1) it can take known or unknown commands at any instruction point;(2) it can handle instructions that apply to either its current situation or toa hypothetical situation specified in language (as in, for instance,conditional instructions); and (3) it can learn, from instructions, each classof knowledge it uses to perform tasks.";https://arxiv.org/abs/cs/9511101
arXiv:cs/9508102;[cs.AI];Computer Science > Artificial Intelligence;An Integrated Framework for Learning and Reasoning;[Submitted on 1 Aug 1995];C. G. Giraud-Carrier;T. R. Martinez;;;;Learning and reasoning are both aspects of what is considered to beintelligence. Their studies within AI have been separated historically,learning being the topic of machine learning and neural networks, and reasoningfalling under classical (or symbolic) AI. However, learning and reasoning arein many ways interdependent. This paper discusses the nature of some of theseinterdependencies and proposes a general framework called FLARE, that combinesinductive learning using prior knowledge together with reasoning in apropositional setting. Several examples that test the framework are presented,including classical induction, many important reasoning protocols and twosimple expert systems.;https://arxiv.org/abs/cs/9508102
arXiv:cs/9505102;[cs.AI];Computer Science > Artificial Intelligence;Adaptive Load Balancing: A Study in Multi-Agent Learning;[Submitted on 1 May 1995];A. Schaerf;Y. Shoham;M. Tennenholtz;;;We study the process of multi-agent reinforcement learning in the context ofload balancing in a distributed system, without use of either centralcoordination or explicit communication. We first define a precise framework inwhich to study adaptive load balancing, important features of which are itsstochastic nature and the purely local information available to individualagents. Given this framework, we show illuminating results on the interplaybetween basic adaptive behavior parameters and their effect on systemefficiency. We then investigate the properties of adaptive load balancing inheterogeneous populations, and address the issue of exploration vs.exploitation in that context. Finally, we show that naive use of communicationmay not improve, and might even harm system efficiency.;https://arxiv.org/abs/cs/9505102
arXiv:cs/9508101;[cs.AI];Computer Science > Artificial Intelligence;Using Qualitative Hypotheses to Identify Inaccurate Data;[Submitted on 1 Aug 1995];Q. Zhao;T. Nishida;;;;Identifying inaccurate data has long been regarded as a significant anddifficult problem in AI. In this paper, we present a new method for identifyinginaccurate data on the basis of qualitative correlations among related data.First, we introduce the definitions of related data and qualitativecorrelations among related data. Then we put forward a new concept calledsupport coefficient function (SCF). SCF can be used to extract, represent, andcalculate qualitative correlations among related data within a dataset. Wepropose an approach to determining dynamic shift intervals of inaccurate data,and an approach to calculating possibility of identifying inaccurate data,respectively. Both of the approaches are based on SCF. Finally we present analgorithm for identifying inaccurate data by using qualitative correlationsamong related data as confirmatory or disconfirmatory evidence. We havedeveloped a practical system for interpreting infrared spectra by applying themethod, and have fully tested the system against several hundred real spectra.The experimental results show that the method is significantly better than theconventional methods used in many similar systems.;https://arxiv.org/abs/cs/9508101
arXiv:cs/9505101;[cs.AI];Computer Science > Artificial Intelligence;Using Pivot Consistency to Decompose and Solve Functional CSPs;[Submitted on 1 May 1995];P. David;;;;;"Many studies have been carried out in order to increase the search efficiencyof constraint satisfaction problems; among them, some make use of structuralproperties of the constraint network; others take into account semanticproperties of the constraints, generally assuming that all the constraintspossess the given property. In this paper, we propose a new decompositionmethod benefiting from both semantic properties of functional constraints (notbijective constraints) and structural properties of the network; furthermore,not all the constraints need to be functional. We show that under someconditions, the existence of solutions can be guaranteed. We first characterizea particular subset of the variables, which we name a root set. We thenintroduce pivot consistency, a new local consistency which is a weak form ofpath consistency and can be achieved in O(n^2d^2) complexity (instead ofO(n^3d^3) for path consistency), and we present associated properties; inparticular, we show that any consistent instantiation of the root set can belinearly extended to a solution, which leads to the presentation of theaforementioned new method for solving by decomposing functional CSPs.";https://arxiv.org/abs/cs/9505101
arXiv:cs/9506101;[cs.AI];Computer Science > Artificial Intelligence;FLECS: Planning with a Flexible Commitment Strategy;[Submitted on 1 Jun 1995];M. Veloso;P. Stone;;;;"There has been evidence that least-commitment planners can efficiently handleplanning problems that involve difficult goal interactions. This evidence hasled to the common belief that delayed-commitment is the ""best"" possibleplanning strategy. However, we recently found evidence that eager-commitmentplanners can handle a variety of planning problems more efficiently, inparticular those with difficult operator choices. Resigned to the futility oftrying to find a universally successful planning strategy, we devised a plannerthat can be used to study which domains and problems are best for whichplanning strategies. In this article we introduce this new planning algorithm,FLECS, which uses a FLExible Commitment Strategy with respect to plan-steporderings. It is able to use any strategy from delayed-commitment toeager-commitment. The combination of delayed and eager operator-orderingcommitments allows FLECS to take advantage of the benefits of explicitly usinga simulated execution state and reasoning about planning constraints. FLECS canvary its commitment strategy across different problems and domains, and alsoduring the course of a single planning problem. FLECS represents a novelcontribution to planning in that it explicitly provides the choice of whichcommitment strategy to use while planning. FLECS provides a framework toinvestigate the mapping from planning domains and problems to efficientplanning strategies.";https://arxiv.org/abs/cs/9506101
arXiv:cs/9504101;[cs.AI];Computer Science > Artificial Intelligence;Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach;[Submitted on 1 Apr 1995];S. K. Donoho;L. A. Rendell;;;;Theory revision integrates inductive learning and background knowledge bycombining training examples with a coarse domain theory to produce a moreaccurate theory. There are two challenges that theory revision and othertheory-guided systems face. First, a representation language appropriate forthe initial theory may be inappropriate for an improved theory. While theoriginal representation may concisely express the initial theory, a moreaccurate theory forced to use that same representation may be bulky,cumbersome, and difficult to reach. Second, a theory structure suitable for acoarse domain theory may be insufficient for a fine-tuned theory. Systems thatproduce only small, local changes to a theory have limited value foraccomplishing complex structural alterations that may be required.Consequently, advanced theory-guided learning systems require flexiblerepresentation and flexible structure. An analysis of various theory revisionsystems and theory-guided learning systems reveals specific strengths andweaknesses in terms of these two desired properties. Designed to capture theunderlying qualities of each system, a new system uses theory-guidedconstructive induction. Experiments in three domains show improvement overprevious theory-guided systems. This leads to a study of the behavior,limitations, and potential of theory-guided constructive induction.;https://arxiv.org/abs/cs/9504101
arXiv:cs/9503102;[cs.AI];Computer Science > Artificial Intelligence;Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm;[Submitted on 1 Mar 1995];P. D. Turney;;;;;This paper introduces ICET, a new algorithm for cost-sensitiveclassification. ICET uses a genetic algorithm to evolve a population of biasesfor a decision tree induction algorithm. The fitness function of the geneticalgorithm is the average cost of classification when using the decision tree,including both the costs of tests (features, measurements) and the costs ofclassification errors. ICET is compared here with three other algorithms forcost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,which classifies without regard to cost. The five algorithms are evaluatedempirically on five real-world medical datasets. Three sets of experiments areperformed. The first set examines the baseline performance of the fivealgorithms on the five datasets and establishes that ICET performssignificantly better than its competitors. The second set tests the robustnessof ICET under a variety of conditions and shows that ICET maintains itsadvantage. The third set looks at ICET's search in bias space and discovers away to improve the search.;https://arxiv.org/abs/cs/9503102
arXiv:cs/9503101;[cs.AI];Computer Science > Artificial Intelligence;On the Informativeness of the DNA Promoter Sequences Domain Theory;[Submitted on 1 Mar 1995];J. Ortega;;;;;The DNA promoter sequences domain theory and database have become popular fortesting systems that integrate empirical and analytical learning. This notereports a simple change and reinterpretation of the domain theory in terms ofM-of-N concepts, involving no learning, that results in an accuracy of 93.4% onthe 106 items of the database. Moreover, an exhaustive search of the space ofM-of-N domain theory interpretations indicates that the expected accuracy of arandomly chosen interpretation is 76.5%, and that a maximum accuracy of 97.2%is achieved in 12 cases. This demonstrates the informativeness of the domaintheory, without the complications of understanding the interactions betweenvarious learning algorithms and the theory. In addition, our results helpcharacterize the difficulty of learning using the DNA promoters theory.;https://arxiv.org/abs/cs/9503101
arXiv:cs/9609101;[cs.AI];Computer Science > Artificial Intelligence;Accelerating Partial-Order Planners: Some Techniques for Effective Search Control and Pruning;[Submitted on 1 Sep 1996];A. Gerevini;L. Schubert;;;;We propose some domain-independent techniques for bringing well-foundedpartial-order planners closer to practicality. The first two techniques areaimed at improving search control while keeping overhead costs low. One isbased on a simple adjustment to the default A* heuristic used by UCPOP toselect plans for refinement. The other is based on preferring ``zerocommitment'' (forced) plan refinements whenever possible, and using LIFOprioritization otherwise. A more radical technique is the use of operatorparameter domains to prune search. These domains are initially computed fromthe definitions of the operators and the initial and goal conditions, using apolynomial-time algorithm that propagates sets of constants through theoperator graph, starting in the initial conditions. During planning, parameterdomains can be used to prune nonviable operator instances and to removespurious clobbering threats. In experiments based on modifications of UCPOP,our improved plan and goal selection strategies gave speedups by factorsranging from 5 to more than 1000 for a variety of problems that are nontrivialfor the unmodified version. Crucially, the hardest problems gave the greatestimprovements. The pruning technique based on parameter domains often gavespeedups by an order of magnitude or more for difficult problems, both with thedefault UCPOP search strategy and with our improved strategy. The Lisp code forour techniques and for the test problems is provided in on-line appendices.;https://arxiv.org/abs/cs/9609101
arXiv:cs/9610102;[cs.AI];Computer Science > Artificial Intelligence;Learning First-Order Definitions of Functions;[Submitted on 1 Oct 1996];J. R. Quinlan;;;;;First-order learning involves finding a clause-form definition of a relationfrom examples of the relation and relevant background information. In thispaper, a particular first-order learning system is modified to customize it forfinding definitions of functional relations. This restriction leads to fasterlearning times and, in some cases, to definitions that have higher predictiveaccuracy. Other first-order learning systems might benefit from similarspecialization.;https://arxiv.org/abs/cs/9610102
arXiv:cs/9610101;[cs.AI];Computer Science > Artificial Intelligence;Mechanisms for Automated Negotiation in State Oriented Domains;[Submitted on 1 Oct 1996];G. Zlotkin;J. S. Rosenschein;;;;This paper lays part of the groundwork for a domain theory of negotiation,that is, a way of classifying interactions so that it is clear, given a domain,which negotiation mechanisms and strategies are appropriate. We define StateOriented Domains, a general category of interaction. Necessary and sufficientconditions for cooperation are outlined. We use the notion of worth in analtered definition of utility, thus enabling agreements in a wider class ofjoint-goal reachable situations. An approach is offered for conflictresolution, and it is shown that even in a conflict situation, partialcooperative steps can be taken by interacting agents (that is, agents infundamental conflict might still agree to cooperate up to a certain point). AUnified Negotiation Protocol (UNP) is developed that can be used in all typesof encounters. It is shown that in certain borderline cooperative situations, apartial cooperative agreement (i.e., one that does not achieve all agents'goals) might be preferred by all agents, even though there exists a rationalagreement that would achieve all their goals. Finally, we analyze cases whereagents have incomplete information on the goals and worth of other agents.First we consider the case where agents' goals are private information, and weanalyze what goal declaration strategies the agents might adopt to increasetheir utility. Then, we consider the situation where the agents' goals (andtherefore stand-alone costs) are common knowledge, but the worth they attach totheir goals is private information. We introduce two mechanisms, one 'strict',the other 'tolerant', and analyze their affects on the stability and efficiencyof negotiation outcomes.;https://arxiv.org/abs/cs/9610101
arXiv:cs/9608104;[cs.AI];Computer Science > Artificial Intelligence;A Hierarchy of Tractable Subsets for Computing Stable Models;[Submitted on 1 Aug 1996];R. Ben-Eliyahu;;;;;"Finding the stable models of a knowledge base is a significant computationalproblem in artificial intelligence. This task is at the computational heart oftruth maintenance systems, autoepistemic logic, and default logic.Unfortunately, it is NP-hard. In this paper we present a hierarchy of classesof knowledge bases, Omega_1,Omega_2,..., with the following properties: first,Omega_1 is the class of all stratified knowledge bases; second, if a knowledgebase Pi is in Omega_k, then Pi has at most k stable models, and all of them maybe found in time O(lnk), where l is the length of the knowledge base and n thenumber of atoms in Pi; third, for an arbitrary knowledge base Pi, we can findthe minimum k such that Pi belongs to Omega_k in time polynomial in the size ofPi; and, last, where K is the class of all knowledge bases, it is the case thatunion{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to someclass in the hierarchy.";https://arxiv.org/abs/cs/9608104
arXiv:cs/9608103;[cs.AI];Computer Science > Artificial Intelligence;Spatial Aggregation: Theory and Applications;[Submitted on 1 Aug 1996];K. Yip;F. Zhao;;;;Visual thinking plays an important role in scientific reasoning. Based on theresearch in automating diverse reasoning tasks about dynamical systems,nonlinear controllers, kinematic mechanisms, and fluid motion, we haveidentified a style of visual thinking, imagistic reasoning. Imagistic reasoningorganizes computations around image-like, analogue representations so thatperceptual and symbolic operations can be brought to bear to infer structureand behavior. Programs incorporating imagistic reasoning have been shown toperform at an expert level in domains that defy current analytic or numericalmethods. We have developed a computational paradigm, spatial aggregation, tounify the description of a class of imagistic problem solvers. A programwritten in this paradigm has the following properties. It takes a continuousfield and optional objective functions as input, and produces high-leveldescriptions of structure, behavior, or control actions. It computes amulti-layer of intermediate representations, called spatial aggregates, byforming equivalence classes and adjacency relations. It employs a small set ofgeneric operators such as aggregation, classification, and localization toperform bidirectional mapping between the information-rich field andsuccessively more abstract spatial aggregates. It uses a data structure, theneighborhood graph, as a common interface to modularize computations. Toillustrate our theory, we describe the computational structure of threeimplemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of thespatial aggregation generic operators by mixing and matching a library ofcommonly used routines.;https://arxiv.org/abs/cs/9608103
arXiv:cs/9611101;[cs.AI];Computer Science > Artificial Intelligence;MUSE CSP: An Extension to the Constraint Satisfaction Problem;[Submitted on 1 Nov 1996];R. A Helzerman;M. P. Harper;;;;This paper describes an extension to the constraint satisfaction problem(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).This extension is especially useful for those problems which segment intomultiple sets of partially shared variables. Such problems arise naturally insignal processing applications including computer vision, speech processing,and handwriting recognition. For these applications, it is often difficult tosegment the data in only one way given the low-level information utilized bythe segmentation algorithms. MUSE CSP can be used to compactly representseveral similar instances of the constraint satisfaction problem. If multipleinstances of a CSP have some common variables which have the same domains andconstraints, then they can be combined into a single instance of a MUSE CSP,reducing the work required to apply the constraints. We introduce the conceptsof MUSE node consistency, MUSE arc consistency, and MUSE path consistency. Wethen demonstrate how MUSE CSP can be used to compactly represent lexicallyambiguous sentences and the multiple sentence hypotheses that are oftengenerated by speech recognition algorithms so that grammar constraints can beused to provide parses for all syntactically correct sentences. Algorithms forMUSE arc and path consistency are provided. Finally, we discuss how to create aMUSE CSP from a set of CSPs which are labeled to indicate when the samevariable is shared by more than a single CSP.;https://arxiv.org/abs/cs/9611101
arXiv:cs/9609102;[cs.AI];Computer Science > Artificial Intelligence;Cue Phrase Classification Using Machine Learning;[Submitted on 1 Sep 1996];D. J. Litman;;;;;Cue phrases may be used in a discourse sense to explicitly signal discoursestructure, but also in a sentential sense to convey semantic rather thanstructural information. Correctly classifying cue phrases as discourse orsentential is critical in natural language processing systems that exploitdiscourse structure, e.g., for performing tasks such as anaphora resolution andplan recognition. This paper explores the use of machine learning forclassifying cue phrases as discourse or sentential. Two machine learningprograms (Cgrendel and C4.5) are used to induce classification models from setsof pre-classified cue phrases and their features in text and speech. Machinelearning is shown to be an effective technique for not only automating thegeneration of classification models, but also for improving upon previousresults. When compared to manually derived classification models already in theliterature, the learned models often perform with higher accuracy and containnew linguistic insights into the data. In addition, the ability toautomatically construct classification models makes it easier to comparativelyanalyze the utility of alternative feature representations of the data.Finally, the ease of retraining makes the learning approach more scalable andflexible than manual methods.;https://arxiv.org/abs/cs/9609102
arXiv:cs/9612103;[cs.AI];Computer Science > Artificial Intelligence;Characterizations of Decomposable Dependency Models;[Submitted on 1 Dec 1996];L. M. deCampos;;;;;Decomposable dependency models possess a number of interesting and usefulproperties. This paper presents new characterizations of decomposable models interms of independence relationships, which are obtained by adding a singleaxiom to the well-known set characterizing dependency models that areisomorphic to undirected graphs. We also briefly discuss a potentialapplication of our results to the problem of learning graphical models fromdata.;https://arxiv.org/abs/cs/9612103
arXiv:cs/9612102;[cs.AI];Computer Science > Artificial Intelligence;Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer;[Submitted on 1 Dec 1996];J. C. Schlimmer;P. C. Wells;;;;Efficiently entering information into a computer is key to enjoying thebenefits of computing. This paper describes three intelligent user interfaces:handwriting recognition, adaptive menus, and predictive fillin. In the contextof adding a personUs name and address to an electronic organizer, tests showhandwriting recognition is slower than typing on an on-screen, soft keyboard,while adaptive menus and predictive fillin can be twice as fast. This paperalso presents strategies for applying these three interfaces to otherinformation collection domains.;https://arxiv.org/abs/cs/9612102
arXiv:cs/9612101;[cs.AI];Computer Science > Artificial Intelligence;Exploiting Causal Independence in Bayesian Network Inference;[Submitted on 1 Dec 1996];N. L. Zhang;D. Poole;;;;A new method is proposed for exploiting causal independencies in exactBayesian network inference. A Bayesian network can be viewed as representing afactorization of a joint probability into the multiplication of a set ofconditional probabilities. We present a notion of causal independence thatenables one to further factorize the conditional probabilities into acombination of even smaller factors and consequently obtain a finer-grainfactorization of the joint probability. The new formulation of causalindependence lets us specify the conditional probability of a variable givenits parents in terms of an associative and commutative operator, such as``or'', ``sum'' or ``max'', on the contribution of each parent. We start with asimple algorithm VE for Bayesian network inference that, given evidence and aquery variable, uses the factorization to find the posterior distribution ofthe query. We show how this algorithm can be extended to exploit causalindependence. Empirical studies, based on the CPCS networks for medicaldiagnosis, show that this method is more efficient than previous methods andallows for inference in larger networks than previous algorithms.;https://arxiv.org/abs/cs/9612101
arXiv:cs/9701101;[cs.AI];Computer Science > Artificial Intelligence;Improved Heterogeneous Distance Functions;[Submitted on 1 Jan 1997];D. R. Wilson;T. R. Martinez;;;;Instance-based learning techniques typically handle continuous and linearinput values well, but often do not handle nominal input attributesappropriately. The Value Difference Metric (VDM) was designed to findreasonable distance values between nominal attribute values, but it largelyignores continuous attributes, requiring discretization to map continuousvalues into nominal values. This paper proposes three new heterogeneousdistance functions, called the Heterogeneous Value Difference Metric (HVDM),the Interpolated Value Difference Metric (IVDM), and the Windowed ValueDifference Metric (WVDM). These new distance functions are designed to handleapplications with nominal attributes, continuous attributes, or both. Inexperiments on 48 applications the new distance metrics achieve higherclassification accuracy on average than three previous distance functions onthose datasets that have both nominal and continuous attributes.;https://arxiv.org/abs/cs/9701101
arXiv:cs/9701102;[cs.AI];Computer Science > Artificial Intelligence;SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks;[Submitted on 1 Jan 1997];S. Wermter;V. Weber;;;;Previous approaches of analyzing spontaneously spoken language often havebeen based on encoding syntactic and semantic knowledge manually andsymbolically. While there has been some progress using statistical orconnectionist language models, many current spoken- language systems still usea relatively brittle, hand-coded symbolic grammar or symbolic semanticcomponent. In contrast, we describe a so-called screening approach for learningrobust processing of spontaneously spoken language. A screening approach is aflat analysis which uses shallow sequences of category representations foranalyzing an utterance at various syntactic, semantic and dialog levels. Ratherthan using a deeply structured symbolic analysis, we use a flat connectionistanalysis. This screening approach aims at supporting speech and languageprocessing by using (1) data-driven learning and (2) robustness ofconnectionist networks. In order to test this approach, we have developed theSCREEN system which is based on this new robust, learned and flat analysis. Inthis paper, we focus on a detailed description of SCREEN's architecture, theflat syntactic and semantic analysis, the interaction with a speech recognizer,and a detailed evaluation analysis of the robustness under the influence ofnoisy or incomplete input. The main result of this paper is that flatrepresentations allow more robust processing of spontaneous spoken languagethan deeply structured representations. In particular, we show how thefault-tolerance and learning capability of connectionist networks can support aflat analysis for providing more robust spoken-language processing within anoverall hybrid symbolic/connectionist framework.;https://arxiv.org/abs/cs/9701102
