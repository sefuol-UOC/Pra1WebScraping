{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.851 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2020-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# Práctica 1: Web scraping\n",
    "\n",
    " - **https://github.com/sfunesolaria/Pra1WebScraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcat_url_generator(field, category):\n",
    "    \n",
    "    years = [str(yr)[-2:] for yr in range(1993,2022)]\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
    "              '10', '11', '12']\n",
    "    \n",
    "    links = []\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            link = 'https://arxiv.org/list?archive='\n",
    "            link += field + '.' + category\n",
    "            link += '&year=' + year\n",
    "            link += '&month=' + month\n",
    "            link += '&submit=Go?skip=0&show=2000'\n",
    "            links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "cs_ai_urls = subcat_url_generator('cs', 'ai')\n",
    "cs_ai_urls[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider should get as `start_urls` an url list, as shown above, to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class link_spider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/list?archive=cs.ai&year=20&month=03&submit=Go?skip=0&show=100']\n",
    "    found_links = []\n",
    "    \n",
    "    base_url = 'https://arxiv.org'\n",
    "\n",
    "    def parse(self, response):\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = link_spider.base_url + rel_url\n",
    "            links['url'] = url\n",
    "            \n",
    "            self.found_links.append(links)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(link_spider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_links: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class PythonSpider(scrapy.Spider):\n",
    "    name = 'pythonspider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/abs/2003.00030',]\n",
    "    found_events = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "            \n",
    "            event_details['subject'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['abstract'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(PythonSpider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# create csv\n",
    "\n",
    "dataset = data_spider.found_events[0]\n",
    "csv_file = \"dataset.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as f:\n",
    "        w = csv.DictWriter(f, dataset.keys())\n",
    "        w.writeheader()\n",
    "        w.writerow(dataset)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://arxiv.org/abs/2003.00030'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00126'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00172'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00201'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00234'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00260'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00274'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00330'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00344'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00411'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00431'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00439'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00443'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00475'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00613'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00635'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00683'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00703'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00719'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00749'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00767'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00806'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00814'}\n",
      "{'url': 'https://arxiv.org/abs/2003.00819'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01008'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01025'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01207'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01226'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01274'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01338'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01668'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01670'}\n",
      "{'url': 'https://arxiv.org/abs/2003.01848'}\n",
      "{'url': 'https://arxiv.org/abs/2003.02320'}\n",
      "{'url': 'https://arxiv.org/abs/2003.02372'}\n",
      "{'url': 'https://arxiv.org/abs/2003.02599'}\n",
      "{'url': 'https://arxiv.org/abs/2003.02979'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03136'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03181'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03220'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03268'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03377'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03410'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03433'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03546'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03623'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03875'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03917'}\n",
      "{'url': 'https://arxiv.org/abs/2003.03932'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04080'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04369'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04371'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04411'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04445'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04518'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04567'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04641'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04707'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04736'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04770'}\n",
      "{'url': 'https://arxiv.org/abs/2003.04792'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05104'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05119'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05196'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05320'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05370'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05562'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05856'}\n",
      "{'url': 'https://arxiv.org/abs/2003.05861'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06071'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06212'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06265'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06347'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06404'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06423'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06492'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06551'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06649'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06695'}\n",
      "{'url': 'https://arxiv.org/abs/2003.06920'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07037'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07060'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07108'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07124'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07182'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07520'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07523'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07596'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07745'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07781'}\n",
      "{'url': 'https://arxiv.org/abs/2003.07813'}\n",
      "{'url': 'https://arxiv.org/abs/2003.08001'}\n",
      "{'url': 'https://arxiv.org/abs/2003.08316'}\n",
      "{'url': 'https://arxiv.org/abs/2003.08445'}\n",
      "{'url': 'https://arxiv.org/abs/2003.08554'}\n",
      "{'url': 'https://arxiv.org/abs/2003.08598'}\n",
      "{'url': 'https://arxiv.org/abs/2003.08727'}\n",
      "{'url': 'https://arxiv.org/abs/2003.09000'}\n",
      "{'url': 'https://arxiv.org/abs/2003.09140'}\n",
      "{'url': 'https://arxiv.org/abs/2003.09301'}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Policy-Aware Model Learning for Policy Gradient Methods', 'author_1': 'Romina Abachi', 'author_2': 'Mohammad Ghavamzadeh', 'author_3': 'Amir-massoud Farahmand', 'author_4': None, 'author_5': None, 'abstract': '  This paper considers the problem of learning a model in model-based\\nreinforcement learning (MBRL). We examine how the planning module of an MBRL\\nalgorithm uses the model, and propose that the model learning module should\\nincorporate the way the planner is going to use the model. This is in contrast\\nto conventional model learning approaches, such as those based on maximum\\nlikelihood estimate, that learn a predictive model of the environment without\\nexplicitly considering the interaction of the model and the planner. We focus\\non policy gradient type of planning algorithms and derive new loss functions\\nfor model learning that incorporate how the planner uses the model. We call\\nthis approach Policy-Aware Model Learning (PAML). We theoretically analyze a\\ngeneric model-based policy gradient algorithm and provide a convergence\\nguarantee for the optimized policy. We also empirically evaluate PAML on some\\nbenchmark problems, showing promising results.\\n\\n    '}\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# 2 spiders\n",
    "\n",
    "class link_spider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/list?archive=cs.ai&year=20&month=03&submit=Go?skip=0&show=100']\n",
    "    found_links = []\n",
    "    \n",
    "    base_url = 'https://arxiv.org'\n",
    "\n",
    "    def parse(self, response):\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = link_spider.base_url + rel_url\n",
    "            links['url'] = url\n",
    "            \n",
    "            self.found_links.append(links)\n",
    "\n",
    "class data_spider(scrapy.Spider):\n",
    "    name = 'pythonspider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/abs/2003.00030',]\n",
    "    found_events = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "            \n",
    "            event_details['subject'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['abstract'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(link_spider)\n",
    "    process.crawl(data_spider)\n",
    "    process.start()\n",
    "\n",
    "    for event in link_spider.found_links: print(event)\n",
    "    for event in data_spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
