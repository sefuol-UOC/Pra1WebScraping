{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.851 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2020-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# Práctica 1: Web scraping\n",
    "\n",
    " - **https://github.com/sfunesolaria/Pra1WebScraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcat_url_generator(field, category):\n",
    "    \n",
    "    years = [str(yr)[-2:] for yr in range(1993,2022)]\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
    "              '10', '11', '12']\n",
    "    \n",
    "    links = []\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            link = 'https://arxiv.org/list?archive='\n",
    "            link += field + '.' + category\n",
    "            link += '&year=' + year\n",
    "            link += '&month=' + month\n",
    "            link += '&submit=Go?skip=0&show=2000'\n",
    "            links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "cs_ai_urls = subcat_url_generator('cs', 'ai')\n",
    "cs_ai_urls[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider should get as `start_urls` an url list, as shown above, to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class link_spider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/list?archive=cs.ai&year=20&month=03&submit=Go?skip=0&show=100']\n",
    "    found_links = []\n",
    "    \n",
    "    base_url = 'https://arxiv.org'\n",
    "\n",
    "    def parse(self, response):\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = link_spider.base_url + rel_url\n",
    "            links['url'] = url\n",
    "            \n",
    "            self.found_links.append(links)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(link_spider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_links: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class PythonSpider(scrapy.Spider):\n",
    "    name = 'pythonspider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/abs/2003.00030',]\n",
    "    found_events = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "            \n",
    "            event_details['subject'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['abstract'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(PythonSpider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems', 'author_1': 'M. P. Wellman', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Market price systems constitute a well-understood class of mechanisms that\\nunder certain conditions provide effective decentralization of decision making\\nwith minimal communication overhead. In a market-oriented programming approach\\nto distributed problem solving, we derive the activities and resource\\nallocations for a set of computational agents by computing the competitive\\nequilibrium of an artificial economy. WALRAS provides basic constructs for\\ndefining computational market structures, and protocols for deriving their\\ncorresponding price equilibria. In a particular realization of this approach\\nfor a form of multicommodity flow problem, we see that careful construction of\\nthe decision process according to economic principles can lead to efficient\\ndistributed resource allocation, and that the behavior of the system can be\\nmeaningfully analyzed in economic terms.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Dynamic Backtracking', 'author_1': 'M. L. Ginsberg', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Because of their occasional need to return to shallow points in a search\\ntree, existing backtracking methods can sometimes erase meaningful progress\\ntoward solving a search problem. In this paper, we present a method by which\\nbacktrack points can be moved deeper in the search space, thereby avoiding this\\ndifficulty. The technique developed is a variant of dependency-directed\\nbacktracking that uses only polynomial space while still providing useful\\ncontrol information and retaining the completeness guarantees provided by\\nearlier approaches.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Teleo-Reactive Programs for Agent Control', 'author_1': 'N. Nilsson', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  A formalism is presented for computing and organizing actions for autonomous\\nagents in dynamic environments. We introduce the notion of teleo-reactive (T-R)\\nprograms whose execution entails the construction of circuitry for the\\ncontinuous computation of the parameters and conditions on which agent action\\nis based. In addition to continuous feedback, T-R programs support parameter\\nbinding and recursion. A primary difference between T-R programs and many other\\ncircuit-based systems is that the circuitry of T-R programs is more compact; it\\nis constructed at run time and thus does not have to anticipate all the\\ncontingencies that might arise over all possible runs. In addition, T-R\\nprograms are intuitive and easy to write and are written in a form that is\\ncompatible with automatic planning and learning methods. We briefly describe\\nsome experimental applications of T-R programs in the control of simulated and\\nactual mobile robots.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Decidable Reasoning in Terminological Knowledge Representation Systems', 'author_1': 'M. Buchheit', 'author_2': 'F. M. Donini', 'author_3': 'A. Schaerf', 'author_4': None, 'author_5': None, 'abstract': '  Terminological knowledge representation systems (TKRSs) are tools for\\ndesigning and using knowledge bases that make use of terminological languages\\n(or concept languages). We analyze from a theoretical point of view a TKRS\\nwhose capabilities go beyond the ones of presently available TKRSs. The new\\nfeatures studied, often required in practical applications, can be summarized\\nin three main points. First, we consider a highly expressive terminological\\nlanguage, called ALCNR, including general complements of concepts, number\\nrestrictions and role conjunction. Second, we allow to express inclusion\\nstatements between general concepts, and terminological cycles as a particular\\ncase. Third, we prove the decidability of a number of desirable TKRS-deduction\\nservices (like satisfiability, subsumption and instance checking) through a\\nsound, complete and terminating calculus for reasoning in ALCNR-knowledge\\nbases. Our calculus extends the general technique of constraint systems. As a\\nbyproduct of the proof, we get also the result that inclusion statements in\\nALCNR can be simulated by terminological cycles, if descriptive semantics is\\nadopted.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': \"Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction\", 'author_1': 'P. M. Murphy', 'author_2': 'M. J. Pazzani', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  We report on a series of experiments in which all decision trees consistent\\nwith the training data are constructed. These experiments were run to gain an\\nunderstanding of the properties of the set of consistent decision trees and the\\nfactors that affect the accuracy of individual trees. In particular, we\\ninvestigated the relationship between the size of a decision tree consistent\\nwith some training data and the accuracy of the tree on test data. The\\nexperiments were performed on a massively parallel Maspar computer. The results\\nof the experiments on several artificial and two real world problems indicate\\nthat, for many of the problems investigated, smaller consistent decision trees\\nare on average less accurate than the average accuracy of slightly larger\\ntrees.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'An Empirical Analysis of Search in GSAT', 'author_1': 'I. P. Gent', 'author_2': 'T. Walsh', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  We describe an extensive study of search in GSAT, an approximation procedure\\nfor propositional satisfiability. GSAT performs greedy hill-climbing on the\\nnumber of satisfied clauses in a truth assignment. Our experiments provide a\\nmore complete picture of GSAT's search than previous accounts. We describe in\\ndetail the two phases of search: rapid hill-climbing followed by a long plateau\\nsearch. We demonstrate that when applied to randomly generated 3SAT problems,\\nthere is a very simple scaling with problem size for both the mean number of\\nsatisfied clauses and the mean branching rate. Our results allow us to make\\ndetailed numerical conjectures about the length of the hill-climbing phase, the\\naverage gradient of this phase, and to conjecture that both the average score\\nand average branching rate decay exponentially during plateau search. We end by\\nshowing how these results can be used to direct future theoretical analysis.\\nThis work provides a case study of how computer experiments can be used to\\nimprove understanding of the theoretical properties of algorithms.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Bias-Driven Revision of Logical Domain Theories', 'author_1': 'M. Koppel', 'author_2': 'R. Feldman', 'author_3': 'A. M. Segre', 'author_4': None, 'author_5': None, 'abstract': \"  The theory revision problem is the problem of how best to go about revising a\\ndeficient domain theory using information contained in examples that expose\\ninaccuracies. In this paper we present our approach to the theory revision\\nproblem for propositional domain theories. The approach described here, called\\nPTR, uses probabilities associated with domain theory elements to numerically\\ntrack the ``flow'' of proof through the theory. This allows us to measure the\\nprecise role of a clause or literal in allowing or preventing a (desired or\\nundesired) derivation for a given example. This information is used to\\nefficiently locate and repair flawed elements of the theory. PTR is proved to\\nconverge to a theory which correctly classifies all examples, and shown\\nexperimentally to be fast and accurate even for deep theories.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Substructure Discovery Using Minimum Description Length and Background Knowledge', 'author_1': 'D. J. Cook', 'author_2': 'L. B. Holder', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  The ability to identify interesting and repetitive substructures is an\\nessential component to discovering knowledge in structural data. We describe a\\nnew version of our SUBDUE substructure discovery system based on the minimum\\ndescription length principle. The SUBDUE system discovers substructures that\\ncompress the original data and represent structural concepts in the data. By\\nreplacing previously-discovered substructures in the data, multiple passes of\\nSUBDUE produce a hierarchical description of the structural regularities in the\\ndata. SUBDUE uses a computationally-bounded inexact graph match that identifies\\nsimilar, but not identical, instances of a substructure and finds an\\napproximate measure of closeness of two substructures when under computational\\nconstraints. In addition to the minimum description length principle, other\\nbackground knowledge can be used by SUBDUE to guide the search towards more\\nappropriate substructures. Experiments in a variety of domains demonstrate\\nSUBDUE's ability to find substructures capable of compressing the original data\\nand to discover structural concepts important to the domain. Description of\\nOnline Appendix: This is a compressed tar file containing the SUBDUE discovery\\nsystem, written in C. The program accepts as input databases represented in\\ngraph form, and will output discovered substructures with their corresponding\\nvalue.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models', 'author_1': 'C. X. Ling', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Learning the past tense of English verbs - a seemingly minor aspect of\\nlanguage acquisition - has generated heated debates since 1986, and has become\\na landmark task for testing the adequacy of cognitive modeling. Several\\nartificial neural networks (ANNs) have been implemented, and a challenge for\\nbetter symbolic models has been posed. In this paper, we present a\\ngeneral-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree\\nlearning algorithm ID3. We conduct extensive head-to-head comparisons on the\\ngeneralization ability between ANN models and the SPA under different\\nrepresentations. We conclude that the SPA generalizes the past tense of unseen\\nverbs better than ANN models by a wide margin, and we offer insights as to why\\nthis should be the case. We also discuss a new default strategy for\\ndecision-tree learning algorithms.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A System for Induction of Oblique Decision Trees', 'author_1': 'S. K. Murthy', 'author_2': 'S. Kasif', 'author_3': 'S. Salzberg', 'author_4': None, 'author_5': None, 'abstract': \"  This article describes a new system for induction of oblique decision trees.\\nThis system, OC1, combines deterministic hill-climbing with two forms of\\nrandomization to find a good oblique split (in the form of a hyperplane) at\\neach node of a decision tree. Oblique decision tree methods are tuned\\nespecially for domains in which the attributes are numeric, although they can\\nbe adapted to symbolic or mixed symbolic/numeric attributes. We present\\nextensive empirical studies, using both real and artificial data, that analyze\\nOC1's ability to construct oblique trees that are smaller and more accurate\\nthan their axis-parallel counterparts. We also examine the benefits of\\nrandomization for the construction of oblique decision trees.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Pattern Matching and Discourse Processing in Information Extraction from Japanese Text', 'author_1': 'T. Kitani', 'author_2': 'Y. Eriguchi', 'author_3': 'M. Hara', 'author_4': None, 'author_5': None, 'abstract': '  Information extraction is the task of automatically picking up information of\\ninterest from an unconstrained text. Information of interest is usually\\nextracted in two steps. First, sentence level processing locates relevant\\npieces of information scattered throughout the text; second, discourse\\nprocessing merges coreferential information to generate the output. In the\\nfirst step, pieces of information are locally identified without recognizing\\nany relationships among them. A key word search or simple pattern search can\\nachieve this purpose. The second step requires deeper knowledge in order to\\nunderstand relationships among separately identified pieces of information.\\nPrevious information extraction systems focused on the first step, partly\\nbecause they were not required to link up each piece of information with other\\npieces. To link the extracted pieces of information and map them onto a\\nstructured output format, complex discourse processing is essential. This paper\\nreports on a Japanese information extraction system that merges information\\nusing a pattern matcher and discourse processor. Evaluation results show a high\\nlevel of system performance which approaches human performance.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Total-Order and Partial-Order Planning: A Comparative Analysis', 'author_1': 'S. Minton', 'author_2': 'J. Bresina', 'author_3': 'M. Drummond', 'author_4': None, 'author_5': None, 'abstract': '  For many years, the intuitions underlying partial-order planning were largely\\ntaken for granted. Only in the past few years has there been renewed interest\\nin the fundamental principles underlying this paradigm. In this paper, we\\npresent a rigorous comparative analysis of partial-order and total-order\\nplanning by focusing on two specific planners that can be directly compared. We\\nshow that there are some subtle assumptions that underly the wide-spread\\nintuitions regarding the supposed efficiency of partial-order planning. For\\ninstance, the superiority of partial-order planning can depend critically upon\\nthe search strategy and the structure of the search space. Understanding the\\nunderlying assumptions is crucial for constructing efficient planners.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Operations for Learning with Graphical Models', 'author_1': 'W. L. Buntine', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper is a multidisciplinary review of empirical, statistical learning\\nfrom a graphical model perspective. Well-known examples of graphical models\\ninclude Bayesian networks, directed graphs representing a Markov chain, and\\nundirected networks representing a Markov field. These graphical models are\\nextended to model data analysis and empirical learning using the notation of\\nplates. Graphical operations for simplifying and manipulating a problem are\\nprovided including decomposition, differentiation, and the manipulation of\\nprobability models from the exponential family. Two standard algorithm schemas\\nfor learning are reviewed in a graphical framework: Gibbs sampling and the\\nexpectation maximization algorithm. Using these operations and schemas, some\\npopular algorithms can be synthesized from their graphical specification. This\\nincludes versions of linear regression, techniques for feed-forward networks,\\nand learning Gaussian and discrete Bayesian networks from data. The paper\\nconcludes by sketching some implications for data analysis and summarizing how\\nsome popular algorithms fall within the framework presented. The main original\\ncontributions here are the decomposition techniques and the demonstration that\\ngraphical models provide a framework for understanding and developing complex\\nlearning algorithms.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Applying GSAT to Non-Clausal Formulas', 'author_1': 'R. Sebastiani', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  In this paper we describe how to modify GSAT so that it can be applied to\\nnon-clausal formulas. The idea is to use a particular ``score'' function which\\ngives the number of clauses of the CNF conversion of a formula which are false\\nunder a given truth assignment. Its value is computed in linear time, without\\nconstructing the CNF conversion itself. The proposed methodology applies to\\nmost of the variants of GSAT proposed so far.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Wrap-Up: a Trainable Discourse Module for Information Extraction', 'author_1': 'S. Soderland', 'author_2': 'Lehnert. W', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  The vast amounts of on-line text now available have led to renewed interest\\nin information extraction (IE) systems that analyze unrestricted text,\\nproducing a structured representation of selected information from the text.\\nThis paper presents a novel approach that uses machine learning to acquire\\nknowledge for some of the higher level IE processing. Wrap-Up is a trainable IE\\ndiscourse component that makes intersentential inferences and identifies\\nlogical relations among information extracted from the text. Previous\\ncorpus-based approaches were limited to lower level processing such as\\npart-of-speech tagging, lexical disambiguation, and dictionary construction.\\nWrap-Up is fully trainable, and not only automatically decides what classifiers\\nare needed, but even derives the feature set for each classifier automatically.\\nPerformance equals that of a partially trainable discourse module requiring\\nmanual customization for each domain.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning', 'author_1': 'P. Cichosz', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Temporal difference (TD) methods constitute a class of methods for learning\\npredictions in multi-step prediction problems, parameterized by a recency\\nfactor lambda. Currently the most important application of these methods is to\\ntemporal credit assignment in reinforcement learning. Well known reinforcement\\nlearning algorithms, such as AHC or Q-learning, may be viewed as instances of\\nTD learning. This paper examines the issues of the efficient and general\\nimplementation of TD(lambda) for arbitrary lambda, for use with reinforcement\\nlearning algorithms optimizing the discounted sum of rewards. The traditional\\napproach, based on eligibility traces, is argued to suffer from both\\ninefficiency and lack of generality. The TTD (Truncated Temporal Differences)\\nprocedure is proposed as an alternative, that indeed only approximates\\nTD(lambda), but requires very little computation per action and can be used\\nwith arbitrary function representation methods. The idea from which it is\\nderived is fairly simple and not new, but probably unexplored so far.\\nEncouraging experimental results are presented, suggesting that using lambda\\n&gt 0 with the TTD procedure allows one to obtain a significant learning\\nspeedup at essentially the same cost as usual TD(0) learning.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Domain-Independent Algorithm for Plan Adaptation', 'author_1': 'S. Hanks', 'author_2': 'D. S. Weld', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  The paradigms of transformational planning, case-based planning, and plan\\ndebugging all involve a process known as plan adaptation - modifying or\\nrepairing an old plan so it solves a new problem. In this paper we provide a\\ndomain-independent algorithm for plan adaptation, demonstrate that it is sound,\\ncomplete, and systematic, and compare it to other adaptation algorithms in the\\nliterature. Our approach is based on a view of planning as searching a graph of\\npartial plans. Generative planning starts at the graph's root and moves from\\nnode to node using plan-refinement operators. In planning by adaptation, a\\nlibrary plan - an arbitrary node in the plan graph - is the starting point for\\nthe search, and the plan-adaptation algorithm can apply both the same\\nrefinement operators available to a generative planner and can also retract\\nconstraints and steps from the plan. Our algorithm's completeness ensures that\\nthe adaptation algorithm will eventually search the entire graph and its\\nsystematicity ensures that it will do so without redundantly searching any\\nparts of the graph.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic', 'author_1': 'A. Borgida', 'author_2': 'P. F. Patel-Schneider', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper analyzes the correctness of the subsumption algorithm used in\\nCLASSIC, a description logic-based knowledge representation system that is\\nbeing used in practical applications. In order to deal efficiently with\\nindividuals in CLASSIC descriptions, the developers have had to use an\\nalgorithm that is incomplete with respect to the standard, model-theoretic\\nsemantics for description logics. We provide a variant semantics for\\ndescriptions with respect to which the current implementation is complete, and\\nwhich can be independently motivated. The soundness and completeness of the\\npolynomial-time subsumption algorithm is established using description graphs,\\nwhich are an abstracted version of the implementation structures used in\\nCLASSIC, and are of independent interest.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach', 'author_1': 'S. K. Donoho', 'author_2': 'L. A. Rendell', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Theory revision integrates inductive learning and background knowledge by\\ncombining training examples with a coarse domain theory to produce a more\\naccurate theory. There are two challenges that theory revision and other\\ntheory-guided systems face. First, a representation language appropriate for\\nthe initial theory may be inappropriate for an improved theory. While the\\noriginal representation may concisely express the initial theory, a more\\naccurate theory forced to use that same representation may be bulky,\\ncumbersome, and difficult to reach. Second, a theory structure suitable for a\\ncoarse domain theory may be insufficient for a fine-tuned theory. Systems that\\nproduce only small, local changes to a theory have limited value for\\naccomplishing complex structural alterations that may be required.\\nConsequently, advanced theory-guided learning systems require flexible\\nrepresentation and flexible structure. An analysis of various theory revision\\nsystems and theory-guided learning systems reveals specific strengths and\\nweaknesses in terms of these two desired properties. Designed to capture the\\nunderlying qualities of each system, a new system uses theory-guided\\nconstructive induction. Experiments in three domains show improvement over\\nprevious theory-guided systems. This leads to a study of the behavior,\\nlimitations, and potential of theory-guided constructive induction.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Pac-learning Recursive Logic Programs: Negative Results', 'author_1': 'W. W. Cohen', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  In a companion paper it was shown that the class of constant-depth\\ndeterminate k-ary recursive clauses is efficiently learnable. In this paper we\\npresent negative results showing that any natural generalization of this class\\nis hard to learn in Valiant's model of pac-learnability. In particular, we show\\nthat the following program classes are cryptographically hard to learn:\\nprograms with an unbounded number of constant-depth linear recursive clauses;\\nprograms with one constant-depth determinate clause containing an unbounded\\nnumber of recursive calls; and programs with one linear recursive clause of\\nconstant locality. These results immediately imply the non-learnability of any\\nmore general class of programs. We also show that learning a constant-depth\\ndeterminate program with either two linear recursive clauses or one linear\\nrecursive clause and one non-recursive clause is as hard as learning boolean\\nDNF. Together with positive results from the companion paper, these negative\\nresults establish a boundary of efficient learnability for recursive\\nfunction-free clauses.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Pac-Learning Recursive Logic Programs: Efficient Algorithms', 'author_1': 'W. W. Cohen', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  We present algorithms that learn certain classes of function-free recursive\\nlogic programs in polynomial time from equivalence queries. In particular, we\\nshow that a single k-ary recursive constant-depth determinate clause is\\nlearnable. Two-clause programs consisting of one learnable recursive clause and\\none constant-depth determinate non-recursive clause are also learnable, if an\\nadditional ``basecase'' oracle is assumed. These results immediately imply the\\npac-learnability of these classes. Although these classes of learnable\\nrecursive programs are very constrained, it is shown in a companion paper that\\nthey are maximally general, in that generalizing either class in any natural\\nway leads to a computationally difficult learning problem. Thus, taken together\\nwith its companion paper, this paper establishes a boundary of efficient\\nlearnability for recursive logic programs.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Adaptive Load Balancing: A Study in Multi-Agent Learning', 'author_1': 'A. Schaerf', 'author_2': 'Y. Shoham', 'author_3': 'M. Tennenholtz', 'author_4': None, 'author_5': None, 'abstract': '  We study the process of multi-agent reinforcement learning in the context of\\nload balancing in a distributed system, without use of either central\\ncoordination or explicit communication. We first define a precise framework in\\nwhich to study adaptive load balancing, important features of which are its\\nstochastic nature and the purely local information available to individual\\nagents. Given this framework, we show illuminating results on the interplay\\nbetween basic adaptive behavior parameters and their effect on system\\nefficiency. We then investigate the properties of adaptive load balancing in\\nheterogeneous populations, and address the issue of exploration vs.\\nexploitation in that context. Finally, we show that naive use of communication\\nmay not improve, and might even harm system efficiency.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm', 'author_1': 'P. D. Turney', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  This paper introduces ICET, a new algorithm for cost-sensitive\\nclassification. ICET uses a genetic algorithm to evolve a population of biases\\nfor a decision tree induction algorithm. The fitness function of the genetic\\nalgorithm is the average cost of classification when using the decision tree,\\nincluding both the costs of tests (features, measurements) and the costs of\\nclassification errors. ICET is compared here with three other algorithms for\\ncost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5,\\nwhich classifies without regard to cost. The five algorithms are evaluated\\nempirically on five real-world medical datasets. Three sets of experiments are\\nperformed. The first set examines the baseline performance of the five\\nalgorithms on the five datasets and establishes that ICET performs\\nsignificantly better than its competitors. The second set tests the robustness\\nof ICET under a variety of conditions and shows that ICET maintains its\\nadvantage. The third set looks at ICET's search in bias space and discovers a\\nway to improve the search.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Using Pivot Consistency to Decompose and Solve Functional CSPs', 'author_1': 'P. David', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Many studies have been carried out in order to increase the search efficiency\\nof constraint satisfaction problems; among them, some make use of structural\\nproperties of the constraint network; others take into account semantic\\nproperties of the constraints, generally assuming that all the constraints\\npossess the given property. In this paper, we propose a new decomposition\\nmethod benefiting from both semantic properties of functional constraints (not\\nbijective constraints) and structural properties of the network; furthermore,\\nnot all the constraints need to be functional. We show that under some\\nconditions, the existence of solutions can be guaranteed. We first characterize\\na particular subset of the variables, which we name a root set. We then\\nintroduce pivot consistency, a new local consistency which is a weak form of\\npath consistency and can be achieved in O(n^2d^2) complexity (instead of\\nO(n^3d^3) for path consistency), and we present associated properties; in\\nparticular, we show that any consistent instantiation of the root set can be\\nlinearly extended to a solution, which leads to the presentation of the\\naforementioned new method for solving by decomposing functional CSPs.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Provably Bounded-Optimal Agents', 'author_1': 'S. J. Russell', 'author_2': 'D. Subramanian', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Since its inception, artificial intelligence has relied upon a theoretical\\nfoundation centered around perfect rationality as the desired property of\\nintelligent systems. We argue, as others have done, that this foundation is\\ninadequate because it imposes fundamentally unsatisfiable requirements. As a\\nresult, there has arisen a wide gap between theory and practice in AI,\\nhindering progress in the field. We propose instead a property called bounded\\noptimality. Roughly speaking, an agent is bounded-optimal if its program is a\\nsolution to the constrained optimization problem presented by its architecture\\nand the task environment. We show how to construct agents with this property\\nfor a simple class of machine architectures in a broad class of real-time\\nenvironments. We illustrate these results using a simple model of an automated\\nmail sorting facility. We also define a weaker property, asymptotic bounded\\noptimality (ABO), that generalizes the notion of optimality in classical\\ncomplexity theory. We then construct universal ABO programs, i.e., programs\\nthat are ABO no matter what real-time constraints are applied. Universal ABO\\nprograms can be used as building blocks for more complex systems. We conclude\\nwith a discussion of the prospects for bounded optimality as a theoretical\\nbasis for AI, and relate it to similar trends in philosophy, economics, and\\ngame theory.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'On the Informativeness of the DNA Promoter Sequences Domain Theory', 'author_1': 'J. Ortega', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  The DNA promoter sequences domain theory and database have become popular for\\ntesting systems that integrate empirical and analytical learning. This note\\nreports a simple change and reinterpretation of the domain theory in terms of\\nM-of-N concepts, involving no learning, that results in an accuracy of 93.4% on\\nthe 106 items of the database. Moreover, an exhaustive search of the space of\\nM-of-N domain theory interpretations indicates that the expected accuracy of a\\nrandomly chosen interpretation is 76.5%, and that a maximum accuracy of 97.2%\\nis achieved in 12 cases. This demonstrates the informativeness of the domain\\ntheory, without the complications of understanding the interactions between\\nvarious learning algorithms and the theory. In addition, our results help\\ncharacterize the difficulty of learning using the DNA promoters theory.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Solving Multiclass Learning Problems via Error-Correcting Output Codes', 'author_1': 'T. G. Dietterich', 'author_2': 'G. Bakiri', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Multiclass learning problems involve finding a definition for an unknown\\nfunction f(x) whose range is a discrete set containing k &gt 2 values (i.e., k\\n``classes''). The definition is acquired by studying collections of training\\nexamples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning\\nproblems include direct application of multiclass algorithms such as the\\ndecision-tree algorithms C4.5 and CART, application of binary concept learning\\nalgorithms to learn individual binary functions for each of the k classes, and\\napplication of binary concept learning algorithms with distributed output\\nrepresentations. This paper compares these three approaches to a new technique\\nin which error-correcting codes are employed as a distributed output\\nrepresentation. We show that these output representations improve the\\ngeneralization performance of both C4.5 and backpropagation on a wide range of\\nmulticlass learning tasks. We also demonstrate that this approach is robust\\nwith respect to changes in the size of the training sample, the assignment of\\ndistributed representations to particular classes, and the application of\\noverfitting avoidance techniques such as decision-tree pruning. Finally, we\\nshow that---like the other methods---the error-correcting code technique can\\nprovide reliable class probability estimates. Taken together, these results\\ndemonstrate that error-correcting output codes provide a general-purpose method\\nfor improving the performance of inductive learning programs on multiclass\\nproblems.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Random Worlds and Maximum Entropy', 'author_1': 'A. J. Grove', 'author_2': 'J. Y. Halpern', 'author_3': 'D. Koller', 'author_4': None, 'author_5': None, 'abstract': '  Given a knowledge base KB containing first-order and statistical facts, we\\nconsider a principled method, called the random-worlds method, for computing a\\ndegree of belief that some formula Phi holds given KB. If we are reasoning\\nabout a world or system consisting of N individuals, then we can consider all\\npossible worlds, or first-order models, with domain {1,...,N} that satisfy KB,\\nand compute the fraction of them in which Phi is true. We define the degree of\\nbelief to be the asymptotic value of this fraction as N grows large. We show\\nthat when the vocabulary underlying Phi and KB uses constants and unary\\npredicates only, we can naturally associate an entropy with each world. As N\\ngrows larger, there are many more worlds with higher entropy. Therefore, we can\\nuse a maximum-entropy computation to compute the degree of belief. This result\\nis in a similar spirit to previous work in physics and artificial intelligence,\\nbut is far more general. Of equal interest to the result itself are the\\nlimitations on its scope. Most importantly, the restriction to unary predicates\\nseems necessary. Although the random-worlds method makes sense in general, the\\nconnection to maximum entropy seems to disappear in the non-unary case. These\\nobservations suggest unexpected limitations to the applicability of\\nmaximum-entropy methods.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'On Planning while Learning', 'author_1': 'S. Safra', 'author_2': 'M. Tennenholtz', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper introduces a framework for Planning while Learning where an agent\\nis given a goal to achieve in an environment whose behavior is only partially\\nknown to the agent. We discuss the tractability of various plan-design\\nprocesses. We show that for a large natural class of Planning while Learning\\nsystems, a plan can be presented and verified in a reasonable time. However,\\ncoming up algorithmically with a plan, even for simple classes of systems is\\napparently intractable. We emphasize the role of off-line plan-design\\nprocesses, and show that, in most natural cases, the verification (projection)\\npart can be carried out in an efficient algorithmic manner.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Software Agents: Completing Patterns and Constructing User Interfaces', 'author_1': 'J. C. Schlimmer', 'author_2': 'L. A. Hermens', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  To support the goal of allowing users to record and retrieve information,\\nthis paper describes an interactive note-taking system for pen-based computers\\nwith two distinctive features. First, it actively predicts what the user is\\ngoing to write. Second, it automatically constructs a custom, button-box user\\ninterface on request. The system is an example of a learning-apprentice\\nsoftware- agent. A machine learning component characterizes the syntax and\\nsemantics of the user's information. A performance system uses this learned\\ninformation to generate completion strings and construct a user interface.\\nDescription of Online Appendix: People like to record information. Doing this\\non paper is initially efficient, but lacks flexibility. Recording information\\non a computer is less efficient but more powerful. In our new note taking\\nsoftwre, the user records information directly on a computer. Behind the\\ninterface, an agent acts for the user. To help, it provides defaults and\\nconstructs a custom user interface. The demonstration is a QuickTime movie of\\nthe note taking agent in action. The file is a binhexed self-extracting\\narchive. Macintosh utilities for binhex are available from\\n\"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'The Difficulties of Learning Logic Programs with Cut', 'author_1': 'F. Bergadano', 'author_2': 'D. Gunetti', 'author_3': 'U. Trinchero', 'author_4': None, 'author_5': None, 'abstract': '  As real logic programmers normally use cut (!), an effective learning\\nprocedure for logic programs should be able to deal with it. Because the cut\\npredicate has only a procedural meaning, clauses containing cut cannot be\\nlearned using an extensional evaluation method, as is done in most learning\\nsystems. On the other hand, searching a space of possible programs (instead of\\na space of independent clauses) is unfeasible. An alternative solution is to\\ngenerate first a candidate base program which covers the positive examples, and\\nthen make it consistent by inserting cut where appropriate. The problem of\\nlearning programs with cut has not been investigated before and this seems to\\nbe a natural and reasonable approach. We generalize this scheme and investigate\\nthe difficulties that arise. Some of the major shortcomings are actually\\ncaused, in general, by the need for intensional evaluation. As a conclusion,\\nthe analysis of this paper suggests, on precise and technical grounds, that\\nlearning cut is difficult, and current induction techniques should probably be\\nrestricted to purely declarative logic languages.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'An Integrated Framework for Learning and Reasoning', 'author_1': 'C. G. Giraud-Carrier', 'author_2': 'T. R. Martinez', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Learning and reasoning are both aspects of what is considered to be\\nintelligence. Their studies within AI have been separated historically,\\nlearning being the topic of machine learning and neural networks, and reasoning\\nfalling under classical (or symbolic) AI. However, learning and reasoning are\\nin many ways interdependent. This paper discusses the nature of some of these\\ninterdependencies and proposes a general framework called FLARE, that combines\\ninductive learning using prior knowledge together with reasoning in a\\npropositional setting. Several examples that test the framework are presented,\\nincluding classical induction, many important reasoning protocols and two\\nsimple expert systems.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Building and Refining Abstract Planning Cases by Change of Representation Language', 'author_1': 'R. Bergmann', 'author_2': 'W. Wilke', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  ion is one of the most promising approaches to improve the performance of\\nproblem solvers. In several domains abstraction by dropping sentences of a\\ndomain description -- as used in most hierarchical planners -- has proven\\nuseful. In this paper we present examples which illustrate significant\\ndrawbacks of abstraction by dropping sentences. To overcome these drawbacks, we\\npropose a more general view of abstraction involving the change of\\nrepresentation language. We have developed a new abstraction methodology and a\\nrelated sound and complete learning algorithm that allows the complete change\\nof representation language of planning cases from concrete to abstract.\\nHowever, to achieve a powerful change of the representation language, the\\nabstract language itself as well as rules which describe admissible ways of\\nabstracting states must be provided in the domain model. This new abstraction\\napproach is the core of Paris (Plan Abstraction and Refinement in an Integrated\\nSystem), a system in which abstract planning cases are automatically learned\\nfrom given concrete cases. An empirical study in the domain of process planning\\nin mechanical engineering shows significant advantages of the proposed\\nreasoning from abstract cases over classical hierarchical planning.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Using Qualitative Hypotheses to Identify Inaccurate Data', 'author_1': 'Q. Zhao', 'author_2': 'T. Nishida', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Identifying inaccurate data has long been regarded as a significant and\\ndifficult problem in AI. In this paper, we present a new method for identifying\\ninaccurate data on the basis of qualitative correlations among related data.\\nFirst, we introduce the definitions of related data and qualitative\\ncorrelations among related data. Then we put forward a new concept called\\nsupport coefficient function (SCF). SCF can be used to extract, represent, and\\ncalculate qualitative correlations among related data within a dataset. We\\npropose an approach to determining dynamic shift intervals of inaccurate data,\\nand an approach to calculating possibility of identifying inaccurate data,\\nrespectively. Both of the approaches are based on SCF. Finally we present an\\nalgorithm for identifying inaccurate data by using qualitative correlations\\namong related data as confirmatory or disconfirmatory evidence. We have\\ndeveloped a practical system for interpreting infrared spectra by applying the\\nmethod, and have fully tested the system against several hundred real spectra.\\nThe experimental results show that the method is significantly better than the\\nconventional methods used in many similar systems.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Flexibly Instructable Agents', 'author_1': 'S. B. Huffman', 'author_2': 'J. E. Laird', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper presents an approach to learning from situated, interactive\\ntutorial instruction within an ongoing agent. Tutorial instruction is a\\nflexible (and thus powerful) paradigm for teaching tasks because it allows an\\ninstructor to communicate whatever types of knowledge an agent might need in\\nwhatever situations might arise. To support this flexibility, however, the\\nagent must be able to learn multiple kinds of knowledge from a broad range of\\ninstructional interactions. Our approach, called situated explanation, achieves\\nsuch learning through a combination of analytic and inductive techniques. It\\ncombines a form of explanation-based learning that is situated for each\\ninstruction with a full suite of contextually guided responses to incomplete\\nexplanations. The approach is implemented in an agent called Instructo-Soar\\nthat learns hierarchies of new tasks and other domain knowledge from\\ninteractive natural language instructions. Instructo-Soar meets three key\\nrequirements of flexible instructability that distinguish it from previous\\nsystems: (1) it can take known or unknown commands at any instruction point;\\n(2) it can handle instructions that apply to either its current situation or to\\na hypothetical situation specified in language (as in, for instance,\\nconditional instructions); and (3) it can learn, from instructions, each class\\nof knowledge it uses to perform tasks.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Learning Membership Functions in a Function-Based Object Recognition System', 'author_1': 'K. Woods', 'author_2': 'D. Cook', 'author_3': 'L. Hall', 'author_4': 'K. Bowyer', 'author_5': 'L. Stark', 'abstract': \"  Functionality-based recognition systems recognize objects at the category\\nlevel by reasoning about how well the objects support the expected function.\\nSuch systems naturally associate a ``measure of goodness'' or ``membership\\nvalue'' with a recognized object. This measure of goodness is the result of\\ncombining individual measures, or membership values, from potentially many\\nprimitive evaluations of different properties of the object's shape. A\\nmembership function is used to compute the membership value when evaluating a\\nprimitive of a particular physical property of an object. In previous versions\\nof a recognition system known as Gruff, the membership function for each of the\\nprimitive evaluations was hand-crafted by the system designer. In this paper,\\nwe provide a learning component for the Gruff system, called Omlet, that\\nautomatically learns membership functions given a set of example objects\\nlabeled with their desired category measure. The learning algorithm is\\ngenerally applicable to any problem in which low-level membership values are\\ncombined through an and-or tree structure to give a final overall membership\\nvalue.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Rule-based Machine Learning Methods for Functional Prediction', 'author_1': 'S. M. Weiss', 'author_2': 'N. Indurkhya', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  We describe a machine learning method for predicting the value of a\\nreal-valued function, given the values of multiple input variables. The method\\ninduces solutions from samples in the form of ordered disjunctive normal form\\n(DNF) decision rules. A central objective of the method and representation is\\nthe induction of compact, easily interpretable solutions. This rule-based\\ndecision model can be extended to search efficiently for similar cases prior to\\napproximating function values. Experimental results on real-world data\\ndemonstrate that the new techniques are competitive with existing machine\\nlearning and statistical methods and can sometimes yield superior regression\\nperformance.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Statistical Feature Combination for the Evaluation of Game Positions', 'author_1': 'M. Buro', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  This article describes an application of three well-known statistical methods\\nin the field of game-tree search: using a large number of classified Othello\\npositions, feature weights for evaluation functions with a\\ngame-phase-independent meaning are estimated by means of logistic regression,\\nFisher's linear discriminant, and the quadratic discriminant function for\\nnormally distributed features. Thereafter, the playing strengths are compared\\nby means of tournaments between the resulting versions of a world-class Othello\\nprogram. In this application, logistic regression - which is used here for the\\nfirst time in the context of game playing - leads to better results than the\\nother approaches.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Translating between Horn Representations and their Characteristic Models', 'author_1': 'R. Khardon', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Characteristic models are an alternative, model based, representation for\\nHorn expressions. It has been shown that these two representations are\\nincomparable and each has its advantages over the other. It is therefore\\nnatural to ask what is the cost of translating, back and forth, between these\\nrepresentations. Interestingly, the same translation questions arise in\\ndatabase theory, where it has applications to the design of relational\\ndatabases. This paper studies the computational complexity of these problems.\\nOur main result is that the two translation problems are equivalent under\\npolynomial reductions, and that they are equivalent to the corresponding\\ndecision problem. Namely, translating is equivalent to deciding whether a given\\nset of models is the set of characteristic models for a given Horn expression.\\nWe also relate these problems to the hypergraph transversal problem, a well\\nknown problem which is related to other applications in AI and for which no\\npolynomial time algorithm is known. It is shown that in general our translation\\nproblems are at least as hard as the hypergraph transversal problem, and in a\\nspecial case they are equivalent to it.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Iterative Optimization and Simplification of Hierarchical Clusterings', 'author_1': 'D. Fisher', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Clustering is often used for discovering structure in data. Clustering\\nsystems differ in the objective function used to evaluate clustering quality\\nand the control strategy used to search the space of clusterings. Ideally, the\\nsearch strategy should consistently construct clusterings of high quality, but\\nbe computationally inexpensive as well. In general, we cannot have it both\\nways, but we can partition the search so that a system inexpensively constructs\\na `tentative' clustering for initial examination, followed by iterative\\noptimization, which continues to search in background for improved clusterings.\\nGiven this motivation, we evaluate an inexpensive strategy for creating initial\\nclusterings, coupled with several control strategies for iterative\\noptimization, each of which repeatedly modifies an initial clustering in search\\nof a better one. One of these methods appears novel as an iterative\\noptimization strategy in clustering contexts. Once a clustering has been\\nconstructed it is judged by analysts -- often according to task-specific\\ncriteria. Several authors have abstracted these criteria and posited a generic\\nperformance task akin to pattern completion, where the error rate over\\ncompleted patterns is used to `externally' judge clustering utility. Given this\\nperformance task, we adapt resampling-based pruning strategies used by\\nsupervised learning systems to the task of simplifying hierarchical\\nclusterings, thus promising to ease post-clustering analysis. Finally, we\\npropose a number of objective functions, based on attribute-selection measures\\nfor decision-tree induction, that might perform well on the error rate and\\nsimplicity dimensions.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': '2Planning for Contingencies: A Decision-based Approach', 'author_1': 'L. Pryor', 'author_2': 'G. Collins', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  A fundamental assumption made by classical AI planners is that there is no\\nuncertainty in the world: the planner has full knowledge of the conditions\\nunder which the plan will be executed and the outcome of every action is fully\\npredictable. These planners cannot therefore construct contingency plans, i.e.,\\nplans in which different actions are performed in different circumstances. In\\nthis paper we discuss some issues that arise in the representation and\\nconstruction of contingency plans and describe Cassandra, a partial-order\\ncontingency planner. Cassandra uses explicit decision-steps that enable the\\nagent executing the plan to decide which plan branch to follow. The\\ndecision-steps in a plan result in subgoals to acquire knowledge, which are\\nplanned for in the same way as any other subgoals. Cassandra thus distinguishes\\nthe process of gathering information from the process of making decisions. The\\nexplicit representation of decisions in Cassandra allows a coherent approach to\\nthe problems of contingent planning, and provides a solid base for extensions\\nsuch as the use of different decision-making procedures.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Formal Framework for Speedup Learning from Problems and Solutions', 'author_1': 'P. Tadepalli', 'author_2': 'B. K. Natarajan', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Speedup learning seeks to improve the computational efficiency of problem\\nsolving with experience. In this paper, we develop a formal framework for\\nlearning efficient problem solving from random problems and their solutions. We\\napply this framework to two different representations of learned knowledge,\\nnamely control rules and macro-operators, and prove theorems that identify\\nsufficient conditions for learning in each representation. Our proofs are\\nconstructive in that they are accompanied with learning algorithms. Our\\nframework captures both empirical and explanation-based speedup learning in a\\nunified fashion. We illustrate our framework with implementations in two\\ndomains: symbolic integration and Eight Puzzle. This work integrates many\\nstrands of experimental and theoretical work in machine learning, including\\nempirical learning of control rules, macro-operator learning, Explanation-Based\\nLearning (EBL), and Probably Approximately Correct (PAC) Learning.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Principled Approach Towards Symbolic Geometric Constraint Satisfaction', 'author_1': 'S. Bhansali', 'author_2': 'G. A. Kramer', 'author_3': 'T. J. Hoar', 'author_4': None, 'author_5': None, 'abstract': '  An important problem in geometric reasoning is to find the configuration of a\\ncollection of geometric bodies so as to satisfy a set of given constraints.\\nRecently, it has been suggested that this problem can be solved efficiently by\\nsymbolically reasoning about geometry. This approach, called degrees of freedom\\nanalysis, employs a set of specialized routines called plan fragments that\\nspecify how to change the configuration of a set of bodies to satisfy a new\\nconstraint while preserving existing constraints. A potential drawback, which\\nlimits the scalability of this approach, is concerned with the difficulty of\\nwriting plan fragments. In this paper we address this limitation by showing how\\nthese plan fragments can be automatically synthesized using first principles\\nabout geometric bodies, actions, and topology.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'On Partially Controlled Multi-Agent Systems', 'author_1': 'R. I. Brafman', 'author_2': 'M. Tennenholtz', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Motivated by the control theoretic distinction between controllable and\\nuncontrollable events, we distinguish between two types of agents within a\\nmulti-agent system: controllable agents, which are directly controlled by the\\nsystem's designer, and uncontrollable agents, which are not under the\\ndesigner's direct control. We refer to such systems as partially controlled\\nmulti-agent systems, and we investigate how one might influence the behavior of\\nthe uncontrolled agents through appropriate design of the controlled agents. In\\nparticular, we wish to understand which problems are naturally described in\\nthese terms, what methods can be applied to influence the uncontrollable\\nagents, the effectiveness of such methods, and whether similar methods work\\nacross different domains. Using a game-theoretic framework, this paper studies\\nthe design of partially controlled multi-agent systems in two contexts: in one\\ncontext, the uncontrollable agents are expected utility maximizers, while in\\nthe other they are reinforcement learners. We suggest different techniques for\\ncontrolling agents' behavior in each domain, assess their success, and examine\\ntheir relationship.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Hierarchy of Tractable Subsets for Computing Stable Models', 'author_1': 'R. Ben-Eliyahu', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Finding the stable models of a knowledge base is a significant computational\\nproblem in artificial intelligence. This task is at the computational heart of\\ntruth maintenance systems, autoepistemic logic, and default logic.\\nUnfortunately, it is NP-hard. In this paper we present a hierarchy of classes\\nof knowledge bases, Omega_1,Omega_2,..., with the following properties: first,\\nOmega_1 is the class of all stratified knowledge bases; second, if a knowledge\\nbase Pi is in Omega_k, then Pi has at most k stable models, and all of them may\\nbe found in time O(lnk), where l is the length of the knowledge base and n the\\nnumber of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find\\nthe minimum k such that Pi belongs to Omega_k in time polynomial in the size of\\nPi; and, last, where K is the class of all knowledge bases, it is the case that\\nunion{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some\\nclass in the hierarchy.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Spatial Aggregation: Theory and Applications', 'author_1': 'K. Yip', 'author_2': 'F. Zhao', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Visual thinking plays an important role in scientific reasoning. Based on the\\nresearch in automating diverse reasoning tasks about dynamical systems,\\nnonlinear controllers, kinematic mechanisms, and fluid motion, we have\\nidentified a style of visual thinking, imagistic reasoning. Imagistic reasoning\\norganizes computations around image-like, analogue representations so that\\nperceptual and symbolic operations can be brought to bear to infer structure\\nand behavior. Programs incorporating imagistic reasoning have been shown to\\nperform at an expert level in domains that defy current analytic or numerical\\nmethods. We have developed a computational paradigm, spatial aggregation, to\\nunify the description of a class of imagistic problem solvers. A program\\nwritten in this paradigm has the following properties. It takes a continuous\\nfield and optional objective functions as input, and produces high-level\\ndescriptions of structure, behavior, or control actions. It computes a\\nmulti-layer of intermediate representations, called spatial aggregates, by\\nforming equivalence classes and adjacency relations. It employs a small set of\\ngeneric operators such as aggregation, classification, and localization to\\nperform bidirectional mapping between the information-rich field and\\nsuccessively more abstract spatial aggregates. It uses a data structure, the\\nneighborhood graph, as a common interface to modularize computations. To\\nillustrate our theory, we describe the computational structure of three\\nimplemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the\\nspatial aggregation generic operators by mixing and matching a library of\\ncommonly used routines.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Cue Phrase Classification Using Machine Learning', 'author_1': 'D. J. Litman', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Cue phrases may be used in a discourse sense to explicitly signal discourse\\nstructure, but also in a sentential sense to convey semantic rather than\\nstructural information. Correctly classifying cue phrases as discourse or\\nsentential is critical in natural language processing systems that exploit\\ndiscourse structure, e.g., for performing tasks such as anaphora resolution and\\nplan recognition. This paper explores the use of machine learning for\\nclassifying cue phrases as discourse or sentential. Two machine learning\\nprograms (Cgrendel and C4.5) are used to induce classification models from sets\\nof pre-classified cue phrases and their features in text and speech. Machine\\nlearning is shown to be an effective technique for not only automating the\\ngeneration of classification models, but also for improving upon previous\\nresults. When compared to manually derived classification models already in the\\nliterature, the learned models often perform with higher accuracy and contain\\nnew linguistic insights into the data. In addition, the ability to\\nautomatically construct classification models makes it easier to comparatively\\nanalyze the utility of alternative feature representations of the data.\\nFinally, the ease of retraining makes the learning approach more scalable and\\nflexible than manual methods.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Learning First-Order Definitions of Functions', 'author_1': 'J. R. Quinlan', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  First-order learning involves finding a clause-form definition of a relation\\nfrom examples of the relation and relevant background information. In this\\npaper, a particular first-order learning system is modified to customize it for\\nfinding definitions of functional relations. This restriction leads to faster\\nlearning times and, in some cases, to definitions that have higher predictive\\naccuracy. Other first-order learning systems might benefit from similar\\nspecialization.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Characterizations of Decomposable Dependency Models', 'author_1': 'L. M. deCampos', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Decomposable dependency models possess a number of interesting and useful\\nproperties. This paper presents new characterizations of decomposable models in\\nterms of independence relationships, which are obtained by adding a single\\naxiom to the well-known set characterizing dependency models that are\\nisomorphic to undirected graphs. We also briefly discuss a potential\\napplication of our results to the problem of learning graphical models from\\ndata.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer', 'author_1': 'J. C. Schlimmer', 'author_2': 'P. C. Wells', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Efficiently entering information into a computer is key to enjoying the\\nbenefits of computing. This paper describes three intelligent user interfaces:\\nhandwriting recognition, adaptive menus, and predictive fillin. In the context\\nof adding a personUs name and address to an electronic organizer, tests show\\nhandwriting recognition is slower than typing on an on-screen, soft keyboard,\\nwhile adaptive menus and predictive fillin can be twice as fast. This paper\\nalso presents strategies for applying these three interfaces to other\\ninformation collection domains.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'MUSE CSP: An Extension to the Constraint Satisfaction Problem', 'author_1': 'R. A Helzerman', 'author_2': 'M. P. Harper', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper describes an extension to the constraint satisfaction problem\\n(CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem).\\nThis extension is especially useful for those problems which segment into\\nmultiple sets of partially shared variables. Such problems arise naturally in\\nsignal processing applications including computer vision, speech processing,\\nand handwriting recognition. For these applications, it is often difficult to\\nsegment the data in only one way given the low-level information utilized by\\nthe segmentation algorithms. MUSE CSP can be used to compactly represent\\nseveral similar instances of the constraint satisfaction problem. If multiple\\ninstances of a CSP have some common variables which have the same domains and\\nconstraints, then they can be combined into a single instance of a MUSE CSP,\\nreducing the work required to apply the constraints. We introduce the concepts\\nof MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We\\nthen demonstrate how MUSE CSP can be used to compactly represent lexically\\nambiguous sentences and the multiple sentence hypotheses that are often\\ngenerated by speech recognition algorithms so that grammar constraints can be\\nused to provide parses for all syntactically correct sentences. Algorithms for\\nMUSE arc and path consistency are provided. Finally, we discuss how to create a\\nMUSE CSP from a set of CSPs which are labeled to indicate when the same\\nvariable is shared by more than a single CSP.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Exploiting Causal Independence in Bayesian Network Inference', 'author_1': 'N. L. Zhang', 'author_2': 'D. Poole', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  A new method is proposed for exploiting causal independencies in exact\\nBayesian network inference. A Bayesian network can be viewed as representing a\\nfactorization of a joint probability into the multiplication of a set of\\nconditional probabilities. We present a notion of causal independence that\\nenables one to further factorize the conditional probabilities into a\\ncombination of even smaller factors and consequently obtain a finer-grain\\nfactorization of the joint probability. The new formulation of causal\\nindependence lets us specify the conditional probability of a variable given\\nits parents in terms of an associative and commutative operator, such as\\n``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a\\nsimple algorithm VE for Bayesian network inference that, given evidence and a\\nquery variable, uses the factorization to find the posterior distribution of\\nthe query. We show how this algorithm can be extended to exploit causal\\nindependence. Empirical studies, based on the CPCS networks for medical\\ndiagnosis, show that this method is more efficient than previous methods and\\nallows for inference in larger networks than previous algorithms.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study', 'author_1': 'J. Gratch', 'author_2': 'S. Chien', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Although most scheduling problems are NP-hard, domain specific techniques\\nperform well in practice but are quite expensive to construct. In adaptive\\nproblem-solving solving, domain specific knowledge is acquired automatically\\nfor a general problem solver with a flexible control architecture. In this\\napproach, a learning system explores a space of possible heuristic methods for\\none well-suited to the eccentricities of the given domain and problem\\ndistribution. In this article, we discuss an application of the approach to\\nscheduling satellite communications. Using problem distributions based on\\nactual mission requirements, our approach identifies strategies that not only\\ndecrease the amount of CPU time required to produce schedules, but also\\nincrease the percentage of problems that are solvable within computational\\nresource limitations.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Accelerating Partial-Order Planners: Some Techniques for Effective Search Control and Pruning', 'author_1': 'A. Gerevini', 'author_2': 'L. Schubert', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  We propose some domain-independent techniques for bringing well-founded\\npartial-order planners closer to practicality. The first two techniques are\\naimed at improving search control while keeping overhead costs low. One is\\nbased on a simple adjustment to the default A* heuristic used by UCPOP to\\nselect plans for refinement. The other is based on preferring ``zero\\ncommitment'' (forced) plan refinements whenever possible, and using LIFO\\nprioritization otherwise. A more radical technique is the use of operator\\nparameter domains to prune search. These domains are initially computed from\\nthe definitions of the operators and the initial and goal conditions, using a\\npolynomial-time algorithm that propagates sets of constants through the\\noperator graph, starting in the initial conditions. During planning, parameter\\ndomains can be used to prune nonviable operator instances and to remove\\nspurious clobbering threats. In experiments based on modifications of UCPOP,\\nour improved plan and goal selection strategies gave speedups by factors\\nranging from 5 to more than 1000 for a variety of problems that are nontrivial\\nfor the unmodified version. Crucially, the hardest problems gave the greatest\\nimprovements. The pruning technique based on parameter domains often gave\\nspeedups by an order of magnitude or more for difficult problems, both with the\\ndefault UCPOP search strategy and with our improved strategy. The Lisp code for\\nour techniques and for the test problems is provided in on-line appendices.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Mechanisms for Automated Negotiation in State Oriented Domains', 'author_1': 'G. Zlotkin', 'author_2': 'J. S. Rosenschein', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  This paper lays part of the groundwork for a domain theory of negotiation,\\nthat is, a way of classifying interactions so that it is clear, given a domain,\\nwhich negotiation mechanisms and strategies are appropriate. We define State\\nOriented Domains, a general category of interaction. Necessary and sufficient\\nconditions for cooperation are outlined. We use the notion of worth in an\\naltered definition of utility, thus enabling agreements in a wider class of\\njoint-goal reachable situations. An approach is offered for conflict\\nresolution, and it is shown that even in a conflict situation, partial\\ncooperative steps can be taken by interacting agents (that is, agents in\\nfundamental conflict might still agree to cooperate up to a certain point). A\\nUnified Negotiation Protocol (UNP) is developed that can be used in all types\\nof encounters. It is shown that in certain borderline cooperative situations, a\\npartial cooperative agreement (i.e., one that does not achieve all agents'\\ngoals) might be preferred by all agents, even though there exists a rational\\nagreement that would achieve all their goals. Finally, we analyze cases where\\nagents have incomplete information on the goals and worth of other agents.\\nFirst we consider the case where agents' goals are private information, and we\\nanalyze what goal declaration strategies the agents might adopt to increase\\ntheir utility. Then, we consider the situation where the agents' goals (and\\ntherefore stand-alone costs) are common knowledge, but the worth they attach to\\ntheir goals is private information. We introduce two mechanisms, one 'strict',\\nthe other 'tolerant', and analyze their affects on the stability and efficiency\\nof negotiation outcomes.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Reinforcement Learning: A Survey', 'author_1': 'L. P. Kaelbling', 'author_2': 'M. L. Littman', 'author_3': 'A. W. Moore', 'author_4': None, 'author_5': None, 'abstract': \"  This paper surveys the field of reinforcement learning from a\\ncomputer-science perspective. It is written to be accessible to researchers\\nfamiliar with machine learning. Both the historical basis of the field and a\\nbroad selection of current work are summarized. Reinforcement learning is the\\nproblem faced by an agent that learns behavior through trial-and-error\\ninteractions with a dynamic environment. The work described here has a\\nresemblance to work in psychology, but differs considerably in the details and\\nin the use of the word ``reinforcement.'' The paper discusses central issues of\\nreinforcement learning, including trading off exploration and exploitation,\\nestablishing the foundations of the field via Markov decision theory, learning\\nfrom delayed reinforcement, constructing empirical models to accelerate\\nlearning, making use of generalization and hierarchy, and coping with hidden\\nstate. It concludes with a survey of some implemented systems and an assessment\\nof the practical utility of current methods for reinforcement learning.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Least Generalizations and Greatest Specializations of Sets of Clauses', 'author_1': 'S. H. Nienhuys-Cheng', 'author_2': 'R. deWolf', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  The main operations in Inductive Logic Programming (ILP) are generalization\\nand specialization, which only make sense in a generality order. In ILP, the\\nthree most important generality orders are subsumption, implication and\\nimplication relative to background knowledge. The two languages used most often\\nare languages of clauses and languages of only Horn clauses. This gives a total\\nof six different ordered languages. In this paper, we give a systematic\\ntreatment of the existence or non-existence of least generalizations and\\ngreatest specializations of finite sets of clauses in each of these six ordered\\nsets. We survey results already obtained by others and also contribute some\\nanswers of our own. Our main new results are, firstly, the existence of a\\ncomputable least generalization under implication of every finite set of\\nclauses containing at least one non-tautologous function-free clause (among\\nother, not necessarily function-free clauses). Secondly, we show that such a\\nleast generalization need not exist under relative implication, not even if\\nboth the set that is to be generalized and the background knowledge are\\nfunction-free. Thirdly, we give a complete discussion of existence and\\nnon-existence of greatest specializations in each of the six ordered languages.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': \"Further Experimental Evidence against the Utility of Occam's Razor\", 'author_1': 'G. I. Webb', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  This paper presents new experimental evidence against the utility of Occam's\\nrazor. A~systematic procedure is presented for post-processing decision trees\\nproduced by C4.5. This procedure was derived by rejecting Occam's razor and\\ninstead attending to the assumption that similar objects are likely to belong\\nto the same class. It increases a decision tree's complexity without altering\\nthe performance of that tree on the training data from which it is inferred.\\nThe resulting more complex decision trees are demonstrated to have, on average,\\nfor a variety of common learning tasks, higher predictive accuracy than the\\nless complex original decision trees. This result raises considerable doubt\\nabout the utility of Occam's razor as it is commonly applied in modern machine\\nlearning.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Practical Methods for Proving Termination of General Logic Programs', 'author_1': 'E. Marchiori', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Termination of logic programs with negated body atoms (here called general\\nlogic programs) is an important topic. One reason is that many computational\\nmechanisms used to process negated atoms, like Clark's negation as failure and\\nChan's constructive negation, are based on termination conditions. This paper\\nintroduces a methodology for proving termination of general logic programs\\nw.r.t. the Prolog selection rule. The idea is to distinguish parts of the\\nprogram depending on whether or not their termination depends on the selection\\nrule. To this end, the notions of low-, weakly up-, and up-acceptable program\\nare introduced. We use these notions to develop a methodology for proving\\ntermination of general logic programs, and show how interesting problems in\\nnon-monotonic reasoning can be formalized and implemented by means of\\nterminating general logic programs.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'A Divergence Critic for Inductive Proof', 'author_1': 'T. Walsh', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Inductive theorem provers often diverge. This paper describes a simple\\ncritic, a computer program which monitors the construction of inductive proofs\\nattempting to identify diverging proof attempts. Divergence is recognized by\\nmeans of a ``difference matching'' procedure. The critic then proposes lemmas\\nand generalizations which ``ripple'' these differences away so that the proof\\ncan go through without divergence. The critic enables the theorem prover Spike\\nto prove many theorems completely automatically from the definitions alone.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Improved Use of Continuous Attributes in C4.5', 'author_1': 'J. R. Quinlan', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  A reported weakness of C4.5 in domains with continuous attributes is\\naddressed by modifying the formation and evaluation of tests on continuous\\nattributes. An MDL-inspired penalty is applied to such tests, eliminating some\\nof them from consideration and altering the relative desirability of all tests.\\nEmpirical trials show that the modifications lead to smaller decision trees\\nwith higher predictive accuracies. Results also confirm that a new version of\\nC4.5 incorporating these changes is superior to recent approaches that use\\nglobal discretization and that construct small trees with multi-interval\\nsplits.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Active Learning with Statistical Models', 'author_1': 'D. A. Cohn', 'author_2': 'Z. Ghahramani', 'author_3': 'M. I. Jordan', 'author_4': None, 'author_5': None, 'abstract': \"  For many types of machine learning algorithms, one can compute the\\nstatistically `optimal' way to select training data. In this paper, we review\\nhow optimal data selection techniques have been used with feedforward neural\\nnetworks. We then show how the same principles may be used to select data for\\ntwo alternative, statistically-based learning architectures: mixtures of\\nGaussians and locally weighted regression. While the techniques for neural\\nnetworks are computationally expensive and approximate, the techniques for\\nmixtures of Gaussians and locally weighted regression are both efficient and\\naccurate. Empirically, we observe that the optimality criterion sharply\\ndecreases the number of training examples the learner needs in order to achieve\\ngood performance.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Mean Field Theory for Sigmoid Belief Networks', 'author_1': 'L. K. Saul', 'author_2': 'T. Jaakkola', 'author_3': 'M. I. Jordan', 'author_4': None, 'author_5': None, 'abstract': '  We develop a mean field theory for sigmoid belief networks based on ideas\\nfrom statistical mechanics. Our mean field theory provides a tractable\\napproximation to the true probability distribution in these networks; it also\\nyields a lower bound on the likelihood of evidence. We demonstrate the utility\\nof this framework on a benchmark problem in statistical pattern\\nrecognition---the classification of handwritten digits.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Quantum Computing and Phase Transitions in Combinatorial Search', 'author_1': 'T. Hogg', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  We introduce an algorithm for combinatorial search on quantum computers that\\nis capable of significantly concentrating amplitude into solutions for some NP\\nsearch problems, on average. This is done by exploiting the same aspects of\\nproblem structure as used by classical backtrack methods to avoid unproductive\\nsearch choices. This quantum algorithm is much more likely to find solutions\\nthan the simple direct use of quantum parallelism. Furthermore, empirical\\nevaluation on small problems shows this quantum algorithm displays the same\\nphase transition behavior, and at the same location, as seen in many previously\\nstudied classical search methods. Specifically, difficult problem instances are\\nconcentrated near the abrupt change from underconstrained to overconstrained\\nproblems.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Logarithmic-Time Updates and Queries in Probabilistic Networks', 'author_1': 'A. L. Delcher', 'author_2': 'A. J. Grove', 'author_3': 'S. Kasif', 'author_4': 'J. Pearl', 'author_5': None, 'abstract': '  Traditional databases commonly support efficient query and update procedures\\nthat operate in time which is sublinear in the size of the database. Our goal\\nin this paper is to take a first step toward dynamic reasoning in probabilistic\\ndatabases with comparable efficiency. We propose a dynamic data structure that\\nsupports efficient algorithms for updating and querying singly connected\\nBayesian networks. In the conventional algorithm, new evidence is absorbed in\\nO(1) time and queries are processed in time O(N), where N is the size of the\\nnetwork. We propose an algorithm which, after a preprocessing phase, allows us\\nto answer queries in time O(log N) at the expense of O(log N) time per evidence\\nabsorption. The usefulness of sub-linear processing time manifests itself in\\napplications requiring (near) real-time response over large probabilistic\\ndatabases. We briefly discuss a potential application of dynamic probabilistic\\nreasoning in computational biology.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences', 'author_1': 'G. Brewka', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  The paper describes an extension of well-founded semantics for logic programs\\nwith two types of negation. In this extension information about preferences\\nbetween rules can be expressed in the logical language and derived dynamically.\\nThis is achieved by using a reserved predicate symbol and a naming technique.\\nConflicts among rules are resolved whenever possible on the basis of derived\\npreference information. The well-founded conclusions of prioritized logic\\nprograms can be computed in polynomial time. A legal reasoning example\\nillustrates the usefulness of the approach.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Generalization of Clauses under Implication', 'author_1': 'P. Idestam-Almquist', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  In the area of inductive learning, generalization is a main operation, and\\nthe usual definition of induction is based on logical implication. Recently\\nthere has been a rising interest in clausal representation of knowledge in\\nmachine learning. Almost all inductive learning systems that perform\\ngeneralization of clauses use the relation theta-subsumption instead of\\nimplication. The main reason is that there is a well-known and simple technique\\nto compute least general generalizations under theta-subsumption, but not under\\nimplication. However generalization under theta-subsumption is inappropriate\\nfor learning recursive clauses, which is a crucial problem since recursion is\\nthe basic program structure of logic programs. We note that implication between\\nclauses is undecidable, and we therefore introduce a stronger form of\\nimplication, called T-implication, which is decidable between clauses. We show\\nthat for every finite set of clauses there exists a least general\\ngeneralization under T-implication. We describe a technique to reduce\\ngeneralizations under implication of a clause to generalizations under\\ntheta-subsumption of what we call an expansion of the original clause. Moreover\\nwe show that for every non-tautological clause there exists a T-complete\\nexpansion, which means that every generalization under T-implication of the\\nclause is reduced to a generalization under theta-subsumption of the expansion.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Decision-Theoretic Foundations for Causal Reasoning', 'author_1': 'D. Heckerman', 'author_2': 'R. Shachter', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  We present a definition of cause and effect in terms of decision-theoretic\\nprimitives and thereby provide a principled foundation for causal reasoning.\\nOur definition departs from the traditional view of causation in that causal\\nassertions may vary with the set of decisions available. We argue that this\\napproach provides added clarity to the notion of cause. Also in this paper, we\\nexamine the encoding of causal relationships in directed acyclic graphs. We\\ndescribe a special class of influence diagrams, those in canonical form, and\\nshow its relationship to Pearl's representation of cause and effect. Finally,\\nwe show how canonical form facilitates counterfactual reasoning.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'The Design and Experimental Analysis of Algorithms for Temporal Reasoning', 'author_1': 'P. vanBeek', 'author_2': 'D. W. Manchak', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Many applications -- from planning and scheduling to problems in molecular\\nbiology -- rely heavily on a temporal reasoning component. In this paper, we\\ndiscuss the design and empirical analysis of algorithms for a temporal\\nreasoning system based on Allen's influential interval-based framework for\\nrepresenting temporal information. At the core of the system are algorithms for\\ndetermining whether the temporal information is consistent, and, if so, finding\\none or more scenarios that are consistent with the temporal information. Two\\nimportant algorithms for these tasks are a path consistency algorithm and a\\nbacktracking algorithm. For the path consistency algorithm, we develop\\ntechniques that can result in up to a ten-fold speedup over an already highly\\noptimized implementation. For the backtracking algorithm, we develop variable\\nand value ordering heuristics that are shown empirically to dramatically\\nimprove the performance of the algorithm. As well, we show that a previously\\nsuggested reformulation of the backtracking search problem can reduce the time\\nand space requirements of the backtracking search. Taken together, the\\ntechniques we develop allow a temporal reasoning component to solve problems\\nthat are of practical size.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'OPUS: An Efficient Admissible Algorithm for Unordered Search', 'author_1': 'G. I. Webb', 'author_2': None, 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  OPUS is a branch and bound search algorithm that enables efficient admissible\\nsearch through spaces for which the order of search operator application is not\\nsignificant. The algorithm's search efficiency is demonstrated with respect to\\nvery large machine learning search spaces. The use of admissible search is of\\npotential value to the machine learning community as it means that the exact\\nlearning biases to be employed for complex learning tasks can be precisely\\nspecified and manipulated. OPUS also has potential for application in other\\nareas of artificial intelligence, notably, truth maintenance.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach', 'author_1': 'A. Broggi', 'author_2': 'S. Berte', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  The main aim of this work is the development of a vision-based road detection\\nsystem fast enough to cope with the difficult real-time constraints imposed by\\nmoving vehicle applications. The hardware platform, a special-purpose massively\\nparallel system, has been chosen to minimize system production and operational\\ncosts. This paper presents a novel approach to expectation-driven low-level\\nimage segmentation, which can be mapped naturally onto mesh-connected massively\\nparallel SIMD architectures capable of handling hierarchical data structures.\\nThe input image is assumed to contain a distorted version of a given template;\\na multiresolution stretching process is used to reshape the original template\\nin accordance with the acquired image content, minimizing a potential function.\\nThe distorted template is the process output.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Improving Connectionist Energy Minimization', 'author_1': 'G. Pinkas', 'author_2': 'R. Dechter', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Symmetric networks designed for energy minimization such as Boltzman machines\\nand Hopfield nets are frequently investigated for use in optimization,\\nconstraint satisfaction and approximation of NP-hard problems. Nevertheless,\\nfinding a global solution (i.e., a global minimum for the energy function) is\\nnot guaranteed and even a local solution may take an exponential number of\\nsteps. We propose an improvement to the standard local activation function used\\nfor such networks. The improved algorithm guarantees that a global minimum is\\nfound in linear time for tree-like subnetworks. The algorithm, called activate,\\nis uniform and does not assume that the network is tree-like. It can identify\\ntree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid\\nlocal minima along these trees. For acyclic networks, the algorithm is\\nguaranteed to converge to a global minimum from any initial state of the system\\n(self-stabilization) and remains correct under various types of schedulers. On\\nthe negative side, we show that in the presence of cycles, no uniform algorithm\\nexists that guarantees optimality even under a sequential asynchronous\\nscheduler. An asynchronous scheduler can activate only one unit at a time while\\na synchronous scheduler can activate any number of units in a single time step.\\nIn addition, no uniform algorithm exists to optimize even acyclic networks when\\nthe scheduler is synchronous. Finally, we show how the algorithm can be\\nimproved using the cycle-cutset scheme. The general algorithm, called\\nactivate-with-cutset, improves over activate and has some performance\\nguarantees that are related to the size of the network's cycle-cutset.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Diffusion of Context and Credit Information in Markovian Models', 'author_1': 'Y. Bengio', 'author_2': 'P. Frasconi', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper studies the problem of ergodicity of transition probability\\nmatrices in Markovian models, such as hidden Markov models (HMMs), and how it\\nmakes very difficult the task of learning to represent long-term context for\\nsequential data. This phenomenon hurts the forward propagation of long-term\\ncontext information, as well as learning a hidden state representation to\\nrepresent long-term context, which depends on propagating credit information\\nbackwards in time. Using results from Markov chain theory, we show that this\\nproblem of diffusion of context and credit is reduced when the transition\\nprobabilities approach 0 or 1, i.e., the transition probability matrices are\\nsparse and the model essentially deterministic. The results found in this paper\\napply to learning approaches based on continuous optimization, such as gradient\\ndescent and the Baum-Welch algorithm.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs', 'author_1': 'R. J. Mooney', 'author_2': 'M. E. Califf', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  This paper presents a method for inducing logic programs from examples that\\nlearns a new class of concepts called first-order decision lists, defined as\\nordered lists of clauses each ending in a cut. The method, called FOIDL, is\\nbased on FOIL (Quinlan, 1990) but employs intensional background knowledge and\\navoids the need for explicit negative examples. It is particularly useful for\\nproblems that involve rules with specific exceptions, such as learning the\\npast-tense of English verbs, a task widely studied in the context of the\\nsymbolic/connectionist debate. FOIDL is able to learn concise, accurate\\nprograms for this problem from significantly fewer examples than previous\\nmethods (both connectionist and symbolic).\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'FLECS: Planning with a Flexible Commitment Strategy', 'author_1': 'M. Veloso', 'author_2': 'P. Stone', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  There has been evidence that least-commitment planners can efficiently handle\\nplanning problems that involve difficult goal interactions. This evidence has\\nled to the common belief that delayed-commitment is the \"best\" possible\\nplanning strategy. However, we recently found evidence that eager-commitment\\nplanners can handle a variety of planning problems more efficiently, in\\nparticular those with difficult operator choices. Resigned to the futility of\\ntrying to find a universally successful planning strategy, we devised a planner\\nthat can be used to study which domains and problems are best for which\\nplanning strategies. In this article we introduce this new planning algorithm,\\nFLECS, which uses a FLExible Commitment Strategy with respect to plan-step\\norderings. It is able to use any strategy from delayed-commitment to\\neager-commitment. The combination of delayed and eager operator-ordering\\ncommitments allows FLECS to take advantage of the benefits of explicitly using\\na simulated execution state and reasoning about planning constraints. FLECS can\\nvary its commitment strategy across different problems and domains, and also\\nduring the course of a single planning problem. FLECS represents a novel\\ncontribution to planning in that it explicitly provides the choice of which\\ncommitment strategy to use while planning. FLECS provides a framework to\\ninvestigate the mapping from planning domains and problems to efficient\\nplanning strategies.\\n\\n    '}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks', 'author_1': 'S. Wermter', 'author_2': 'V. Weber', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': \"  Previous approaches of analyzing spontaneously spoken language often have\\nbeen based on encoding syntactic and semantic knowledge manually and\\nsymbolically. While there has been some progress using statistical or\\nconnectionist language models, many current spoken- language systems still use\\na relatively brittle, hand-coded symbolic grammar or symbolic semantic\\ncomponent. In contrast, we describe a so-called screening approach for learning\\nrobust processing of spontaneously spoken language. A screening approach is a\\nflat analysis which uses shallow sequences of category representations for\\nanalyzing an utterance at various syntactic, semantic and dialog levels. Rather\\nthan using a deeply structured symbolic analysis, we use a flat connectionist\\nanalysis. This screening approach aims at supporting speech and language\\nprocessing by using (1) data-driven learning and (2) robustness of\\nconnectionist networks. In order to test this approach, we have developed the\\nSCREEN system which is based on this new robust, learned and flat analysis. In\\nthis paper, we focus on a detailed description of SCREEN's architecture, the\\nflat syntactic and semantic analysis, the interaction with a speech recognizer,\\nand a detailed evaluation analysis of the robustness under the influence of\\nnoisy or incomplete input. The main result of this paper is that flat\\nrepresentations allow more robust processing of spontaneous spoken language\\nthan deeply structured representations. In particular, we show how the\\nfault-tolerance and learning capability of connectionist networks can support a\\nflat analysis for providing more robust spoken-language processing within an\\noverall hybrid symbolic/connectionist framework.\\n\\n    \"}\n",
      "{'subject': 'Computer Science > Artificial Intelligence', 'title': 'Improved Heterogeneous Distance Functions', 'author_1': 'D. R. Wilson', 'author_2': 'T. R. Martinez', 'author_3': None, 'author_4': None, 'author_5': None, 'abstract': '  Instance-based learning techniques typically handle continuous and linear\\ninput values well, but often do not handle nominal input attributes\\nappropriately. The Value Difference Metric (VDM) was designed to find\\nreasonable distance values between nominal attribute values, but it largely\\nignores continuous attributes, requiring discretization to map continuous\\nvalues into nominal values. This paper proposes three new heterogeneous\\ndistance functions, called the Heterogeneous Value Difference Metric (HVDM),\\nthe Interpolated Value Difference Metric (IVDM), and the Windowed Value\\nDifference Metric (WVDM). These new distance functions are designed to handle\\napplications with nominal attributes, continuous attributes, or both. In\\nexperiments on 48 applications the new distance metrics achieve higher\\nclassification accuracy on average than three previous distance functions on\\nthose datasets that have both nominal and continuous attributes.\\n\\n    '}\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Double spiders\n",
    "\n",
    "class data_spider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "\n",
    "    def subcat_url_generator(field, category):\n",
    "        years = [str(yr)[-2:] for yr in range(1993,2022)]\n",
    "        months = ['01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
    "              '10', '11', '12']\n",
    "        links = []\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                link = 'https://arxiv.org/list?archive='\n",
    "                link += field + '.' + category\n",
    "                link += '&year=' + year\n",
    "                link += '&month=' + month\n",
    "                link += '&submit=Go?skip=0&show=2000'\n",
    "                links.append(link)\n",
    "        return links\n",
    "    \n",
    "    start_urls = subcat_url_generator('cs', 'ai')[:50]\n",
    "    found_events = []\n",
    "    \n",
    "    base_url = 'https://arxiv.org'\n",
    "    \n",
    "    \n",
    "    def parse(self, response):\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = data_spider.base_url + rel_url\n",
    "            yield scrapy.Request(url, callback=self.parse2)\n",
    "            \n",
    "    def parse2(self, response):\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "            \n",
    "            event_details['subject'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['abstract'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(data_spider)\n",
    "    process.start()\n",
    "\n",
    "    for event in data_spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# create csv\n",
    "\n",
    "dataset = data_spider.found_events\n",
    "keys= dataset[0].keys()\n",
    "\n",
    "try:\n",
    "    with open(\"dataset.csv\", 'w', newline='') as output_file:\n",
    "        w = csv.DictWriter(output_file, keys, delimiter=';')\n",
    "        w.writeheader()\n",
    "        w.writerows(dataset)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
