{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.851 · Tipología y ciclo de vida de los datos · PRA1</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2020-2 · Máster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# Práctica 1: Web scraping\n",
    "\n",
    " - **https://github.com/sfunesolaria/Pra1WebScraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcat_url_generator(field, category):\n",
    "    \n",
    "    years = [str(yr)[-2:] for yr in range(1993,2022)]\n",
    "    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
    "              '10', '11', '12']\n",
    "    \n",
    "    links = []\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            link = 'https://arxiv.org/list?archive='\n",
    "            link += field + '.' + category\n",
    "            link += '&year=' + year\n",
    "            link += '&month=' + month\n",
    "            link += '&submit=Go?skip=0&show=2000'\n",
    "            links.append(link)\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "cs_ai_urls = subcat_url_generator('cs', 'ai')\n",
    "cs_ai_urls[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spider should get as `start_urls` an url list, as shown above, to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class link_spider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/list?archive=cs.ai&year=20&month=03&submit=Go?skip=0&show=100']\n",
    "    found_links = []\n",
    "    \n",
    "    base_url = 'https://arxiv.org'\n",
    "\n",
    "    def parse(self, response):\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = link_spider.base_url + rel_url\n",
    "            links['url'] = url\n",
    "            \n",
    "            self.found_links.append(links)\n",
    "            \n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(link_spider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_links: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class PythonSpider(scrapy.Spider):\n",
    "    name = 'pythonspider'\n",
    "\n",
    "    start_urls = ['https://arxiv.org/abs/2003.00030',]\n",
    "    found_events = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "            \n",
    "            event_details['subject'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['abstract'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(PythonSpider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Double spiders\n",
    "\n",
    "class data_spider(scrapy.Spider):\n",
    "    name = 'link_spider'\n",
    "\n",
    "    found_events = []\n",
    "    base_url = 'https://arxiv.org'\n",
    "\n",
    "    def subcat_url_generator(field, category):\n",
    "        years = [str(yr)[-2:] for yr in range(1993,2022)]\n",
    "        months = ['01', '02', '03', '04', '05', '06', '07', '08', '09',\n",
    "              '10', '11', '12']\n",
    "        links = []\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                link = 'https://arxiv.org/list?archive='\n",
    "                link += field + '.' + category\n",
    "                link += '&year=' + year\n",
    "                link += '&month=' + month\n",
    "                link += '&submit=Go?skip=0&show=2000'\n",
    "                links.append(link)\n",
    "        return links\n",
    "\n",
    "    start_urls = subcat_url_generator('cs', 'ai')[:300]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = data_spider.base_url + rel_url\n",
    "            yield scrapy.Request(url, callback=self.parse2)\n",
    "\n",
    "    def parse2(self, response):\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "\n",
    "            event_details['id'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"metatable\")]/table/tr/td[contains(@class, \"tablecell arxivid\")]/span/a/text()').extract_first()\n",
    "\n",
    "            event_details['category_code'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"metatable\")]/table/tr/td[contains(@class, \"tablecell arxivid\")]/span/a/following-sibling::text()').extract_first().strip()\n",
    "            \n",
    "            event_details['category'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "\n",
    "            event_details['date'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div/text()').extract_first().replace(\"\\n\", \"\").strip()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['summary'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first().replace(\"\\n\", \"\").strip()\n",
    "            \n",
    "            event_details['link'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"metatable\")]/table/tr/td[contains(@class, \"tablecell arxivid\")]/span/a/@href').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(data_spider)\n",
    "    process.start()\n",
    "\n",
    "#    for event in data_spider.found_events: print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# create csv\n",
    "\n",
    "dataset = data_spider.found_events\n",
    "keys= dataset[0].keys()\n",
    "\n",
    "try:\n",
    "    with open(\"dataset.csv\", 'w', newline='') as output_file:\n",
    "        w = csv.DictWriter(output_file, keys, delimiter=';')\n",
    "        w.writeheader()\n",
    "        w.writerows(dataset)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double spiders\n",
    "\n",
    "class data_spider(scrapy.Spider):\n",
    "    name = 'data_spider'\n",
    "\n",
    "    found_events = []\n",
    "\n",
    "    def search_url_generator(n_results):\n",
    "        \"\"\" Get all results URLs from selected search\n",
    "    \n",
    "        n_results (int) -- Number of total results in search\n",
    "        \"\"\"\n",
    "        skip_articles = [str(ini) for ini in range(0, n_results, 25)]\n",
    "        links = []\n",
    "        for skip in skip_articles:\n",
    "            link = 'https://export.arxiv.org/find/all/1/'\n",
    "            link += 'abs:+AND+vaccines+OR+EXACT+COVID_19+EXACT+SARS_COV-2'\n",
    "            link += '/0/1/0/all/0/1?'\n",
    "            link += 'skip=' + skip\n",
    "            links.append(link)\n",
    "        return links\n",
    "\n",
    "    start_urls = search_url_generator(266)\n",
    "    base_url = 'https://export.arxiv.org'\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Get all links in search page\n",
    "        for link in response.xpath('//span[@class=\"list-identifier\"]'):\n",
    "            links = {}\n",
    "            rel_url = str(link.xpath('.//a/@href').extract_first())\n",
    "            url = data_spider.base_url + rel_url\n",
    "            yield scrapy.Request(url, callback=self.parse2)\n",
    "\n",
    "\n",
    "    def parse2(self, response):\n",
    "        # Get elements of interest in article page and store\n",
    "        for event in response.xpath('//div[contains(@class, \"leftcolumn\")]'):\n",
    "            event_details = dict()\n",
    "\n",
    "            event_details['id'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"metatable\")]/table/tr/td[contains(@class, \"tablecell arxivid\")]/span/a/text()').extract_first()\n",
    "\n",
    "            event_details['category_code'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"metatable\")]/table/tr/td[contains(@class, \"tablecell arxivid\")]/span/a/following-sibling::text()').extract_first().strip()\n",
    "            \n",
    "            event_details['category'] = event.xpath('div[contains(@class, \"subheader\")]/h1/text()').extract_first()\n",
    "            \n",
    "            event_details['title'] = event.xpath('div[contains(@id, \"content-inner\")]/div/h1[contains(@class, \"title mathjax\")]/span/following-sibling::text()').extract_first()\n",
    "\n",
    "            event_details['date'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div/text()').extract_first().replace(\"\\n\", \"\").strip()\n",
    "            \n",
    "            event_details['author_1'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[1]/text()').extract_first()\n",
    "\n",
    "            event_details['author_2'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[2]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_3'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[3]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_4'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[4]/text()').extract_first()\n",
    "            \n",
    "            event_details['author_5'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"authors\")]/a[5]/text()').extract_first()\n",
    "            \n",
    "            event_details['summary'] = event.xpath('div[contains(@id, \"content-inner\")]/div/blockquote[contains(@class, \"abstract mathjax\")]/span/following-sibling::text()').extract_first().replace(\"\\n\", \"\").strip()\n",
    "            \n",
    "            event_details['link'] = event.xpath('div[contains(@id, \"content-inner\")]/div/div[contains(@class, \"metatable\")]/table/tr/td[contains(@class, \"tablecell arxivid\")]/span/a/@href').extract_first()\n",
    "            \n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({\n",
    "        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "        'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(data_spider)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset into csv\n",
    "\n",
    "dataset = data_spider.found_events\n",
    "keys = dataset[0].keys()\n",
    "\n",
    "try:\n",
    "    with open(\"dataset_covid.csv\", 'w', newline='') as output_file:\n",
    "        w = csv.DictWriter(output_file, keys, delimiter=';')\n",
    "        w.writeheader()\n",
    "        w.writerows(dataset)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
